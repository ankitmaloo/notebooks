{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# verl + Modular RL Primitives Training\n",
    "\n",
    "This notebook demonstrates how to combine:\n",
    "- **verl**: Production-ready RL framework with FSDP, vLLM, distributed training\n",
    "- **Modular primitives**: Flexible, composable components for custom rewards and environments\n",
    "\n",
    "## What you'll learn:\n",
    "1. Setting up verl with modular primitives\n",
    "2. Running PPO training with custom rewards\n",
    "3. Experimenting with different configurations\n",
    "4. Evaluating and analyzing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, os.path.abspath('../..'))\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check verl installation\n",
    "try:\n",
    "    import verl\n",
    "    print(\"✓ verl installed\")\n",
    "    VERL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"✗ verl not installed\")\n",
    "    print(\"Install with: pip install verl\")\n",
    "    VERL_AVAILABLE = False\n",
    "\n",
    "# Check vLLM installation\n",
    "try:\n",
    "    import vllm\n",
    "    print(\"✓ vLLM installed\")\n",
    "    VLLM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"✗ vLLM not installed (optional, but recommended for fast generation)\")\n",
    "    print(\"Install with: pip install vllm\")\n",
    "    VLLM_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Configure your training run. Start with small values for quick experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Training configuration\"\"\"\n",
    "    \n",
    "    # Model settings\n",
    "    model_name: str = \"gpt2\"  # Start small for testing\n",
    "    # model_name: str = \"meta-llama/Llama-2-7b-hf\"  # Use for production\n",
    "    \n",
    "    # Training settings\n",
    "    num_epochs: int = 3\n",
    "    rollout_batch_size: int = 8  # Small for notebook\n",
    "    train_batch_size: int = 8\n",
    "    \n",
    "    # PPO hyperparameters\n",
    "    learning_rate: float = 1e-5\n",
    "    ppo_epochs: int = 4\n",
    "    gamma: float = 1.0\n",
    "    gae_lambda: float = 0.95\n",
    "    clip_range: float = 0.2\n",
    "    vf_coef: float = 0.5\n",
    "    kl_coef: float = 0.05\n",
    "    \n",
    "    # Generation settings\n",
    "    max_prompt_length: int = 256\n",
    "    max_new_tokens: int = 128\n",
    "    temperature: float = 0.8\n",
    "    top_p: float = 0.9\n",
    "    \n",
    "    # verl backend\n",
    "    use_vllm: bool = VLLM_AVAILABLE\n",
    "    \n",
    "    # Paths\n",
    "    output_dir: str = \"./outputs/verl_notebook\"\n",
    "    \n",
    "config = Config()\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {config.model_name}\")\n",
    "print(f\"  Epochs: {config.num_epochs}\")\n",
    "print(f\"  Batch size: {config.rollout_batch_size}\")\n",
    "print(f\"  Using vLLM: {config.use_vllm}\")\n",
    "print(f\"  Output dir: {config.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Training Data\n",
    "\n",
    "Create sample prompts for training. Replace with your actual dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample prompts for demonstration\n",
    "train_prompts = [\n",
    "    \"Explain how neural networks learn:\",\n",
    "    \"What is the difference between supervised and unsupervised learning?\",\n",
    "    \"Describe the attention mechanism in transformers:\",\n",
    "    \"How does backpropagation work?\",\n",
    "    \"What is overfitting and how can it be prevented?\",\n",
    "    \"Explain the concept of gradient descent:\",\n",
    "    \"What are the advantages of deep learning?\",\n",
    "    \"Describe how convolutional neural networks process images:\",\n",
    "    \"What is transfer learning and when should it be used?\",\n",
    "    \"Explain the role of activation functions:\",\n",
    "]\n",
    "\n",
    "eval_prompts = [\n",
    "    \"What is reinforcement learning?\",\n",
    "    \"Explain batch normalization:\",\n",
    "    \"What are recurrent neural networks used for?\",\n",
    "]\n",
    "\n",
    "print(f\"Training prompts: {len(train_prompts)}\")\n",
    "print(f\"Eval prompts: {len(eval_prompts)}\")\n",
    "print(\"\\nExample prompt:\")\n",
    "print(f\"  {train_prompts[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(f\"Loading model: {config.model_name}...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Set pad_token = eos_token\")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    ")\n",
    "\n",
    "# Create reference model (for KL penalty)\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    ")\n",
    "ref_model.eval()  # Keep reference model frozen\n",
    "\n",
    "print(\"✓ Model loaded\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
    "print(f\"  Device: {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Custom Reward Function\n",
    "\n",
    "**This is where you customize!** Define how to score model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRewardComputer:\n",
    "    \"\"\"\n",
    "    Custom reward function for your task.\n",
    "    \n",
    "    Modify this to implement your specific reward logic:\n",
    "    - Call a reward model\n",
    "    - Rule-based scoring\n",
    "    - Verification (code execution, math checking)\n",
    "    - Human feedback proxy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize reward model or scoring tools here\n",
    "        pass\n",
    "    \n",
    "    def compute_reward(self, prompt: str, response: str) -> float:\n",
    "        \"\"\"\n",
    "        Compute reward for a prompt-response pair.\n",
    "        \n",
    "        Returns:\n",
    "            float: Reward score (higher is better)\n",
    "        \"\"\"\n",
    "        reward = 0.0\n",
    "        \n",
    "        # 1. Length reward (prefer detailed responses)\n",
    "        words = response.split()\n",
    "        length_score = min(len(words) / 50.0, 1.0)  # Cap at 50 words\n",
    "        reward += length_score * 0.3\n",
    "        \n",
    "        # 2. Informativeness (presence of explanatory words)\n",
    "        informative_words = [\n",
    "            'because', 'therefore', 'however', 'which', 'when',\n",
    "            'where', 'how', 'why', 'example', 'such as'\n",
    "        ]\n",
    "        info_count = sum(1 for word in informative_words \n",
    "                        if word in response.lower())\n",
    "        reward += min(info_count / 5.0, 1.0) * 0.3\n",
    "        \n",
    "        # 3. Structure (has proper sentences)\n",
    "        has_period = '.' in response\n",
    "        has_capital = any(c.isupper() for c in response)\n",
    "        structure_score = (has_period + has_capital) / 2.0\n",
    "        reward += structure_score * 0.2\n",
    "        \n",
    "        # 4. Relevance (contains words from prompt)\n",
    "        prompt_words = set(prompt.lower().split())\n",
    "        response_words = set(response.lower().split())\n",
    "        overlap = len(prompt_words & response_words) / max(len(prompt_words), 1)\n",
    "        reward += overlap * 0.2\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def compute_batch_rewards(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        responses: List[str]\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Compute rewards for a batch\"\"\"\n",
    "        rewards = [\n",
    "            self.compute_reward(p, r)\n",
    "            for p, r in zip(prompts, responses)\n",
    "        ]\n",
    "        return np.array(rewards, dtype=np.float32)\n",
    "\n",
    "# Initialize reward computer\n",
    "reward_computer = CustomRewardComputer()\n",
    "\n",
    "# Test the reward function\n",
    "test_prompt = \"Explain neural networks:\"\n",
    "test_response_good = \"Neural networks are computational models inspired by biological neurons. They learn by adjusting weights through backpropagation, which allows them to recognize patterns in data.\"\n",
    "test_response_bad = \"Networks.\"\n",
    "\n",
    "print(\"Testing reward function:\")\n",
    "print(f\"  Good response: {reward_computer.compute_reward(test_prompt, test_response_good):.3f}\")\n",
    "print(f\"  Bad response:  {reward_computer.compute_reward(test_prompt, test_response_bad):.3f}\")\n",
    "print(\"\\n✓ Reward function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup Training Components\n",
    "\n",
    "Initialize optimizer and training utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    betas=(0.9, 0.95),\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "print(\"✓ Optimizer initialized\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "\n",
    "# Metrics tracking\n",
    "metrics_history = {\n",
    "    'epoch': [],\n",
    "    'policy_loss': [],\n",
    "    'value_loss': [],\n",
    "    'total_loss': [],\n",
    "    'reward_mean': [],\n",
    "    'reward_std': [],\n",
    "    'kl_div': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generation Function\n",
    "\n",
    "Generate responses from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(\n",
    "    prompts: List[str],\n",
    "    model,\n",
    "    tokenizer,\n",
    "    max_new_tokens: int = 128,\n",
    "    temperature: float = 0.8,\n",
    "    top_p: float = 0.9,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Generate responses for a batch of prompts.\n",
    "    \n",
    "    Returns:\n",
    "        responses: List of generated text\n",
    "        response_ids: Tensor of token IDs\n",
    "        log_probs: Log probabilities of generated tokens\n",
    "    \"\"\"\n",
    "    # Tokenize prompts\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=config.max_prompt_length,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "    \n",
    "    # Decode responses\n",
    "    response_ids = outputs.sequences[:, inputs.input_ids.shape[1]:]\n",
    "    responses = tokenizer.batch_decode(response_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute log probabilities\n",
    "    log_probs = []\n",
    "    for score in outputs.scores:\n",
    "        log_probs.append(F.log_softmax(score, dim=-1))\n",
    "    \n",
    "    return responses, response_ids, log_probs\n",
    "\n",
    "print(\"✓ Generation function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PPO Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantages(\n",
    "    rewards: np.ndarray,\n",
    "    values: Optional[np.ndarray] = None,\n",
    "    gamma: float = 1.0,\n",
    "    gae_lambda: float = 0.95,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute advantages using GAE.\n",
    "    \n",
    "    For language modeling, we typically use gamma=1.0 (no discounting)\n",
    "    since there's no meaningful temporal structure.\n",
    "    \"\"\"\n",
    "    if values is None:\n",
    "        # Simple baseline: mean reward\n",
    "        advantages = rewards - rewards.mean()\n",
    "    else:\n",
    "        # GAE with value function\n",
    "        advantages = rewards - values\n",
    "    \n",
    "    # Normalize\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    \n",
    "    return advantages\n",
    "\n",
    "\n",
    "def compute_kl_divergence(\n",
    "    model,\n",
    "    ref_model,\n",
    "    input_ids: torch.Tensor,\n",
    "    response_ids: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute KL divergence between current policy and reference policy.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        ref_logits = ref_model(input_ids=response_ids).logits\n",
    "        ref_log_probs = F.log_softmax(ref_logits, dim=-1)\n",
    "    \n",
    "    curr_logits = model(input_ids=response_ids).logits\n",
    "    curr_log_probs = F.log_softmax(curr_logits, dim=-1)\n",
    "    \n",
    "    kl_div = F.kl_div(\n",
    "        curr_log_probs,\n",
    "        ref_log_probs,\n",
    "        log_target=True,\n",
    "        reduction='batchmean'\n",
    "    )\n",
    "    \n",
    "    return kl_div\n",
    "\n",
    "print(\"✓ PPO functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop\n",
    "\n",
    "Main PPO training loop combining all components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Epoch {epoch + 1}/{config.num_epochs}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Sample batch of prompts\n",
    "    batch_prompts = random.sample(\n",
    "        train_prompts,\n",
    "        min(config.rollout_batch_size, len(train_prompts))\n",
    "    )\n",
    "    \n",
    "    # 1. Generate responses\n",
    "    print(f\"\\n[1/5] Generating {len(batch_prompts)} responses...\")\n",
    "    model.eval()\n",
    "    responses, response_ids, log_probs = generate_responses(\n",
    "        batch_prompts,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        max_new_tokens=config.max_new_tokens,\n",
    "        temperature=config.temperature,\n",
    "        top_p=config.top_p,\n",
    "    )\n",
    "    \n",
    "    # Show example\n",
    "    print(f\"\\nExample generation:\")\n",
    "    print(f\"  Prompt: {batch_prompts[0]}\")\n",
    "    print(f\"  Response: {responses[0][:100]}...\")\n",
    "    \n",
    "    # 2. Compute rewards\n",
    "    print(f\"\\n[2/5] Computing rewards...\")\n",
    "    rewards = reward_computer.compute_batch_rewards(\n",
    "        batch_prompts,\n",
    "        responses\n",
    "    )\n",
    "    \n",
    "    print(f\"  Reward stats:\")\n",
    "    print(f\"    Mean: {rewards.mean():.3f}\")\n",
    "    print(f\"    Std:  {rewards.std():.3f}\")\n",
    "    print(f\"    Min:  {rewards.min():.3f}\")\n",
    "    print(f\"    Max:  {rewards.max():.3f}\")\n",
    "    \n",
    "    # 3. Compute advantages\n",
    "    print(f\"\\n[3/5] Computing advantages...\")\n",
    "    advantages = compute_advantages(\n",
    "        rewards,\n",
    "        gamma=config.gamma,\n",
    "        gae_lambda=config.gae_lambda\n",
    "    )\n",
    "    advantages_tensor = torch.tensor(\n",
    "        advantages,\n",
    "        dtype=torch.float32,\n",
    "        device=model.device\n",
    "    )\n",
    "    \n",
    "    # 4. PPO update\n",
    "    print(f\"\\n[4/5] Running PPO update ({config.ppo_epochs} epochs)...\")\n",
    "    model.train()\n",
    "    \n",
    "    epoch_policy_loss = 0.0\n",
    "    epoch_kl_div = 0.0\n",
    "    \n",
    "    for ppo_epoch in range(config.ppo_epochs):\n",
    "        # Simplified PPO update (full implementation would batch this)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get current log probs\n",
    "        # (In full implementation, compute ratio with old log probs and clip)\n",
    "        \n",
    "        # Simplified loss: -advantages * log_probs\n",
    "        # In practice, you'd compute proper PPO clipped loss\n",
    "        policy_loss = -(advantages_tensor.mean())  # Placeholder\n",
    "        \n",
    "        # KL penalty\n",
    "        # kl_div = compute_kl_divergence(model, ref_model, ...)\n",
    "        kl_div = torch.tensor(0.01)  # Placeholder\n",
    "        \n",
    "        # Total loss\n",
    "        loss = policy_loss + config.kl_coef * kl_div\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_policy_loss += policy_loss.item()\n",
    "        epoch_kl_div += kl_div.item()\n",
    "    \n",
    "    avg_policy_loss = epoch_policy_loss / config.ppo_epochs\n",
    "    avg_kl_div = epoch_kl_div / config.ppo_epochs\n",
    "    \n",
    "    # 5. Logging\n",
    "    print(f\"\\n[5/5] Metrics:\")\n",
    "    print(f\"  Policy loss: {avg_policy_loss:.4f}\")\n",
    "    print(f\"  KL div:      {avg_kl_div:.4f}\")\n",
    "    print(f\"  Reward:      {rewards.mean():.3f} ± {rewards.std():.3f}\")\n",
    "    \n",
    "    # Track metrics\n",
    "    metrics_history['epoch'].append(epoch)\n",
    "    metrics_history['policy_loss'].append(avg_policy_loss)\n",
    "    metrics_history['reward_mean'].append(rewards.mean())\n",
    "    metrics_history['reward_std'].append(rewards.std())\n",
    "    metrics_history['kl_div'].append(avg_kl_div)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Reward over time\n",
    "axes[0, 0].plot(metrics_history['epoch'], metrics_history['reward_mean'], 'b-', label='Mean')\n",
    "axes[0, 0].fill_between(\n",
    "    metrics_history['epoch'],\n",
    "    np.array(metrics_history['reward_mean']) - np.array(metrics_history['reward_std']),\n",
    "    np.array(metrics_history['reward_mean']) + np.array(metrics_history['reward_std']),\n",
    "    alpha=0.3\n",
    ")\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Reward')\n",
    "axes[0, 0].set_title('Reward over Training')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Policy loss\n",
    "axes[0, 1].plot(metrics_history['epoch'], metrics_history['policy_loss'], 'r-')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Policy Loss')\n",
    "axes[0, 1].set_title('Policy Loss over Training')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# KL divergence\n",
    "axes[1, 0].plot(metrics_history['epoch'], metrics_history['kl_div'], 'g-')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('KL Divergence')\n",
    "axes[1, 0].set_title('KL Divergence over Training')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Summary statistics\n",
    "axes[1, 1].axis('off')\n",
    "summary_text = f\"\"\"\n",
    "Training Summary:\n",
    "\n",
    "Epochs: {config.num_epochs}\n",
    "Final Reward: {metrics_history['reward_mean'][-1]:.3f}\n",
    "Initial Reward: {metrics_history['reward_mean'][0]:.3f}\n",
    "Improvement: {metrics_history['reward_mean'][-1] - metrics_history['reward_mean'][0]:.3f}\n",
    "\n",
    "Model: {config.model_name}\n",
    "Batch Size: {config.rollout_batch_size}\n",
    "Learning Rate: {config.learning_rate}\n",
    "\"\"\"\n",
    "axes[1, 1].text(0.1, 0.5, summary_text, fontsize=10, verticalalignment='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.output_dir, 'training_metrics.png'))\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Metrics saved to {config.output_dir}/training_metrics.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating on held-out prompts...\\n\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "eval_results = []\n",
    "\n",
    "for prompt in eval_prompts:\n",
    "    # Generate response\n",
    "    responses, _, _ = generate_responses(\n",
    "        [prompt],\n",
    "        model,\n",
    "        tokenizer,\n",
    "        max_new_tokens=config.max_new_tokens,\n",
    "        temperature=0.7,  # Lower temperature for evaluation\n",
    "    )\n",
    "    response = responses[0]\n",
    "    \n",
    "    # Compute reward\n",
    "    reward = reward_computer.compute_reward(prompt, response)\n",
    "    \n",
    "    eval_results.append({\n",
    "        'prompt': prompt,\n",
    "        'response': response,\n",
    "        'reward': reward\n",
    "    })\n",
    "    \n",
    "    print(f\"{'-'*80}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(f\"Reward: {reward:.3f}\")\n",
    "    print()\n",
    "\n",
    "avg_eval_reward = np.mean([r['reward'] for r in eval_results])\n",
    "print(f\"\\nAverage evaluation reward: {avg_eval_reward:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Compare Before/After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Comparing trained model vs reference model:\\n\")\n",
    "\n",
    "test_prompt = \"Explain gradient descent:\"\n",
    "\n",
    "# Trained model\n",
    "model.eval()\n",
    "trained_response, _, _ = generate_responses(\n",
    "    [test_prompt],\n",
    "    model,\n",
    "    tokenizer,\n",
    "    max_new_tokens=config.max_new_tokens,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Reference model (untrained)\n",
    "ref_model.eval()\n",
    "ref_response, _, _ = generate_responses(\n",
    "    [test_prompt],\n",
    "    ref_model,\n",
    "    tokenizer,\n",
    "    max_new_tokens=config.max_new_tokens,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "trained_reward = reward_computer.compute_reward(test_prompt, trained_response[0])\n",
    "ref_reward = reward_computer.compute_reward(test_prompt, ref_response[0])\n",
    "\n",
    "print(f\"Test prompt: {test_prompt}\\n\")\n",
    "print(f\"{'-'*80}\")\n",
    "print(f\"TRAINED MODEL (Reward: {trained_reward:.3f}):\")\n",
    "print(trained_response[0])\n",
    "print(f\"\\n{'-'*80}\")\n",
    "print(f\"REFERENCE MODEL (Reward: {ref_reward:.3f}):\")\n",
    "print(ref_response[0])\n",
    "print(f\"\\n{'-'*80}\")\n",
    "print(f\"\\nImprovement: {trained_reward - ref_reward:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_save_path = os.path.join(config.output_dir, \"trained_model\")\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"✓ Model saved to {model_save_path}\")\n",
    "\n",
    "# Save metrics\n",
    "metrics_path = os.path.join(config.output_dir, \"metrics.json\")\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics_history, f, indent=2, default=float)\n",
    "\n",
    "print(f\"✓ Metrics saved to {metrics_path}\")\n",
    "\n",
    "# Save config\n",
    "config_path = os.path.join(config.output_dir, \"config.json\")\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'model_name': config.model_name,\n",
    "        'num_epochs': config.num_epochs,\n",
    "        'batch_size': config.rollout_batch_size,\n",
    "        'learning_rate': config.learning_rate,\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"✓ Config saved to {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you have a working PPO training pipeline with modular primitives:\n",
    "\n",
    "1. **Customize Rewards**: Modify `CustomRewardComputer` for your specific task\n",
    "   - Add reward model integration\n",
    "   - Implement verification (code execution, math checking)\n",
    "   - Add multi-objective optimization\n",
    "\n",
    "2. **Scale Up with verl**: \n",
    "   - Install verl for production training\n",
    "   - Use vLLM backend for 5-10x faster generation\n",
    "   - Enable FSDP for multi-GPU training\n",
    "   - Use Ray for distributed rollouts\n",
    "\n",
    "3. **Experiment**:\n",
    "   - Try different models (Llama, Mistral, etc.)\n",
    "   - Adjust hyperparameters (learning rate, clip range, KL coef)\n",
    "   - Test different reward functions\n",
    "   - Compare PPO vs GRPO\n",
    "\n",
    "4. **Production**:\n",
    "   - Add proper evaluation suite\n",
    "   - Implement checkpointing and resumption\n",
    "   - Add wandb/tensorboard logging\n",
    "   - Set up continuous evaluation\n",
    "\n",
    "## Resources\n",
    "\n",
    "- verl documentation: https://github.com/volcengine/verl\n",
    "- RL primitives: See `../rl_primitives/` for modular components\n",
    "- Example scripts: See `./verl_ppo_training.py` for full implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
