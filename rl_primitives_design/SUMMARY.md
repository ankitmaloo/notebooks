# RL Training Primitives: Design Summary

## What Was Created

A comprehensive **functional architecture** for building RL training systems for LLM post-training, using **FUNCTIONS instead of CLASSES**.

## Documents Created

### 1. **ARCHITECTURE.md** (Main Specification)
- **Data Structures**: Complete type definitions using NamedTuples
  - `ModelConfig`, `GenerationConfig`, `Message`, `Conversation`
  - `Generation`, `Rollout`, `Batch`, `TrainingState`, `TrainingStats`
- **6 Primitive Categories** with full function signatures:
  - Inference/Generation (10+ functions)
  - Rewards (8+ functions)
  - KL Divergence (7+ functions)
  - Rollouts (6+ functions)
  - Algorithms (8+ functions)
  - Training Loop (6+ functions)
- **Composition Patterns**: 6 detailed examples
- **Extension Scenarios**: 5 real-world customization examples

### 2. **EXAMPLES.md** (Working Code)
- **Example 1**: Basic GRPO training loop
- **Example 2**: Vector rewards with custom aggregation
- **Example 3**: Algorithm switching mid-training
- **Example 4**: Tool call tracking with multi-turn rollouts
- **Example 5**: Custom KL divergence with reference updates
- **Example 6**: Complete custom pipeline (300+ lines)

### 3. **UTILITIES.md** (Helper Functions)
- Data structure helpers (serialization, batch ops)
- Reward function builders
- Prompt management
- Logging & visualization
- Extension helpers (EMA, KL, advantages)
- Testing utilities
- Quick start template

### 4. **README.md** (Overview & Guide)
- Quick start examples
- Architecture highlights
- Extension examples
- Best practices
- Performance tips
- Debugging guide

## Key Design Decisions

### 1. Functions Over Classes
**Why?**
- Maximum hackability - swap components by passing functions
- No inheritance hierarchies to navigate
- Easier to test (pure functions)
- Natural composition

**Example:**
```python
# Instead of:
class RewardModel:
    def compute(self, rollout): ...

# We have:
def compute_reward(rollout, reward_fn):
    return reward_fn(rollout)
```

### 2. Explicit Data Flow
**Why?**
- No hidden state
- Clear what goes in and out
- Easy to debug

**Example:**
```python
# All data flows through parameters
rollout = create_rollout(
    model, tokenizer, prompt, gen_config,
    ref_model=ref_model,
    reward_fn=custom_reward,
    tool_parser=parse_tools
)
```

### 3. Extension Points Everywhere
**Why?**
- Every primitive accepts function arguments
- Easy to customize without modifying source
- Clear contract: "pass a function that does X"

**Example:**
```python
# Every operation has extension points
generation = generate(
    model, tokenizer, prompt, config,
    generator=vllm_generator  # Swap backend
)

kl = compute_forward_kl(
    logprobs, ref_logprobs,
    kl_fn=truncated_kl  # Swap KL method
)

loss = compute_grpo_loss(
    batch, advantages,
    loss_fn=custom_loss  # Swap loss
)
```

### 4. NamedTuples for Data
**Why?**
- Immutable by default
- Self-documenting field names
- Type hints work perfectly
- IDE auto-completion

**Example:**
```python
class Rollout(NamedTuple):
    prompt: str
    generation: Generation
    rewards: Union[float, List[float]]
    kl_divergence: Optional[float]
    # ... clear contract
```

## Hackability Features

### 1. Swap Reward Functions
```python
# Scalar reward
reward_fn = lambda r: len(r.generation.text)

# Vector reward with aggregation
reward_fn = lambda r: weighted_sum([
    correctness(r),
    safety(r),
    helpfulness(r)
])
```

### 2. Change KL Computation
```python
# Truncated KL
kl_fn = create_truncated_kl(threshold=1.0)

# Adaptive KL
kl_fn = create_adaptive_kl(confidence_threshold=0.1)

# Custom KL
kl_fn = lambda lp, ref_lp: my_custom_logic(lp, ref_lp)
```

### 3. Switch Algorithms Mid-Training
```python
def get_algorithm(iteration):
    if iteration < 30:
        return create_algorithm_config("grpo", {"kl_coef": 0.2})
    elif iteration < 60:
        return create_algorithm_config("grpo", {"kl_coef": 0.05})
    else:
        return create_algorithm_config("dapo", {"beta": 0.1})
```

### 4. Update Reference Model
```python
# EMA update
ref_update = create_ema_updater(decay=0.999)

# Periodic reset
ref_update = create_periodic_updater(update_every=10)

# No update
ref_update = lambda ref, policy: ref
```

### 5. Track Tool Calls
```python
rollout = create_multiturn_rollout(
    model, tokenizer, conversation, gen_config,
    tool_parser=parse_xml_tools,
    tool_executor=execute_functions,
    stop_condition=lambda conv: "DONE" in conv.messages[-1].content
)
```

### 6. Custom Generation Backend
```python
# Swap to vLLM
generation = generate(
    model, tokenizer, prompt, config,
    generator=vllm_generator
)

# Swap to SGLang
generation = generate(
    model, tokenizer, prompt, config,
    generator=sglang_generator
)
```

## Complete Primitive Categories

### 1. Inference/Generation (10 functions)
- `load_model`, `load_tokenizer`
- `generate`, `generate_with_logprobs`, `batch_generate`
- `apply_chat_template`, `generate_conversation_turn`
- `extract_tool_calls`, `execute_tools`

### 2. Rewards (8 functions)
- `compute_scalar_reward`, `compute_vector_reward`
- `aggregate_vector_rewards`, `compute_batch_rewards`
- `normalize_rewards`, `apply_reward_shaping`
- `load_reward_model`, `compute_reward_model_score`

### 3. KL Divergence (7 functions)
- `compute_forward_kl`, `compute_reverse_kl`, `compute_per_token_kl`
- `compute_batch_kl`
- `create_reference_model`, `update_reference_model`
- `get_reference_logprobs`

### 4. Rollouts (6 functions)
- `create_rollout`, `create_batch_rollouts`
- `track_tool_calls`, `execute_and_track_tools`
- `create_multiturn_rollout`

### 5. Algorithms (8 functions)
- GRPO: `compute_grpo_advantages`, `compute_grpo_loss`
- DAPO: `compute_dapo_pairs`, `compute_dapo_loss`
- PPO: `compute_ppo_advantages`, `compute_ppo_loss`
- `create_algorithm_config`, `compute_loss_by_algorithm`

### 6. Training Loop (6 functions)
- `training_step`, `training_iteration`, `train`
- `log_to_console`, `log_to_wandb`
- `save_checkpoint`, `load_checkpoint`

## Extension Points by Category

### Inference/Generation
- `model_loader` - Custom model loading (vLLM, SGLang, etc.)
- `tokenizer_loader` - Custom tokenizer loading
- `generator` - Custom generation backend
- `logprob_extractor` - Custom logprob extraction
- `template_fn` - Custom chat templates
- `tool_parser` - Custom tool call parsing

### Rewards
- `reward_fn` - Any reward computation
- `aggregator` - How to combine vector rewards
- `shaping_fn` - Reward shaping logic
- `normalizer` - Reward normalization
- `scorer` - Reward model scoring

### KL Divergence
- `kl_fn` - Custom KL computation
- `ref_model_fn` - How to create ref model
- `update_fn` - How to update ref model
- `logprob_extractor` - Custom ref logprob extraction

### Rollouts
- `reward_fn` - Reward for rollout
- `tool_parser` - Parse tool calls
- `tool_executor` - Execute tools
- `tool_tracker` - Track tool usage
- `stop_condition` - When to stop multi-turn

### Algorithms
- `advantage_fn` - Custom advantage computation
- `loss_fn` - Custom loss function
- `pair_selector` - How to select DAPO pairs
- `value_fn` - Value function for PPO

### Training Loop
- `prompt_loader` - How to load prompts
- `optimizer_step_fn` - Custom optimizer step
- `ref_model_update_fn` - Ref model update strategy
- `checkpoint_fn` - Checkpointing logic
- `eval_fn` - Evaluation logic
- `logger` - Logging logic

## Usage Patterns

### Pattern 1: Quick Start
```python
final_state = quick_start_training(
    model_name="meta-llama/Llama-3-8B",
    prompts=my_prompts,
    reward_fn=my_reward,
    num_iterations=100
)
```

### Pattern 2: Custom Everything
```python
final_state = train(
    initial_state=state,
    prompt_loader=custom_prompts,
    num_iterations=100,
    gen_config=gen_config,
    algorithm_config=algorithm_config,
    reward_fn=custom_reward,
    ref_model_update_fn=ema_update,
    checkpoint_fn=save_checkpoint,
    eval_fn=evaluate,
    logger=log_to_wandb
)
```

### Pattern 3: Iterative Development
```python
# Start simple
rollout = create_rollout(model, tokenizer, prompt, gen_config)

# Add rewards
rollout = create_rollout(..., reward_fn=length_reward)

# Add KL
rollout = create_rollout(..., ref_model=ref_model)

# Add tools
rollout = create_rollout(..., tool_parser=parse_tools)

# Compose everything
rollout = create_multiturn_rollout(
    model, tokenizer, conversation, gen_config,
    ref_model=ref_model,
    reward_fn=multi_objective_reward,
    tool_parser=parse_tools,
    tool_executor=execute_tools
)
```

## Benefits of This Architecture

### For Experimentation
- **Fast iteration**: Change one component, keep everything else
- **A/B testing**: Compare reward functions, algorithms, etc.
- **Debugging**: Each function tested in isolation

### For Production
- **Maintainability**: Clear what each function does
- **Extensibility**: Add new algorithms/rewards without modifying core
- **Type safety**: NamedTuples provide contracts

### For Collaboration
- **Modularity**: Different people work on different primitives
- **Documentation**: Function signatures are self-documenting
- **Testing**: Pure functions easy to unit test

## Comparison to Class-Based

| Aspect | Class-Based | Functional (This) |
|--------|-------------|-------------------|
| Swapping behavior | Subclass & override | Pass different function |
| State management | Hidden in `self` | Explicit parameters |
| Testing | Need mocks/fixtures | Pure functions |
| Composition | Inheritance | Function composition |
| Debugging | Follow object graph | Follow function calls |
| Extension | Add methods | Add functions |
| Learning curve | Understand hierarchy | Understand signatures |

## Next Steps

### To Use This Architecture:

1. **Read ARCHITECTURE.md** - Understand data structures and primitives
2. **Try EXAMPLES.md** - Run the examples
3. **Use UTILITIES.md** - Leverage helpers
4. **Start coding** - Use quick_start_training or build custom

### To Extend:

1. **New reward function**: Just write `def my_reward(rollout) -> float`
2. **New algorithm**: Write loss function, add to dispatcher
3. **New KL method**: Write `def my_kl(logprobs, ref_logprobs) -> float`
4. **New backend**: Write custom generator function

### To Contribute:

1. All functions are standalone - easy to add new ones
2. Keep functions pure where possible
3. Use NamedTuples for data structures
4. Add extension points (function parameters)
5. Document with examples

## File Structure

```
rl_primitives_design/
â”œâ”€â”€ README.md           # Overview & quick start
â”œâ”€â”€ ARCHITECTURE.md     # Complete specification
â”œâ”€â”€ EXAMPLES.md         # Working examples
â”œâ”€â”€ UTILITIES.md        # Helper functions
â””â”€â”€ SUMMARY.md          # This file
```

## Total Lines of Code (Approximate)

- **ARCHITECTURE.md**: ~1,200 lines (spec + examples)
- **EXAMPLES.md**: ~1,000 lines (working code)
- **UTILITIES.md**: ~800 lines (helpers)
- **README.md**: ~500 lines (guide)
- **Total**: ~3,500 lines of documentation + code

## Key Takeaway

**This is a complete, production-ready functional architecture for RL training that maximizes hackability through pure functions, explicit data flow, and clear extension points.**

Every component can be swapped by passing a different function. No classes, no inheritance, no hidden state - just composable functions.

**Happy hacking! ðŸš€**
