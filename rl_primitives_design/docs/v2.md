## Core Architecture

### 1. **Inference Module** (Standard, Reusable)
```python
class InferenceModule:
    """Handles ALL generation - tools, text, everything"""
    def __init__(self, model_name, backend="vllm"):
        self.backend = self._init_backend(model_name, backend)
        self.generation_cache = {}
        
    def generate(self, prompts, **kwargs):
        """Core generation - environment will call this"""
        return self.backend.generate(prompts, **kwargs)
    
    def get_logits(self, prompts, responses):
        """Get logits for KL calculation"""
        return self.backend.get_logits(prompts, responses)
    
    def get_logprobs(self, prompts, responses):
        """Get log probabilities for policy gradient"""
        return self.backend.get_logprobs(prompts, responses)
    
    def batch_generate_with_tools(self, prompts, available_tools=None):
        """Generate with tool access - tools defined by environment"""
        if available_tools:
            # Parse for tool calls in response
            responses = self.generate(prompts)
            tool_calls = self.parse_tool_calls(responses)
            return responses, tool_calls
        return self.generate(prompts), None
```

### 3. **Backprop Module** (Standard, Reusable)
```python
class BackpropModule:
    """Handles all gradient computation and model updates"""
    def __init__(self, model, ref_model=None, optimizer=None):
        self.model = model
        self.ref_model = ref_model or model  # Can be same initially
        self.optimizer = optimizer or Adam(model.parameters())
        
    def compute_advantages(self, rewards, values=None, gamma=0.99, lam=0.95):
        """GAE or simple advantages"""
        if values is None:
            # Simple cumulative reward
            return self.discount_cumsum(rewards, gamma)
        else:
            # GAE
            return self.gae(rewards, values, gamma, lam)
    
    def compute_kl(self, prompts, responses, ref_override=None):
        """KL divergence - can override ref model dynamically"""
        ref_model = ref_override or self.ref_model
        
        curr_logits = self.model.get_logits(prompts, responses)
        ref_logits = ref_model.get_logits(prompts, responses)
        
        return kl_divergence(curr_logits, ref_logits)
    
    def compute_loss(self, batch, advantages, kl_weight=0.1):
        """Combine policy gradient and KL"""
        # Policy gradient loss
        log_probs = self.model.get_logprobs(batch.prompts, batch.responses)
        pg_loss = -(log_probs * advantages).mean()
        
        # KL penalty (optional)
        kl_loss = 0
        if kl_weight > 0:
            kl_loss = self.compute_kl(batch.prompts, batch.responses)
        
        return pg_loss + kl_weight * kl_loss
    
    def update(self, loss):
        """Standard backprop"""
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
    def update_ref_model(self, method="copy"):
        """Update reference model"""
        if method == "copy":
            self.ref_model.load_state_dict(self.model.state_dict())
        elif method == "ema":
            # Exponential moving average
            for param, ref_param in zip(self.model.parameters(), 
                                       self.ref_model.parameters()):
                ref_param.data = 0.99 * ref_param.data + 0.01 * param.data
```

### 4. **Algorithm Wrapper** (Orchestrates Everything)
```python
class RLTrainer:
    """Main training loop - minimal config, sensible defaults"""
    def __init__(
        self, 
        environment: BaseEnvironment,
        backprop: BackpropModule,
        inference: InferenceModule,
        config: dict = None
    ):
        self.env = environment
        self.backprop = backprop
        self.inference = inference
        self.config = config or self.default_config()
        
    def default_config(self):
        return {
            "batch_size": 32,
            "max_steps": 50,
            "parallel_rollouts": 16,
            "kl_weight": 0.1,
            "update_ref_every": 100,
            "algorithm": "ppo"  # or "reinforce", "grpo"
        }
    
    def run_rollout(self):
        """Single rollout - environment handles complexity"""
        state = self.env.reset()
        trajectory = []
        
        for step in range(self.config["max_steps"]):
            state = self.env.step(state)
            trajectory.append(state)
            
            if state.done:
                break
        
        # Environment computes rewards
        rewards = self.env.compute_reward(trajectory)
        return trajectory, rewards
    
    def train(self, num_iterations):
        """Main training loop"""
        for i in range(num_iterations):
            # Collect rollouts
            rollouts = []
            for _ in range(self.config["parallel_rollouts"]):
                trajectory, rewards = self.run_rollout()
                rollouts.append((trajectory, rewards))
            
            # Compute advantages
            all_rewards = [r for _, r in rollouts]
            advantages = self.backprop.compute_advantages(all_rewards)
            
            # Create batch
            batch = self.create_batch(rollouts)
            
            # Compute loss and update
            loss = self.backprop.compute_loss(
                batch, 
                advantages, 
                kl_weight=self.config["kl_weight"]
            )
            self.backprop.update(loss)
            
            # Update reference model periodically
            if i % self.config["update_ref_every"] == 0:
                self.backprop.update_ref_model()
```

## Usage Pattern - Your Experiments


```python
# Monday's experiment
trainer = RLTrainer(environment=ParetoEnv(...), ...)

# Tuesday's experiment - just change environment!
trainer = RLTrainer(environment=SVRLEnv(...), ...)

# Wednesday - combine them!
class ParetoPlusSVRLEnv(ParetoEnvironment, SVRLEnvironment):
    # Mix both capabilities
    pass
```

This architecture lets you focus on what matters - your experimental logic in the environment - while the boring stuff (inference, backprop) just works. Is this closer to what you had in mind?

## (rewards and rollouts corrected in env)

### 1. **Environment** (Just the Interaction Logic)
```python
class BaseEnvironment:
    """Pure environment logic - no reward computation"""
    def __init__(self, inference: InferenceModule):
        self.inference = inference
        
    def reset(self) -> State:
        """Initialize a new episode"""
        raise NotImplementedError
        
    def step(self, state: State) -> State:
        """Single step of interaction - just state transitions"""
        raise NotImplementedError
    
    def is_terminal(self, state: State) -> bool:
        """Check if this trajectory should stop"""
        raise NotImplementedError

# Your custom environments just define interactions
class SVRLEnvironment(BaseEnvironment):
    def __init__(self, inference, verification_tools):
        super().__init__(inference)
        self.tools = verification_tools
        
    def step(self, state):
        # Just the interaction logic
        prompt = self.build_prompt(state)
        response, tool_calls = self.inference.generate_with_tools(prompt, self.tools)
        
        # Process tool calls and update state
        for call in tool_calls:
            if state.can_afford_verification(call.cost):
                result = self.execute_tool(call)
                state.verification_history.append((call, result))
                state.budget -= call.cost
        
        state.response = response
        state.step_count += 1
        return state
    
    def is_terminal(self, state):
        return (state.budget <= 0 or 
                state.step_count >= 50 or 
                "DONE" in state.response)
```

### 2. **RolloutManager** (Handles Collection & Synchronization)
```python
class RolloutManager:
    """Manages parallel rollouts with different completion times"""
    def __init__(self, env: BaseEnvironment, num_parallel: int = 32):
        self.env = env
        self.num_parallel = num_parallel
        self.active_rollouts = {}
        self.completed_buffer = []
        
    def initialize_batch(self) -> Dict[int, State]:
        """Start a new batch of rollouts"""
        self.active_rollouts = {
            i: self.env.reset() 
            for i in range(self.num_parallel)
        }
        self.completed_buffer = []
        return self.active_rollouts
    
    def step_active(self) -> Dict[str, List]:
        """Step all active rollouts, handle completions"""
        if not self.active_rollouts:
            return {"active": [], "just_completed": []}
        
        # Batch all active states for efficient inference
        active_ids = list(self.active_rollouts.keys())
        active_states = [self.active_rollouts[i] for i in active_ids]
        
        # Environment steps all at once (batched inference)
        new_states = self.batch_step(active_states)
        
        just_completed = []
        still_active = {}
        
        # Check which ones are done
        for rollout_id, new_state in zip(active_ids, new_states):
            if self.env.is_terminal(new_state):
                just_completed.append((rollout_id, new_state))
                self.completed_buffer.append(new_state)
            else:
                still_active[rollout_id] = new_state
        
        self.active_rollouts = still_active
        
        return {
            "active": list(still_active.keys()),
            "just_completed": just_completed
        }
    
    def batch_step(self, states: List[State]) -> List[State]:
        """Step multiple states efficiently"""
        # This is where batching happens for efficiency
        prompts = [self.env.build_prompt(s) for s in states]
        responses = self.env.inference.generate(prompts)
        
        new_states = []
        for state, response in zip(states, responses):
            new_state = state.copy()
            new_state.response = response
            new_state.step_count += 1
            # Environment-specific state updates
            new_state = self.env.update_state(new_state, response)
            new_states.append(new_state)
            
        return new_states
    
    def collect_rollouts(self, min_trajectories: int) -> List[State]:
        """Collect rollouts until we have enough trajectories"""
        all_trajectories = []
        
        while len(all_trajectories) < min_trajectories:
            # Initialize new batch
            self.initialize_batch()
            
            # Run until all complete (with different lengths)
            step_stats = {"step": 0, "active": self.num_parallel}
            
            while self.active_rollouts:
                step_info = self.step_active()
                
                # Log completion info
                if step_info["just_completed"]:
                    for rid, state in step_info["just_completed"]:
                        print(f"Rollout {rid} completed at step {state.step_count}")
                        all_trajectories.append(state)
                
                # Optional: Start new rollouts to replace completed ones
                if self.maintain_parallel_count:
                    for _ in range(len(step_info["just_completed"])):
                        new_id = max(self.active_rollouts.keys()) + 1 if self.active_rollouts else 0
                        self.active_rollouts[new_id] = self.env.reset()
                
                step_stats["step"] += 1
                step_stats["active"] = len(self.active_rollouts)
        
        return all_trajectories
```

### 3. **RewardComputer** (Separate from Environment)
```python
class RewardComputer:
    """Compute rewards AFTER collecting trajectories"""
    
    def compute_rewards(self, trajectories: List[State], method: str = "absolute") -> np.ndarray:
        """Compute rewards for a batch of completed trajectories"""
        if method == "absolute":
            return self.absolute_rewards(trajectories)
        elif method == "relative":  # For GRPO
            return self.relative_rewards(trajectories)
        elif method == "pareto":
            return self.pareto_rewards(trajectories)
    
    def relative_rewards(self, trajectories: List[State]) -> np.ndarray:
        """GRPO-style relative ranking"""
        # Score each trajectory
        scores = np.array([self.score_trajectory(t) for t in trajectories])
        
        # Rank and normalize
        ranks = scipy.stats.rankdata(scores)
        normalized_ranks = (ranks - ranks.mean()) / (ranks.std() + 1e-8)
        
        return normalized_ranks
    
    def pareto_rewards(self, trajectories: List[State]) -> np.ndarray:
        """Multi-objective Pareto ranking"""
        # Get multi-dimensional scores
        objectives = np.array([
            [self.obj1(t), self.obj2(t), self.obj3(t)] 
            for t in trajectories
        ])
        
        # Compute Pareto ranks
        pareto_ranks = self.compute_pareto_ranks(objectives)
        return pareto_ranks
    
    def score_trajectory(self, trajectory: State) -> float:
        """Score a single trajectory - override this"""
        raise NotImplementedError

# Specific reward computers for your experiments
class SVRLRewardComputer(RewardComputer):
    def score_trajectory(self, trajectory: State) -> float:
        # Your scoring logic
        task_reward = self.evaluate_task_completion(trajectory)
        efficiency_penalty = -0.1 * trajectory.verification_cost
        return task_reward + efficiency_penalty

class ParetoRewardComputer(RewardComputer):
    def score_trajectory(self, trajectory: State) -> np.ndarray:
        return np.array([
            self.accuracy_score(trajectory),
            self.efficiency_score(trajectory),
            self.safety_score(trajectory)
        ])
```

### 4. **Training Algorithm** (Orchestrates Everything)
```python
class RLAlgorithm:
    """Base training algorithm"""
    def __init__(
        self,
        env: BaseEnvironment,
        rollout_manager: RolloutManager,
        reward_computer: RewardComputer,
        backprop: BackpropModule,
        config: dict = None
    ):
        self.env = env
        self.rollout_manager = rollout_manager
        self.reward_computer = reward_computer
        self.backprop = backprop
        self.config = config or {}
        
    def collect_and_score(self, num_trajectories: int):
        """Collect trajectories and compute rewards"""
        # Collect trajectories (handles different lengths)
        trajectories = self.rollout_manager.collect_rollouts(num_trajectories)
        
        # Group by length for analysis
        by_length = defaultdict(list)
        for t in trajectories:
            by_length[t.step_count].append(t)
        
        print(f"Trajectory length distribution: {dict(by_length.keys())}")
        
        # Compute rewards AFTER collection
        rewards = self.reward_computer.compute_rewards(
            trajectories, 
            method=self.config.get("reward_method", "relative")
        )
        
        return trajectories, rewards

class GRPOAlgorithm(RLAlgorithm):
    """GRPO specific - relative ranking"""
    def train_step(self):
        # Collect batch of trajectories
        trajectories, rewards = self.collect_and_score(
            self.config.get("batch_size", 32)
        )
        
        # GRPO uses relative rewards
        relative_rewards = self.reward_computer.compute_rewards(
            trajectories, 
            method="relative"
        )
        
        # Only update on better half
        threshold = np.percentile(relative_rewards, 50)
        good_trajectories = [
            t for t, r in zip(trajectories, relative_rewards) 
            if r > threshold
        ]
        
        # Compute loss only on good trajectories
        loss = self.backprop.compute_loss(good_trajectories, relative_rewards)
        self.backprop.update(loss)

class PPOAlgorithm(RLAlgorithm):
    """PPO - uses advantages"""
    def train_step(self):
        trajectories, rewards = self.collect_and_score(
            self.config.get("batch_size", 32)
        )
        
        # Compute advantages
        advantages = self.backprop.compute_advantages(rewards)
        
        # Multiple epochs on same data
        for _ in range(self.config.get("ppo_epochs", 4)):
            loss = self.backprop.compute_ppo_loss(trajectories, advantages)
            self.backprop.update(loss)
```

## Usage: Your Experiments

### Setup for Pareto Multi-Objective
```python
# 1. Environment just defines interactions
class MyParetoEnv(BaseEnvironment):
    def step(self, state):
        # Just interaction logic - NO rewards
        response = self.inference.generate(state.prompt)
        state.response = response
        state.step_count += 1
        return state
    
    def is_terminal(self, state):
        return state.step_count >= 30 or "COMPLETE" in state.response

# 2. Rollout manager handles collection
rollout_mgr = RolloutManager(
    env=MyParetoEnv(inference),
    num_parallel=32  # Run 32 trajectories in parallel
)

# 3. Reward computer handles scoring AFTER collection
class MyParetoRewards(RewardComputer):
    def compute_rewards(self, trajectories, method="pareto"):
        # Compare all trajectories against each other
        all_objectives = np.array([
            [self.accuracy(t), self.efficiency(t), self.safety(t)]
            for t in trajectories
        ])
        
        # Find Pareto frontier
        pareto_frontier = self.find_pareto_frontier(all_objectives)
        
        # Reward based on Pareto dominance
        rewards = np.zeros(len(trajectories))
        for i, obj in enumerate(all_objectives):
            if i in pareto_frontier:
                rewards[i] = 1.0  # On frontier
            else:
                # Distance to frontier
                rewards[i] = -self.distance_to_frontier(obj, pareto_frontier)
        
        return rewards

# 4. Train
algo = PPOAlgorithm(
    env=MyParetoEnv(inference),
    rollout_manager=rollout_mgr,
    reward_computer=MyParetoRewards(),
    backprop=backprop
)
```

### Setup for SVRL with Variable Lengths
```python
class MySVRLEnv(BaseEnvironment):
    def step(self, state):
        # Model decides whether to verify
        verify_prompt = f"{state.prompt}\nBudget: {state.budget}\nVerify? (y/n/done)"
        response = self.inference.generate(verify_prompt)
        
        if "VERIFY" in response:
            # Pay cost and get info
            state.budget -= self.verify_cost
            state.verified_info = self.get_verification()
        elif "DONE" in response:
            state.complete = True
        
        state.response = response
        return state
    
    def is_terminal(self, state):
        return state.complete or state.budget <= 0 or state.step_count >= 50

class SVRLRewards(RewardComputer):
    def compute_rewards(self, trajectories, method="relative"):
        # Score based on task completion vs verification cost
        scores = []
        for t in trajectories:
            task_score = self.eval_task_completion(t)
            efficiency = 1.0 - (t.verification_spent / t.initial_budget)
            
            # Bonus for early completion
            speed_bonus = max(0, (50 - t.step_count) / 50)
            
            scores.append(task_score * efficiency + 0.1 * speed_bonus)
        
        if method == "relative":
            # Rank against each other (GRPO style)
            ranks = scipy.stats.rankdata(scores)
            return (ranks - ranks.mean()) / ranks.std()
        else:
            return np.array(scores)
```

## Key Advantages

1. **Clean Separation**: Environment doesn't know about rewards. It just handles state transitions.

2. **Flexible Trajectory Collection**: RolloutManager handles different trajectory lengths naturally.

3. **Batch Reward Computation**: Can compare trajectories against each other (essential for GRPO).

4. **Reusable Components**: Same RolloutManager and BackpropModule across experiments.

5. **Natural Parallelism**: Collect many trajectories in parallel, compute rewards once.

This structure makes it easy to experiment with different reward schemes without changing your environment logic, and handles variable-length trajectories naturally. Does this better match your mental model?