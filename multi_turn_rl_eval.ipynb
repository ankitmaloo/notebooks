{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Turn RL Evaluation\n",
    "\n",
    "Framework for evaluating multi-turn RL rollouts where:\n",
    "- Tasks run for ~50 steps (some terminate early)\n",
    "- Every step is recorded for each rollout\n",
    "- Rewards assigned at the end\n",
    "- Data collected for backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from typing import List, Dict, Union, Optional, Any, Tuple\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import pickle\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMGenerator:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, dtype=\"auto\", device_map=\"auto\")\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"left\"  # decoder-only: left padding\n",
    "        self.think_start_id = 151667  # <think>\n",
    "        self.think_end_id = 151668    # </think>\n",
    "\n",
    "    def generate(self,\n",
    "                 prompts: Union[str, List[str]],\n",
    "                 max_new_tokens: int = 512,\n",
    "                 temperature: float = 0.6,\n",
    "                 num_return_sequences: int = 1,\n",
    "                 enable_thinking: bool = True,\n",
    "                 return_thinking: bool = True,\n",
    "                 **kwargs\n",
    "                 ) -> Union[str, List[str]]:\n",
    "\n",
    "        single_prompt = isinstance(prompts[0], dict)\n",
    "\n",
    "        print(\"single prompt\", single_prompt)\n",
    "\n",
    "        if single_prompt:\n",
    "            prompts = [prompts]\n",
    "\n",
    "        # Apply chat template with thinking enabled/disabled\n",
    "        texts = [\n",
    "            self.tokenizer.apply_chat_template(\n",
    "                prompt,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "                enable_thinking=enable_thinking\n",
    "            )\n",
    "            for prompt in prompts\n",
    "        ]\n",
    "\n",
    "        model_inputs = self.tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **model_inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=temperature > 0,\n",
    "                num_return_sequences=num_return_sequences,\n",
    "                **kwargs\n",
    "            )\n",
    "\n",
    "        results = self._decode_batch(\n",
    "            outputs,\n",
    "            model_inputs.input_ids,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            return_thinking=return_thinking\n",
    "        )\n",
    "\n",
    "        return results[0] if single_prompt else results\n",
    "\n",
    "    def _decode_batch(self, outputs, input_ids, num_return_sequences, return_thinking):\n",
    "        batch_size = input_ids.shape[0]\n",
    "        prompt_lens = [input_ids[i].shape[0] for i in range(batch_size)]\n",
    "\n",
    "        # Reshape to [batch_size, num_return_sequences, seq_len]\n",
    "        outputs = outputs.view(batch_size, num_return_sequences, -1)\n",
    "\n",
    "        results = []\n",
    "        for batch_idx in range(batch_size):\n",
    "            prompt_len = prompt_lens[batch_idx]\n",
    "            batch_results = []\n",
    "\n",
    "            for seq_idx in range(num_return_sequences):\n",
    "                # Get full generated sequence for this sample\n",
    "                full_seq = outputs[batch_idx, seq_idx]\n",
    "\n",
    "                # Slice off prompt tokens to get model's output only\n",
    "                output_ids = full_seq[prompt_len:].tolist()\n",
    "\n",
    "                if return_thinking:\n",
    "                    thinking, content = self._parse_thinking(output_ids)\n",
    "                    # Return tuple of (thinking, content)\n",
    "                    batch_results.append([thinking, content])\n",
    "                else:\n",
    "                    # Standard: decode everything after prompt\n",
    "                    content = self.tokenizer.decode(\n",
    "                        full_seq[prompt_len:],\n",
    "                        skip_special_tokens=True\n",
    "                    ).strip()\n",
    "                    batch_results.append(content)\n",
    "\n",
    "            # If num_return_sequences=1, unwrap the list\n",
    "            results.append(batch_results[0] if num_return_sequences == 1 else batch_results)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _parse_thinking(self, output_ids: List[int]) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Split thinking and content at the </think> token.\n",
    "        Uses reverse index trick to find last occurrence.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Find last </think> token (handles nested <think> tags)\n",
    "            # output_ids[::-1] creates reversed list\n",
    "            # .index() finds first occurrence in reversed = last in original\n",
    "            think_end_idx = len(output_ids) - output_ids[::-1].index(self.think_end_id)\n",
    "\n",
    "            # Include </think> in thinking part\n",
    "            thinking_ids = output_ids[:think_end_idx]\n",
    "            content_ids = output_ids[think_end_idx:]\n",
    "        except ValueError:\n",
    "            # No </think> token found - model skipped thinking\n",
    "            thinking_ids = []\n",
    "            content_ids = output_ids\n",
    "\n",
    "        thinking = self.tokenizer.decode(thinking_ids, skip_special_tokens=True)\n",
    "        content = self.tokenizer.decode(content_ids, skip_special_tokens=True)\n",
    "\n",
    "        return thinking.strip(), content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "source": "import re\n\n\nclass ToolUseLLM:\n    \"\"\"\n    Tool-using LLM with Qwen's Hermes-style function calling.\n    \n    Answers two key questions:\n    1. How do you know if there's a tool call? → Check for <tool_call> tags\n    2. How do you execute efficiently? → Parse JSON and use **kwargs\n    \"\"\"\n    \n    def __init__(self, model_name: str):\n        \"\"\"Initialize model with tool calling support\"\"\"\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            dtype=\"auto\",\n            device_map=\"auto\"\n        )\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n    \n    def has_tool_call(self, response_text: str) -> bool:\n        \"\"\"\n        Q1: How do you know if the LLM responded with a tool call?\n        A: Check if response contains <tool_call>...</tool_call> tags\n        \n        Qwen3 (Hermes-style) outputs:\n        <tool_call>\n        {\"name\": \"function_name\", \"arguments\": {\"arg\": \"value\"}}\n        </tool_call>\n        \"\"\"\n        return '<tool_call>' in response_text and '</tool_call>' in response_text\n    \n    def execute_tool_call(\n        self, \n        tool_call: Dict[str, Any], \n        tool_implementations: Dict[str, callable]\n    ) -> str:\n        \"\"\"\n        Q2: How do you execute the tool call with supplied parameters efficiently?\n        A: Parse JSON arguments and use **kwargs unpacking\n        \n        Steps:\n        1. Extract function name and arguments from tool_call\n        2. If arguments are JSON string, parse them\n        3. Call function with **kwargs: func(**args)\n        \n        Example:\n            tool_call = {\n                \"function\": {\n                    \"name\": \"get_weather\",\n                    \"arguments\": {\"location\": \"SF\", \"unit\": \"celsius\"}\n                }\n            }\n            \n            # Executes: get_weather(location=\"SF\", unit=\"celsius\")\n        \"\"\"\n        tool_name = tool_call[\"function\"][\"name\"]\n        tool_args = tool_call[\"function\"][\"arguments\"]\n        \n        # Step 1: Validate tool exists\n        if tool_name not in tool_implementations:\n            return f\"Tool {tool_name} not found\"\n        \n        # Step 2: Parse arguments if they're a JSON string\n        if isinstance(tool_args, str):\n            try:\n                tool_args = json.loads(tool_args)\n            except json.JSONDecodeError as e:\n                return f\"Error parsing arguments: {str(e)}\"\n        \n        # Step 3: Execute with **kwargs unpacking (EFFICIENT!)\n        try:\n            result = tool_implementations[tool_name](**tool_args)\n            return json.dumps(result) if not isinstance(result, str) else result\n        except Exception as e:\n            return f\"Error executing {tool_name}: {str(e)}\"\n    \n    def run(\n        self,\n        messages: List[Dict[str, str]],\n        tools: List[Dict[str, Any]],\n        tool_implementations: Dict[str, callable],\n        max_turns: int = 10,\n        max_new_tokens: int = 1024,\n        temperature: float = 0.6\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Run multi-turn agent loop with tool calling.\n        \n        Args:\n            messages: Initial conversation history\n            tools: Tool definitions in OpenAI format\n            tool_implementations: Dict mapping tool names to functions\n            max_turns: Maximum agent turns\n            max_new_tokens: Max tokens per generation\n            temperature: Sampling temperature\n        \n        Returns:\n            {\n                \"messages\": full conversation history,\n                \"final_response\": last assistant message,\n                \"tool_calls\": list of all tool calls made,\n                \"turns\": number of turns taken\n            }\n        \"\"\"\n        conversation = deepcopy(messages)\n        all_tool_calls = []\n        \n        for turn in range(max_turns):\n            # Generate response with tools in context\n            prompt = self.tokenizer.apply_chat_template(\n                conversation,\n                tools=tools,\n                tokenize=False,\n                add_generation_prompt=True\n            )\n            \n            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n            \n            with torch.no_grad():\n                outputs = self.model.generate(\n                    **inputs,\n                    max_new_tokens=max_new_tokens,\n                    temperature=temperature,\n                    do_sample=temperature > 0,\n                    pad_token_id=self.tokenizer.pad_token_id\n                )\n            \n            response_ids = outputs[0][inputs.input_ids.shape[1]:]\n            response_text = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n            \n            # Q1: Check if response has tool calls\n            if not self.has_tool_call(response_text):\n                # No tool calls → final response\n                conversation.append({\n                    \"role\": \"assistant\",\n                    \"content\": response_text\n                })\n                return {\n                    \"messages\": conversation,\n                    \"final_response\": response_text,\n                    \"tool_calls\": all_tool_calls,\n                    \"turns\": turn + 1\n                }\n            \n            # Parse tool calls\n            tool_calls = self._parse_tool_calls(response_text)\n            \n            conversation.append({\n                \"role\": \"assistant\",\n                \"content\": response_text,\n                \"tool_calls\": tool_calls\n            })\n            \n            # Q2: Execute each tool call efficiently\n            for tool_call in tool_calls:\n                all_tool_calls.append(tool_call)\n                \n                result_str = self.execute_tool_call(tool_call, tool_implementations)\n                \n                # Add tool result to conversation\n                conversation.append({\n                    \"role\": \"tool\",\n                    \"name\": tool_call[\"function\"][\"name\"],\n                    \"content\": result_str\n                })\n        \n        return {\n            \"messages\": conversation,\n            \"final_response\": \"Max turns reached\",\n            \"tool_calls\": all_tool_calls,\n            \"turns\": max_turns\n        }\n    \n    def _parse_tool_calls(self, response_text: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        Parse Hermes-style tool calls from response text.\n        \n        Format: <tool_call>{\"name\": \"...\", \"arguments\": {...}}</tool_call>\n        \n        Returns OpenAI-format tool call dicts\n        \"\"\"\n        tool_calls = []\n        \n        pattern = r'<tool_call>\\s*(\\{.*?\\})\\s*</tool_call>'\n        matches = re.findall(pattern, response_text, re.DOTALL)\n        \n        for i, match in enumerate(matches):\n            try:\n                tool_data = json.loads(match)\n                \n                tool_call = {\n                    \"id\": f\"call_{i}\",\n                    \"type\": \"function\",\n                    \"function\": {\n                        \"name\": tool_data.get(\"name\", \"\"),\n                        \"arguments\": tool_data.get(\"arguments\", {})\n                    }\n                }\n                tool_calls.append(tool_call)\n            except json.JSONDecodeError:\n                continue\n        \n        return tool_calls",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_templates(prompts):\n",
    "    \"\"\"Returns an array of prompts. Array of 1 prompt if only one.\"\"\"\n",
    "    gen_p = [{\"role\": \"user\", \"content\": p} for p in prompts]\n",
    "    return gen_p\n",
    "\n",
    "\n",
    "def batch_history(history, prompts):\n",
    "    \"\"\"returns a batch of history + prompts based on the number of prompts\"\"\"\n",
    "    print(len(prompts))\n",
    "    batch = []\n",
    "    templated_prompts = generate_templates(prompts)\n",
    "    for i in range(len(templated_prompts)):\n",
    "        k = deepcopy(history)\n",
    "        k.append(templated_prompts[i])\n",
    "        batch.append(k)\n",
    "    return batch\n",
    "\n",
    "\n",
    "def create_batch(bh, num_gen, outputs, include_thinking=False):\n",
    "    \"\"\"flattens a prompts x generations x content array to (prompts * generations) x content array\"\"\"\n",
    "    batch_size = len(bh)\n",
    "    bh_history = []\n",
    "    bh_wthinking = []\n",
    "    for i in range(batch_size):\n",
    "        _wo_thinking = deepcopy(bh[i])\n",
    "        _thinking = deepcopy(bh[i])\n",
    "        if num_gen > 1:  # 3d array\n",
    "            for j in range(num_gen):\n",
    "                if include_thinking:\n",
    "                    _thinking.append({\"role\": \"assistant\", \"content\": outputs[i][j]})\n",
    "\n",
    "                _wo_thinking.append({\"role\": \"assistant\", \"content\": outputs[i][j][1]})\n",
    "                bh_history.append(_wo_thinking)\n",
    "                bh_wthinking.append(_thinking)\n",
    "        else:  # 2d array\n",
    "            a = deepcopy(bh[i])\n",
    "            if include_thinking:\n",
    "                _thinking.append({\"role\": \"assistant\", \"content\": outputs[i]})\n",
    "\n",
    "            _wo_thinking.append({\"role\": \"assistant\", \"content\": outputs[i][1]})\n",
    "            bh_history.append(_wo_thinking)\n",
    "            bh_wthinking.append(_thinking)\n",
    "    return bh_history, bh_wthinking\n",
    "\n",
    "\n",
    "def res(generator, batch, max_new_tokens=1000, num_return_sequences=1):\n",
    "    outputs = generator.generate(batch, max_new_tokens=max_new_tokens, num_return_sequences=num_return_sequences)\n",
    "    gens_with_history, gens_w_thinking = create_batch(batch, num_return_sequences, outputs, include_thinking=True)\n",
    "    return outputs, gens_with_history, gens_w_thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class StepData:\n",
    "    \"\"\"Data for a single step in a rollout\"\"\"\n",
    "    step_num: int\n",
    "    prompt: str\n",
    "    thinking: str\n",
    "    content: str\n",
    "    history_wo_thinking: List[Dict[str, str]]\n",
    "    history_w_thinking: List[Dict[str, str]]\n",
    "    tokens: List[int] = field(default_factory=list)  # Token IDs for backprop\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RolloutData:\n",
    "    \"\"\"Complete data for a single rollout\"\"\"\n",
    "    rollout_id: int\n",
    "    initial_prompt: str\n",
    "    steps: List[StepData] = field(default_factory=list)\n",
    "    is_complete: bool = False\n",
    "    terminated_early: bool = False\n",
    "    termination_step: int = -1\n",
    "    reward: float = 0.0\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    def add_step(self, step_data: StepData):\n",
    "        self.steps.append(step_data)\n",
    "\n",
    "    def get_total_steps(self) -> int:\n",
    "        return len(self.steps)\n",
    "\n",
    "    def mark_complete(self, early: bool = False):\n",
    "        self.is_complete = True\n",
    "        self.terminated_early = early\n",
    "        self.termination_step = len(self.steps)\n",
    "\n",
    "    def set_reward(self, reward: float):\n",
    "        self.reward = reward\n",
    "\n",
    "    def to_dict(self):\n",
    "        return asdict(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutManager:\n",
    "    \"\"\"Manages multiple parallel rollouts for multi-turn RL evaluation\"\"\"\n",
    "\n",
    "    def __init__(self, generator: LLMGenerator, n_rollouts: int, max_steps: int = 50):\n",
    "        self.generator = generator\n",
    "        self.n_rollouts = n_rollouts\n",
    "        self.max_steps = max_steps\n",
    "        self.rollouts: List[RolloutData] = []\n",
    "        self.active_rollout_indices: List[int] = []\n",
    "\n",
    "    def initialize_rollouts(self, initial_prompts: List[str]):\n",
    "        \"\"\"Initialize n rollouts with initial prompts\"\"\"\n",
    "        assert len(initial_prompts) == self.n_rollouts, \"Number of prompts must match n_rollouts\"\n",
    "\n",
    "        self.rollouts = [\n",
    "            RolloutData(\n",
    "                rollout_id=i,\n",
    "                initial_prompt=initial_prompts[i],\n",
    "                metadata={\"created_at\": datetime.now().isoformat()}\n",
    "            )\n",
    "            for i in range(self.n_rollouts)\n",
    "        ]\n",
    "        self.active_rollout_indices = list(range(self.n_rollouts))\n",
    "        print(f\"Initialized {self.n_rollouts} rollouts\")\n",
    "\n",
    "    def execute_step(self, step_num: int, prompts: List[str], max_new_tokens: int = 1000):\n",
    "        \"\"\"Execute a single step for all active rollouts\"\"\"\n",
    "        if not self.active_rollout_indices:\n",
    "            print(\"No active rollouts\")\n",
    "            return\n",
    "\n",
    "        # Get active rollouts\n",
    "        active_rollouts = [self.rollouts[i] for i in self.active_rollout_indices]\n",
    "\n",
    "        # Build histories for each active rollout\n",
    "        histories = []\n",
    "        for rollout in active_rollouts:\n",
    "            if len(rollout.steps) == 0:\n",
    "                # First step: empty history\n",
    "                histories.append([])\n",
    "            else:\n",
    "                # Use the last step's history without thinking\n",
    "                histories.append(rollout.steps[-1].history_wo_thinking)\n",
    "\n",
    "        # Create batch with new prompts\n",
    "        batch = []\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            hist = deepcopy(histories[i])\n",
    "            hist.append({\"role\": \"user\", \"content\": prompt})\n",
    "            batch.append(hist)\n",
    "\n",
    "        # Generate outputs (num_return_sequences=1 as per requirement)\n",
    "        outputs, gens_with_history, gens_w_thinking = res(\n",
    "            self.generator,\n",
    "            batch,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "\n",
    "        # Store step data for each active rollout\n",
    "        for i, rollout_idx in enumerate(self.active_rollout_indices):\n",
    "            thinking, content = outputs[i]  # outputs[i] is [thinking, content]\n",
    "\n",
    "            step_data = StepData(\n",
    "                step_num=step_num,\n",
    "                prompt=prompts[i],\n",
    "                thinking=thinking,\n",
    "                content=content,\n",
    "                history_wo_thinking=gens_with_history[i],\n",
    "                history_w_thinking=gens_w_thinking[i],\n",
    "                tokens=[]  # Will be populated if needed for backprop\n",
    "            )\n",
    "\n",
    "            self.rollouts[rollout_idx].add_step(step_data)\n",
    "\n",
    "        print(f\"Step {step_num} completed for {len(self.active_rollout_indices)} rollouts\")\n",
    "\n",
    "    def check_termination_conditions(self, termination_fn=None) -> List[int]:\n",
    "        \"\"\"Check which rollouts should terminate. Returns indices of rollouts to terminate.\"\"\"\n",
    "        to_terminate = []\n",
    "\n",
    "        for rollout_idx in self.active_rollout_indices:\n",
    "            rollout = self.rollouts[rollout_idx]\n",
    "\n",
    "            # Check max steps\n",
    "            if rollout.get_total_steps() >= self.max_steps:\n",
    "                to_terminate.append(rollout_idx)\n",
    "                rollout.mark_complete(early=False)\n",
    "                continue\n",
    "\n",
    "            # Custom termination function\n",
    "            if termination_fn and rollout.steps:\n",
    "                last_step = rollout.steps[-1]\n",
    "                if termination_fn(rollout, last_step):\n",
    "                    to_terminate.append(rollout_idx)\n",
    "                    rollout.mark_complete(early=True)\n",
    "\n",
    "        # Remove terminated rollouts from active list\n",
    "        for idx in to_terminate:\n",
    "            self.active_rollout_indices.remove(idx)\n",
    "\n",
    "        if to_terminate:\n",
    "            print(f\"Terminated {len(to_terminate)} rollouts. {len(self.active_rollout_indices)} still active\")\n",
    "\n",
    "        return to_terminate\n",
    "\n",
    "    def run_rollouts(self, prompt_generator_fn, termination_fn=None, max_new_tokens: int = 1000):\n",
    "        \"\"\"\n",
    "        Run all rollouts until completion.\n",
    "\n",
    "        Args:\n",
    "            prompt_generator_fn: Function that takes (rollout_data, step_num) and returns next prompt\n",
    "            termination_fn: Optional function that takes (rollout_data, step_data) and returns bool\n",
    "            max_new_tokens: Max tokens to generate per step\n",
    "        \"\"\"\n",
    "        step_num = 0\n",
    "\n",
    "        while self.active_rollout_indices and step_num < self.max_steps:\n",
    "            # Generate prompts for active rollouts\n",
    "            prompts = []\n",
    "            for rollout_idx in self.active_rollout_indices:\n",
    "                rollout = self.rollouts[rollout_idx]\n",
    "                prompt = prompt_generator_fn(rollout, step_num)\n",
    "                prompts.append(prompt)\n",
    "\n",
    "            # Execute step\n",
    "            self.execute_step(step_num, prompts, max_new_tokens)\n",
    "\n",
    "            # Check termination\n",
    "            self.check_termination_conditions(termination_fn)\n",
    "\n",
    "            step_num += 1\n",
    "\n",
    "        # Mark any remaining active rollouts as complete\n",
    "        for rollout_idx in self.active_rollout_indices:\n",
    "            self.rollouts[rollout_idx].mark_complete(early=False)\n",
    "\n",
    "        print(f\"\\nAll rollouts complete. Total steps: {step_num}\")\n",
    "\n",
    "    def assign_rewards(self, reward_fn):\n",
    "        \"\"\"Assign rewards to all rollouts using provided reward function\"\"\"\n",
    "        for rollout in self.rollouts:\n",
    "            reward = reward_fn(rollout)\n",
    "            rollout.set_reward(reward)\n",
    "        print(f\"Rewards assigned to {len(self.rollouts)} rollouts\")\n",
    "\n",
    "    def get_rollout_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary statistics of all rollouts\"\"\"\n",
    "        total_steps = [r.get_total_steps() for r in self.rollouts]\n",
    "        early_terminations = sum(1 for r in self.rollouts if r.terminated_early)\n",
    "        rewards = [r.reward for r in self.rollouts]\n",
    "\n",
    "        return {\n",
    "            \"total_rollouts\": len(self.rollouts),\n",
    "            \"avg_steps\": sum(total_steps) / len(total_steps) if total_steps else 0,\n",
    "            \"min_steps\": min(total_steps) if total_steps else 0,\n",
    "            \"max_steps\": max(total_steps) if total_steps else 0,\n",
    "            \"early_terminations\": early_terminations,\n",
    "            \"avg_reward\": sum(rewards) / len(rewards) if rewards else 0,\n",
    "            \"step_distribution\": total_steps,\n",
    "            \"rewards\": rewards\n",
    "        }\n",
    "\n",
    "    def save_rollouts(self, filepath: str, format: str = \"pickle\"):\n",
    "        \"\"\"Save all rollout data to file\"\"\"\n",
    "        if format == \"pickle\":\n",
    "            with open(filepath, \"wb\") as f:\n",
    "                pickle.dump(self.rollouts, f)\n",
    "        elif format == \"json\":\n",
    "            with open(filepath, \"w\") as f:\n",
    "                data = [r.to_dict() for r in self.rollouts]\n",
    "                json.dump(data, f, indent=2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown format: {format}\")\n",
    "\n",
    "        print(f\"Saved {len(self.rollouts)} rollouts to {filepath}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load_rollouts(filepath: str, format: str = \"pickle\") -> List[RolloutData]:\n",
    "        \"\"\"Load rollout data from file\"\"\"\n",
    "        if format == \"pickle\":\n",
    "            with open(filepath, \"rb\") as f:\n",
    "                return pickle.load(f)\n",
    "        elif format == \"json\":\n",
    "            with open(filepath, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "                # Would need to reconstruct RolloutData objects from dicts\n",
    "                return data\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown format: {format}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_reward_function(rollout: RolloutData) -> float:\n",
    "    \"\"\"\n",
    "    Dummy reward function - replace with actual task-specific logic.\n",
    "    \n",
    "    Examples:\n",
    "    - Check if final output matches expected answer\n",
    "    - Count number of correct steps\n",
    "    - Measure similarity to reference solution\n",
    "    \"\"\"\n",
    "    # Example: reward based on number of steps completed\n",
    "    num_steps = rollout.get_total_steps()\n",
    "    \n",
    "    # Higher reward for completing more steps (up to max)\n",
    "    if rollout.terminated_early:\n",
    "        # Penalty for early termination\n",
    "        return num_steps * 0.5\n",
    "    else:\n",
    "        # Bonus for completing full rollout\n",
    "        return num_steps * 1.0 + 10.0\n",
    "\n",
    "\n",
    "def dummy_termination_check(rollout: RolloutData, last_step: StepData) -> bool:\n",
    "    \"\"\"\n",
    "    Dummy termination check - replace with actual task-specific logic.\n",
    "    \n",
    "    Examples:\n",
    "    - Check if output contains stop phrase (\"DONE\", \"FINAL ANSWER:\", etc.)\n",
    "    - Check if task objective is met\n",
    "    - Check for errors or invalid states\n",
    "    \"\"\"\n",
    "    # Example: terminate if content contains \"DONE\"\n",
    "    if \"DONE\" in last_step.content.upper():\n",
    "        return True\n",
    "    \n",
    "    # Example: random early termination (10% chance) to simulate giving up\n",
    "    import random\n",
    "    if random.random() < 0.1:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def dummy_prompt_generator(rollout: RolloutData, step_num: int) -> str:\n",
    "    \"\"\"\n",
    "    Dummy prompt generator - replace with actual task-specific logic.\n",
    "    \n",
    "    Examples:\n",
    "    - For math: generate next problem in sequence\n",
    "    - For coding: provide next test case or requirement\n",
    "    - For reasoning: ask follow-up questions\n",
    "    \"\"\"\n",
    "    if step_num == 0:\n",
    "        # First step uses initial prompt\n",
    "        return rollout.initial_prompt\n",
    "    else:\n",
    "        # Subsequent steps: simple continuation\n",
    "        return f\"Continue with step {step_num + 1}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "\n",
    "# Initialize generator\n",
    "generator = LLMGenerator(\"Qwen/Qwen3-0.6B\")\n",
    "\n",
    "# Setup rollouts\n",
    "n_rollouts = 4\n",
    "max_steps = 50\n",
    "\n",
    "# Create initial prompts for each rollout\n",
    "initial_prompts = [\n",
    "    \"Solve this problem step by step: What is 15 + 27?\",\n",
    "    \"Write a story about a robot learning to cook.\",\n",
    "    \"Explain how photosynthesis works.\",\n",
    "    \"Debug this code: for i in range(10) print(i)\"\n",
    "]\n",
    "\n",
    "# Create manager\n",
    "manager = RolloutManager(generator, n_rollouts=n_rollouts, max_steps=max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize rollouts\n",
    "manager.initialize_rollouts(initial_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all rollouts to completion\n",
    "manager.run_rollouts(\n",
    "    prompt_generator_fn=dummy_prompt_generator,\n",
    "    termination_fn=dummy_termination_check,\n",
    "    max_new_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign rewards\n",
    "manager.assign_rewards(dummy_reward_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary\n",
    "summary = manager.get_rollout_summary()\n",
    "print(\"\\n=== Rollout Summary ===\")\n",
    "for key, value in summary.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save rollouts for later analysis/backprop\n",
    "manager.save_rollouts(\"rollout_data.pkl\", format=\"pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect individual rollout\n",
    "rollout = manager.rollouts[0]\n",
    "print(f\"\\n=== Rollout {rollout.rollout_id} ===\")\n",
    "print(f\"Initial prompt: {rollout.initial_prompt}\")\n",
    "print(f\"Total steps: {rollout.get_total_steps()}\")\n",
    "print(f\"Terminated early: {rollout.terminated_early}\")\n",
    "print(f\"Reward: {rollout.reward}\")\n",
    "print(f\"\\nSteps:\")\n",
    "for step in rollout.steps:\n",
    "    print(f\"  Step {step.step_num}: {step.prompt[:50]}... -> {step.content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# ========================================\n# TOOL USE / FUNCTION CALLING\n# ========================================",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Example: Using ToolUseLLM for agentic workflows\n\n# Define tools in OpenAI format\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\"\n                    },\n                    \"unit\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"celsius\", \"fahrenheit\"],\n                        \"description\": \"Temperature unit\"\n                    }\n                },\n                \"required\": [\"location\"]\n            }\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"calculate\",\n            \"description\": \"Perform a mathematical calculation\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"expression\": {\n                        \"type\": \"string\",\n                        \"description\": \"The mathematical expression to evaluate\"\n                    }\n                },\n                \"required\": [\"expression\"]\n            }\n        }\n    }\n]\n\n# Implement the tools\ndef get_weather(location: str, unit: str = \"fahrenheit\") -> str:\n    \\\"\\\"\\\"Dummy weather function\\\"\\\"\\\"\n    temps = {\n        \"San Francisco, CA\": (65, 18),\n        \"New York, NY\": (50, 10),\n        \"London, UK\": (55, 13)\n    }\n    temp_f, temp_c = temps.get(location, (70, 21))\n    temp = temp_c if unit == \"celsius\" else temp_f\n    return f\"The weather in {location} is {temp}°{unit[0].upper()} and sunny\"\n\ndef calculate(expression: str) -> float:\n    \\\"\\\"\\\"Safe calculator\\\"\\\"\\\"\n    try:\n        # Simple eval (use safe evaluation in production!)\n        result = eval(expression, {\"__builtins__\": {}}, {})\n        return result\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n# Map tool names to implementations\ntool_implementations = {\n    \"get_weather\": get_weather,\n    \"calculate\": calculate\n}\n\n# Initialize ToolUseLLM\ntool_llm = ToolUseLLM(\"Qwen/Qwen3-0.6B\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# DEMO: Showing the two key mechanisms explicitly\n\n# Initialize\ntool_llm_demo = ToolUseLLM(\"Qwen/Qwen3-0.6B\")\n\n# Example response from model\nexample_response = \"\"\"\nI'll check the weather for you.\n<tool_call>\n{\"name\": \"get_weather\", \"arguments\": {\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}}\n</tool_call>\n\"\"\"\n\nprint(\"=== DEMO: Tool Calling Mechanisms ===\\n\")\n\n# Q1: How do you know there's a tool call?\nhas_call = tool_llm_demo.has_tool_call(example_response)\nprint(f\"Q1: Does response have tool call? {has_call}\")\nprint(f\"    Detection: Check for '<tool_call>' tags in response text\\n\")\n\n# Parse the call\ntool_calls = tool_llm_demo._parse_tool_calls(example_response)\nprint(f\"Parsed {len(tool_calls)} tool call(s):\")\nprint(f\"  {tool_calls[0]}\\n\")\n\n# Q2: How do you execute efficiently?\nprint(\"Q2: Executing tool call...\")\nresult = tool_llm_demo.execute_tool_call(\n    tool_calls[0],\n    {\"get_weather\": get_weather}  # From earlier definition\n)\nprint(f\"    Method: **kwargs unpacking\")\nprint(f\"    Execution: get_weather(location='San Francisco, CA', unit='celsius')\")\nprint(f\"    Result: {result}\\n\")\n\nprint(\"Key insight: Detection = string check, Execution = **kwargs unpacking!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Example 1: Weather query\nresult = tool_llm.run(\n    messages=[\n        {\"role\": \"user\", \"content\": \"What's the weather in San Francisco?\"}\n    ],\n    tools=tools,\n    tool_implementations=tool_implementations,\n    max_turns=5\n)\n\nprint(\"=== Weather Query Example ===\")\nprint(f\"Turns: {result['turns']}\")\nprint(f\"Tool calls made: {len(result['tool_calls'])}\")\nprint(f\"\\nFinal response: {result['final_response']}\")\nprint(f\"\\nFull conversation:\")\nfor msg in result['messages']:\n    role = msg['role']\n    content = msg.get('content', '')\n    print(f\"{role}: {content[:100]}...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Example 2: Multi-step calculation\nresult = tool_llm.run(\n    messages=[\n        {\"role\": \"user\", \"content\": \"Calculate 15 * 27, then add 100 to the result\"}\n    ],\n    tools=tools,\n    tool_implementations=tool_implementations,\n    max_turns=10\n)\n\nprint(\"\\n=== Multi-step Calculation Example ===\")\nprint(f\"Turns: {result['turns']}\")\nprint(f\"Tool calls made: {len(result['tool_calls'])}\")\nprint(f\"\\nTool calls:\")\nfor i, tc in enumerate(result['tool_calls']):\n    print(f\"  {i+1}. {tc['function']['name']}({tc['function']['arguments']})\")\nprint(f\"\\nFinal response: {result['final_response']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Tool Use Implementation Notes\n\n**Q1: How do you know if the LLM responded with a tool call?**\n```python\ndef has_tool_call(response_text: str) -> bool:\n    # Qwen3 outputs: <tool_call>{\"name\": \"...\", \"arguments\": {...}}</tool_call>\n    return '<tool_call>' in response_text and '</tool_call>' in response_text\n```\n\n**Q2: How do you execute the tool call efficiently?**\n```python\ndef execute_tool_call(tool_call, tool_implementations):\n    tool_name = tool_call[\"function\"][\"name\"]\n    tool_args = tool_call[\"function\"][\"arguments\"]\n    \n    # Parse if JSON string\n    if isinstance(tool_args, str):\n        tool_args = json.loads(tool_args)\n    \n    # Execute with **kwargs unpacking (EFFICIENT!)\n    return tool_implementations[tool_name](**tool_args)\n```\n\n**Example:**\n```python\n# Tool call from LLM:\n# <tool_call>{\"name\": \"get_weather\", \"arguments\": {\"location\": \"SF\", \"unit\": \"celsius\"}}</tool_call>\n\n# Parsed to:\ntool_call = {\n    \"function\": {\n        \"name\": \"get_weather\",\n        \"arguments\": {\"location\": \"SF\", \"unit\": \"celsius\"}\n    }\n}\n\n# Executed as:\nresult = get_weather(location=\"SF\", unit=\"celsius\")  # **kwargs unpacking\n```\n\n**Key Points:**\n- Detection: Simple string check for `<tool_call>` tags\n- Execution: `**kwargs` unpacking for clean parameter passing\n- Hermes format: JSON inside XML tags (Qwen3 native)\n- Multi-turn: Loop until no more tool calls detected",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Access full step data for backprop\n# Each step contains:\n# - step.prompt: the input prompt\n# - step.thinking: the thinking process\n# - step.content: the actual response\n# - step.history_wo_thinking: conversation history without thinking\n# - step.history_w_thinking: conversation history with thinking\n# - step.tokens: token IDs (can be populated during generation if needed)\n\n# Example: get all responses for backprop\nfor rollout in manager.rollouts:\n    print(f\"\\nRollout {rollout.rollout_id} - Reward: {rollout.reward}\")\n    for step in rollout.steps:\n        # Here you would:\n        # 1. Re-tokenize or use stored tokens\n        # 2. Compute loss with reward signal\n        # 3. Backprop gradients\n        print(f\"  Step {step.step_num}: content length = {len(step.content)} chars\")"
  },
  {
   "cell_type": "markdown",
   "source": "# ========================================\n# RL TRAINING COMPONENTS\n# ========================================",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nimport numpy as np\n\n\n@dataclass\nclass RLStepData(StepData):\n    \"\"\"Extended step data for RL training with logprobs and values\"\"\"\n    logprobs: torch.Tensor = None  # Log probabilities of generated tokens\n    values: torch.Tensor = None     # Value estimates V(s_t)\n    response_tokens: List[int] = field(default_factory=list)  # Response token IDs\n    advantages: float = 0.0         # Computed advantage A_t\n    returns: float = 0.0            # Computed return R_t\n    old_logprobs: torch.Tensor = None  # For PPO ratio calculation\n\n\nclass SimpleValueHead(nn.Module):\n    \"\"\"Simple value head that attaches to the LM\"\"\"\n    def __init__(self, hidden_size: int):\n        super().__init__()\n        self.value_head = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.ReLU(),\n            nn.Linear(hidden_size // 2, 1)\n        )\n    \n    def forward(self, hidden_states):\n        \"\"\"\n        Args:\n            hidden_states: [batch_size, seq_len, hidden_size]\n        Returns:\n            values: [batch_size, seq_len, 1]\n        \"\"\"\n        return self.value_head(hidden_states)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def compute_advantages_and_returns(\n    rollouts: List[RolloutData],\n    gamma: float = 0.99,\n    lam: float = 0.95,\n    use_gae: bool = True\n) -> None:\n    \"\"\"\n    Compute advantages and returns for all rollouts using GAE.\n    \n    For outcome-based RL (reward at end):\n    - R_T = reward (final reward)\n    - R_t = 0 for t < T\n    - V(s_t) is the value estimate\n    - δ_t = r_t + γV(s_{t+1}) - V(s_t)  \n    - A_t = δ_t + γλδ_{t+1} + (γλ)^2δ_{t+2} + ...\n    \n    Args:\n        rollouts: List of rollout data with values populated\n        gamma: Discount factor\n        lam: GAE lambda parameter\n        use_gae: Whether to use GAE (True) or simple advantage (False)\n    \"\"\"\n    for rollout in rollouts:\n        T = len(rollout.steps)\n        if T == 0:\n            continue\n        \n        # Extract values for this rollout\n        values = []\n        for step in rollout.steps:\n            if hasattr(step, 'values') and step.values is not None:\n                # Take last token value as state value\n                val = step.values[-1].item() if torch.is_tensor(step.values) else step.values\n                values.append(val)\n            else:\n                values.append(0.0)\n        \n        values = np.array(values)\n        \n        # Rewards: 0 everywhere except final step\n        rewards = np.zeros(T)\n        rewards[-1] = rollout.reward\n        \n        if use_gae:\n            # GAE: A_t = Σ_{l=0}^{T-t-1} (γλ)^l δ_{t+l}\n            deltas = []\n            for t in range(T):\n                if t < T - 1:\n                    delta = rewards[t] + gamma * values[t + 1] - values[t]\n                else:\n                    # Last step: no next value\n                    delta = rewards[t] - values[t]\n                deltas.append(delta)\n            \n            deltas = np.array(deltas)\n            \n            # Compute advantages via reverse iteration\n            advantages = np.zeros(T)\n            advantages[-1] = deltas[-1]\n            for t in reversed(range(T - 1)):\n                advantages[t] = deltas[t] + gamma * lam * advantages[t + 1]\n        else:\n            # Simple advantage: A_t = R_t - V(s_t)\n            # where R_t is the discounted return from time t\n            returns = np.zeros(T)\n            returns[-1] = rewards[-1]\n            for t in reversed(range(T - 1)):\n                returns[t] = rewards[t] + gamma * returns[t + 1]\n            \n            advantages = returns - values\n        \n        # Compute returns: R_t = A_t + V(s_t)\n        returns = advantages + values\n        \n        # Store in step data\n        for t, step in enumerate(rollout.steps):\n            if hasattr(step, 'advantages'):\n                step.advantages = float(advantages[t])\n                step.returns = float(returns[t])\n    \n    print(f\\\"Computed advantages and returns for {len(rollouts)} rollouts using GAE={use_gae}\\\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def compute_ppo_loss(\n    logprobs: torch.Tensor,\n    old_logprobs: torch.Tensor,\n    advantages: torch.Tensor,\n    clip_epsilon: float = 0.2\n) -> Tuple[torch.Tensor, Dict[str, float]]:\n    \"\"\"\n    Compute PPO clipped policy loss.\n    \n    L^CLIP(θ) = -E[min(r(θ)A, clip(r(θ), 1-ε, 1+ε)A)]\n    where r(θ) = π_θ(a|s) / π_θ_old(a|s) = exp(logprob - old_logprob)\n    \n    Args:\n        logprobs: Log probabilities from current policy [batch_size, seq_len]\n        old_logprobs: Log probabilities from old policy [batch_size, seq_len]\n        advantages: Advantage estimates [batch_size]\n        clip_epsilon: PPO clip parameter\n    \n    Returns:\n        loss: Policy loss (scalar)\n        stats: Dictionary with statistics\n    \"\"\"\n    # Compute probability ratio\n    log_ratio = logprobs - old_logprobs\n    ratio = torch.exp(log_ratio)\n    \n    # Expand advantages to match sequence dimension if needed\n    if advantages.dim() == 1 and logprobs.dim() == 2:\n        advantages = advantages.unsqueeze(-1)  # [batch_size, 1]\n    \n    # Compute surrogate losses\n    surr1 = ratio * advantages\n    surr2 = torch.clamp(ratio, 1.0 - clip_epsilon, 1.0 + clip_epsilon) * advantages\n    \n    # PPO loss is negative of minimum (we want to maximize)\n    policy_loss = -torch.min(surr1, surr2).mean()\n    \n    # Statistics\n    with torch.no_grad():\n        approx_kl = ((ratio - 1) - log_ratio).mean().item()\n        clipfrac = ((ratio - 1.0).abs() > clip_epsilon).float().mean().item()\n    \n    stats = {\n        'policy_loss': policy_loss.item(),\n        'approx_kl': approx_kl,\n        'clipfrac': clipfrac,\n        'ratio_mean': ratio.mean().item(),\n        'ratio_std': ratio.std().item()\n    }\n    \n    return policy_loss, stats\n\n\ndef compute_value_loss(\n    values: torch.Tensor,\n    returns: torch.Tensor,\n    clip_value: bool = False,\n    old_values: Optional[torch.Tensor] = None,\n    clip_epsilon: float = 0.2\n) -> Tuple[torch.Tensor, Dict[str, float]]:\n    \"\"\"\n    Compute value function loss.\n    \n    L^VF(θ) = (V_θ(s) - R)^2\n    \n    With optional clipping (PPO-style):\n    L^VF_CLIP(θ) = max((V - R)^2, (clip(V, V_old - ε, V_old + ε) - R)^2)\n    \n    Args:\n        values: Value predictions [batch_size, seq_len] or [batch_size]\n        returns: Target returns [batch_size]\n        clip_value: Whether to use value clipping\n        old_values: Old value predictions (needed if clip_value=True)\n        clip_epsilon: Clip parameter\n    \n    Returns:\n        loss: Value loss (scalar)\n        stats: Dictionary with statistics\n    \"\"\"\n    # Take last value if sequence\n    if values.dim() == 2:\n        values = values[:, -1]  # [batch_size]\n    \n    if clip_value and old_values is not None:\n        if old_values.dim() == 2:\n            old_values = old_values[:, -1]\n        \n        # Clipped value loss\n        values_clipped = old_values + torch.clamp(\n            values - old_values, -clip_epsilon, clip_epsilon\n        )\n        vf_loss1 = (values - returns) ** 2\n        vf_loss2 = (values_clipped - returns) ** 2\n        value_loss = torch.max(vf_loss1, vf_loss2).mean()\n    else:\n        # Standard MSE loss\n        value_loss = F.mse_loss(values, returns)\n    \n    stats = {\n        'value_loss': value_loss.item(),\n        'value_mean': values.mean().item(),\n        'value_std': values.std().item(),\n        'returns_mean': returns.mean().item()\n    }\n    \n    return value_loss, stats\n\n\ndef compute_kl_divergence(\n    logprobs: torch.Tensor,\n    old_logprobs: torch.Tensor\n) -> torch.Tensor:\n    \"\"\"\n    Compute KL divergence between old and new policy.\n    \n    KL(π_old || π_new) = E[log(π_old) - log(π_new)]\n                        = E[old_logprobs - logprobs]\n    \n    Args:\n        logprobs: Log probabilities from new policy\n        old_logprobs: Log probabilities from old policy\n    \n    Returns:\n        kl: KL divergence (scalar)\n    \"\"\"\n    kl = (old_logprobs - logprobs).mean()\n    return kl",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class RLTrainer:\n    \"\"\"\n    Outer training loop for multi-turn RL.\n    \n    Workflow:\n    1. Generate n rollouts (trajectories)\n    2. Assign rewards to completed rollouts\n    3. Compute values for all steps\n    4. Calculate advantages using GAE\n    5. Update model using PPO\n    6. Repeat\n    \"\"\"\n    \n    def __init__(\n        self,\n        model: AutoModelForCausalLM,\n        tokenizer: AutoTokenizer,\n        value_head: SimpleValueHead,\n        config: Dict[str, Any]\n    ):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.value_head = value_head\n        self.config = config\n        \n        # Optimizers\n        self.policy_optimizer = Adam(\n            self.model.parameters(),\n            lr=config.get('policy_lr', 1e-5)\n        )\n        self.value_optimizer = Adam(\n            self.value_head.parameters(),\n            lr=config.get('value_lr', 1e-4)\n        )\n        \n        # Training stats\n        self.training_stats = []\n    \n    def generate_with_values(\n        self,\n        prompts: List[List[Dict[str, str]]],\n        max_new_tokens: int = 512,\n        temperature: float = 0.6\n    ) -> Tuple[List[Tuple[str, str]], List[torch.Tensor], List[torch.Tensor], List[torch.Tensor]]:\n        \"\"\"\n        Generate responses and compute values + logprobs.\n        \n        Returns:\n            outputs: List of (thinking, content) tuples\n            all_logprobs: List of logprob tensors for each response\n            all_values: List of value tensors for each response  \n            all_tokens: List of token ID tensors for each response\n        \"\"\"\n        # Apply chat template\n        texts = [\n            self.tokenizer.apply_chat_template(\n                prompt,\n                tokenize=False,\n                add_generation_prompt=True,\n                enable_thinking=True\n            )\n            for prompt in prompts\n        ]\n        \n        model_inputs = self.tokenizer(\n            texts,\n            padding=True,\n            truncation=True,\n            return_tensors=\\\"pt\\\"\n        ).to(self.model.device)\n        \n        # Generate with output scores and hidden states\n        with torch.no_grad():\n            generation_output = self.model.generate(\n                **model_inputs,\n                max_new_tokens=max_new_tokens,\n                temperature=temperature,\n                do_sample=temperature > 0,\n                output_scores=True,\n                output_hidden_states=True,\n                return_dict_in_generate=True\n            )\n        \n        generated_sequences = generation_output.sequences\n        scores = generation_output.scores  # Tuple of [batch_size, vocab_size] for each step\n        \n        # Compute logprobs and values\n        batch_size = generated_sequences.shape[0]\n        prompt_lens = [model_inputs.input_ids[i].ne(self.tokenizer.pad_token_id).sum().item() \n                       for i in range(batch_size)]\n        \n        all_outputs = []\n        all_logprobs = []\n        all_values = []\n        all_tokens = []\n        \n        for i in range(batch_size):\n            prompt_len = prompt_lens[i]\n            response_ids = generated_sequences[i, prompt_len:].tolist()\n            \n            # Decode thinking and content\n            thinking, content = self._parse_thinking(response_ids)\n            all_outputs.append((thinking, content))\n            all_tokens.append(response_ids)\n            \n            # Compute logprobs for generated tokens\n            logprobs = []\n            for t, score in enumerate(scores):\n                if t < len(response_ids):\n                    token_logprobs = F.log_softmax(score[i], dim=-1)\n                    selected_logprob = token_logprobs[response_ids[t]]\n                    logprobs.append(selected_logprob)\n            \n            all_logprobs.append(torch.stack(logprobs) if logprobs else torch.tensor([]))\n            \n            # Compute values (dummy for now - would need hidden states)\n            # In practice, you'd run value_head on the hidden states\n            dummy_values = torch.zeros(len(response_ids))\n            all_values.append(dummy_values)\n        \n        return all_outputs, all_logprobs, all_values, all_tokens\n    \n    def _parse_thinking(self, output_ids: List[int]) -> Tuple[str, str]:\n        \\\"\\\"\\\"Parse thinking from token IDs\\\"\\\"\\\"\n        think_start_id = 151667\n        think_end_id = 151668\n        \n        try:\n            think_end_idx = len(output_ids) - output_ids[::-1].index(think_end_id)\n            thinking_ids = output_ids[:think_end_idx]\n            content_ids = output_ids[think_end_idx:]\n        except ValueError:\n            thinking_ids = []\n            content_ids = output_ids\n        \n        thinking = self.tokenizer.decode(thinking_ids, skip_special_tokens=True)\n        content = self.tokenizer.decode(content_ids, skip_special_tokens=True)\n        \n        return thinking.strip(), content.strip()\n    \n    def train_step(\n        self,\n        rollouts: List[RolloutData],\n        n_epochs: int = 4,\n        batch_size: int = 32\n    ) -> Dict[str, float]:\n        \\\"\\\"\\\"\n        Perform one training step on collected rollouts.\n        \n        Args:\n            rollouts: List of completed rollouts with advantages computed\n            n_epochs: Number of PPO epochs\n            batch_size: Minibatch size\n        \n        Returns:\n            stats: Training statistics\n        \\\"\\\"\\\"\n        # Flatten all steps from all rollouts\n        all_steps = []\n        for rollout in rollouts:\n            for step in rollout.steps:\n                if hasattr(step, 'logprobs') and step.logprobs is not None:\n                    all_steps.append(step)\n        \n        if not all_steps:\n            print(\\\"No steps with logprobs to train on\\\")\n            return {}\n        \n        # Training loop\n        total_stats = {\n            'policy_loss': 0.0,\n            'value_loss': 0.0,\n            'total_loss': 0.0,\n            'approx_kl': 0.0,\n            'clipfrac': 0.0\n        }\n        \n        n_updates = 0\n        \n        for epoch in range(n_epochs):\n            # Shuffle steps\n            import random\n            random.shuffle(all_steps)\n            \n            # Mini-batch training\n            for i in range(0, len(all_steps), batch_size):\n                batch = all_steps[i:i + batch_size]\n                \n                # Prepare batch tensors\n                logprobs_batch = torch.stack([s.logprobs for s in batch])\n                old_logprobs_batch = torch.stack([s.old_logprobs for s in batch])\n                values_batch = torch.stack([s.values for s in batch])\n                advantages_batch = torch.tensor([s.advantages for s in batch])\n                returns_batch = torch.tensor([s.returns for s in batch])\n                \n                # Move to device\n                device = self.model.device\n                logprobs_batch = logprobs_batch.to(device)\n                old_logprobs_batch = old_logprobs_batch.to(device)\n                values_batch = values_batch.to(device)\n                advantages_batch = advantages_batch.to(device)\n                returns_batch = returns_batch.to(device)\n                \n                # Normalize advantages\n                advantages_batch = (advantages_batch - advantages_batch.mean()) / (advantages_batch.std() + 1e-8)\n                \n                # Compute losses\n                policy_loss, policy_stats = compute_ppo_loss(\n                    logprobs_batch,\n                    old_logprobs_batch,\n                    advantages_batch,\n                    clip_epsilon=self.config.get('clip_epsilon', 0.2)\n                )\n                \n                value_loss, value_stats = compute_value_loss(\n                    values_batch,\n                    returns_batch,\n                    clip_value=self.config.get('clip_value', False)\n                )\n                \n                # Total loss\n                total_loss = (\n                    policy_loss +\n                    self.config.get('vf_coef', 0.5) * value_loss\n                )\n                \n                # Backprop\n                self.policy_optimizer.zero_grad()\n                self.value_optimizer.zero_grad()\n                total_loss.backward()\n                \n                # Gradient clipping\n                if self.config.get('max_grad_norm', 0) > 0:\n                    torch.nn.utils.clip_grad_norm_(\n                        self.model.parameters(),\n                        self.config['max_grad_norm']\n                    )\n                    torch.nn.utils.clip_grad_norm_(\n                        self.value_head.parameters(),\n                        self.config['max_grad_norm']\n                    )\n                \n                self.policy_optimizer.step()\n                self.value_optimizer.step()\n                \n                # Accumulate stats\n                total_stats['policy_loss'] += policy_stats['policy_loss']\n                total_stats['value_loss'] += value_stats['value_loss']\n                total_stats['total_loss'] += total_loss.item()\n                total_stats['approx_kl'] += policy_stats['approx_kl']\n                total_stats['clipfrac'] += policy_stats['clipfrac']\n                n_updates += 1\n        \n        # Average stats\n        for key in total_stats:\n            total_stats[key] /= max(n_updates, 1)\n        \n        return total_stats\n    \n    def train(\n        self,\n        initial_prompts_generator,\n        reward_function,\n        termination_function,\n        prompt_generator_function,\n        n_iterations: int = 10,\n        n_rollouts_per_iter: int = 4,\n        max_steps: int = 50\n    ):\n        \\\"\\\"\\\"\n        Main training loop.\n        \n        Args:\n            initial_prompts_generator: Function that returns list of initial prompts\n            reward_function: Function to compute rewards\n            termination_function: Function to check termination\n            prompt_generator_function: Function to generate next prompt\n            n_iterations: Number of training iterations\n            n_rollouts_per_iter: Number of rollouts per iteration\n            max_steps: Max steps per rollout\n        \\\"\\\"\\\"\n        for iteration in range(n_iterations):\n            print(f\\\"\\\\n=== Iteration {iteration + 1}/{n_iterations} ===\\\")\n            \n            # 1. Generate rollouts\n            print(\\\"Generating rollouts...\\\")\n            generator = LLMGenerator(self.model)  # Wrap model\n            manager = RolloutManager(generator, n_rollouts_per_iter, max_steps)\n            \n            initial_prompts = initial_prompts_generator(n_rollouts_per_iter)\n            manager.initialize_rollouts(initial_prompts)\n            manager.run_rollouts(\n                prompt_generator_fn=prompt_generator_function,\n                termination_fn=termination_function,\n                max_new_tokens=self.config.get('max_new_tokens', 512)\n            )\n            \n            # 2. Assign rewards\n            print(\\\"Assigning rewards...\\\")\n            manager.assign_rewards(reward_function)\n            \n            # 3. Compute advantages\n            print(\\\"Computing advantages...\\\")\n            compute_advantages_and_returns(\n                manager.rollouts,\n                gamma=self.config.get('gamma', 0.99),\n                lam=self.config.get('lam', 0.95)\n            )\n            \n            # 4. Train\n            print(\\\"Training model...\\\")\n            stats = self.train_step(\n                manager.rollouts,\n                n_epochs=self.config.get('ppo_epochs', 4),\n                batch_size=self.config.get('batch_size', 32)\n            )\n            \n            # 5. Log stats\n            summary = manager.get_rollout_summary()\n            print(f\\\"\\\\nIteration {iteration + 1} Results:\\\")\n            print(f\\\"  Avg reward: {summary['avg_reward']:.2f}\\\")\n            print(f\\\"  Avg steps: {summary['avg_steps']:.2f}\\\")\n            print(f\\\"  Policy loss: {stats.get('policy_loss', 0):.4f}\\\")\n            print(f\\\"  Value loss: {stats.get('value_loss', 0):.4f}\\\")\n            print(f\\\"  Approx KL: {stats.get('approx_kl', 0):.4f}\\\")\n            \n            self.training_stats.append({\n                'iteration': iteration + 1,\n                'rollout_summary': summary,\n                'training_stats': stats\n            })\n            \n            # Save checkpoint\n            if (iteration + 1) % self.config.get('save_interval', 5) == 0:\n                self.save_checkpoint(f\\\"checkpoint_iter_{iteration + 1}.pt\\\")\n        \n        print(\\\"\\\\nTraining complete!\\\")\n        return self.training_stats\n    \n    def save_checkpoint(self, filepath: str):\n        \\\"\\\"\\\"Save model checkpoint\\\"\\\"\\\"\n        torch.save({\n            'model_state_dict': self.model.state_dict(),\n            'value_head_state_dict': self.value_head.state_dict(),\n            'policy_optimizer_state_dict': self.policy_optimizer.state_dict(),\n            'value_optimizer_state_dict': self.value_optimizer.state_dict(),\n            'training_stats': self.training_stats,\n            'config': self.config\n        }, filepath)\n        print(f\\\"Checkpoint saved to {filepath}\\\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# ========================================\n# EXAMPLE: Using RLTrainer\n# ========================================",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Initialize model and value head\nmodel_name = \\\"Qwen/Qwen3-0.6B\\\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, dtype=\\\"auto\\\", device_map=\\\"auto\\\")\n\n# Create value head (get hidden size from model config)\nhidden_size = model.config.hidden_size\nvalue_head = SimpleValueHead(hidden_size).to(model.device)\n\n# Training configuration\nconfig = {\n    'policy_lr': 1e-5,\n    'value_lr': 1e-4,\n    'gamma': 0.99,\n    'lam': 0.95,\n    'clip_epsilon': 0.2,\n    'vf_coef': 0.5,\n    'max_grad_norm': 1.0,\n    'ppo_epochs': 4,\n    'batch_size': 32,\n    'max_new_tokens': 512,\n    'save_interval': 5,\n    'clip_value': False\n}\n\n# Create trainer\ntrainer = RLTrainer(model, tokenizer, value_head, config)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Define task-specific functions\ndef generate_initial_prompts(n):\n    \\\"\\\"\\\"Generate n initial prompts for rollouts\\\"\\\"\\\"\n    prompts = [\n        \\\"Solve: What is 15 + 27?\\\",\n        \\\"Write a short story about AI.\\\",\n        \\\"Explain quantum computing.\\\",\n        \\\"Debug: for i in range(10) print(i)\\\"\n    ]\n    return (prompts * ((n // len(prompts)) + 1))[:n]\n\n# Use the dummy functions defined earlier:\n# - dummy_reward_function\n# - dummy_termination_check  \n# - dummy_prompt_generator\n\n# Run training\ntraining_stats = trainer.train(\n    initial_prompts_generator=generate_initial_prompts,\n    reward_function=dummy_reward_function,\n    termination_function=dummy_termination_check,\n    prompt_generator_function=dummy_prompt_generator,\n    n_iterations=10,\n    n_rollouts_per_iter=4,\n    max_steps=50\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Analyze training progress\nimport matplotlib.pyplot as plt\n\niterations = [s['iteration'] for s in training_stats]\navg_rewards = [s['rollout_summary']['avg_reward'] for s in training_stats]\npolicy_losses = [s['training_stats'].get('policy_loss', 0) for s in training_stats]\nvalue_losses = [s['training_stats'].get('value_loss', 0) for s in training_stats]\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\naxes[0].plot(iterations, avg_rewards)\naxes[0].set_title('Average Reward over Training')\naxes[0].set_xlabel('Iteration')\naxes[0].set_ylabel('Reward')\n\naxes[1].plot(iterations, policy_losses)\naxes[1].set_title('Policy Loss over Training')\naxes[1].set_xlabel('Iteration')\naxes[1].set_ylabel('Loss')\n\naxes[2].plot(iterations, value_losses)\naxes[2].set_title('Value Loss over Training')\naxes[2].set_xlabel('Iteration')\naxes[2].set_ylabel('Loss')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Key Implementation Notes\n\n## How it works:\n\n1. **Trajectory Generation**: `RolloutManager` creates n parallel rollouts, executing steps until completion or early termination\n2. **Reward Assignment**: Rewards given at the end of each rollout (outcome-based RL)\n3. **Advantage Calculation**: Uses GAE (Generalized Advantage Estimation):\n   - δ_t = r_t + γV(s_{t+1}) - V(s_t)\n   - A_t = Σ_{l=0}^∞ (γλ)^l δ_{t+l}\n4. **PPO Loss**: \n   - Policy: L = -E[min(r(θ)A, clip(r(θ), 1-ε, 1+ε)A)]\n   - Value: L = (V(s) - R)^2\n   - KL: For monitoring policy drift\n5. **Training**: Multiple epochs over collected data, with gradient clipping and advantage normalization\n\n## Data Structure:\n- Each `StepData` stores: prompt, response, thinking, logprobs, values, advantages, returns\n- Each `RolloutData` tracks full trajectory with all steps and final reward\n- All data saved for later analysis/debugging\n\n## To Customize:\n- `reward_function`: Define your task-specific reward\n- `termination_function`: Define when rollouts should stop early\n- `prompt_generator_function`: Define how to generate next prompts\n- Config params: Learning rates, PPO hyperparameters, etc.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}