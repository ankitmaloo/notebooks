{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0O3kjUDwUKxW"
   },
   "source": [
    "In this Colab notebook, you'll do on-policy distillation (OPD) on Qwen3-0.6b using Qwen3-4b-Instruct-2507, to make it better at [GSM8K](https://huggingface.co/datasets/openai/gsm8k) (a dataset of math problems).\n",
    "\n",
    "Unlike standard supervised fine-tuning (SFT), the student model (Qwen-3-0.6b) learns from its own generated outputs rather than fixed gold data â€” reducing exposure bias and better matching the inference-time distribution.\n",
    "\n",
    "You'll need to connect an A100 GPU (40 GB Ram) or better. You might be able to get away with smaller GPUs if you change some of the config parameters, like samples_per_prompt and max_new_tokens!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7PRWDrLT8BM"
   },
   "source": [
    "Inspired by [Thinking Machines](https://thinkingmachines.ai/blog/on-policy-distillation/) and prior art like [Agarwal et al](https://arxiv.org/abs/2306.13649)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SzHpn5I0DoyP",
    "outputId": "9621c1ef-241a-4393-812c-cb369db20c9a"
   },
   "outputs": [],
   "source": [
    "#@title ðŸ› ï¸ Setup\n",
    "!nvidia-smi -L || true\n",
    "\n",
    "import os, sys, random, numpy as np, torch, json, time, platform, math\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Qwen3 requires Transformers >= 4.51\n",
    "try:\n",
    "    get_ipython().run_line_magic(\"uv\", \"pip -q install transformers==4.51.3 accelerate==1.4.0 peft==0.14.0 datasets==3.3.2 evaluate==0.4.3 sentencepiece protobuf tqdm matplotlib > /dev/null\")\n",
    "except Exception:\n",
    "    get_ipython().run_line_magic(\"pip\", \"-q install transformers==4.51.3 accelerate==1.4.0 peft==0.14.0 datasets==3.3.2 evaluate==0.4.3 sentencepiece protobuf tqdm matplotlib > /dev/null\")\n",
    "\n",
    "import transformers, datasets, peft, accelerate, matplotlib\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Accelerate:\", accelerate.__version__)\n",
    "print(\"PEFT:\", peft.__version__)\n",
    "print(\"matplotlib:\", matplotlib.__version__)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "assert DEVICE == \"cuda\", \"Please connect a GPU (A100+ recommended).\"\n",
    "\n",
    "def print_header():\n",
    "    print(\"== Environment ==\")\n",
    "    print(dict(\n",
    "        python=sys.version,\n",
    "        torch=torch.__version__,\n",
    "        transformers=transformers.__version__,\n",
    "        accelerate=accelerate.__version__,\n",
    "        peft=peft.__version__,\n",
    "        cuda=torch.version.cuda if torch.cuda.is_available() else \"cpu\",\n",
    "        device_name=torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\",\n",
    "        platform=platform.platform(),\n",
    "        seed=SEED\n",
    "    ))\n",
    "print_header()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”‘ Weights & Biases Login\n",
    "\n",
    "To enable wandb logging, run the cell below to authenticate.\n",
    "\n",
    "- Get your API key from: https://wandb.ai/authorize\n",
    "- This will prompt you to paste your key or open a browser\n",
    "- You only need to do this once per session\n",
    "- If you skip this, training will continue without wandb logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_line_magic(\"uv\", \"pip install wandb huggingface\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['WANDB_API_KEY'] = \"\" #your api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Wandb Login (Optional - Skip if you don't want wandb logging)\n",
    "# Get your API key from: https://wandb.ai/authorize\n",
    "import wandb\n",
    "try:\n",
    "    wandb.login()\n",
    "    print(\"âœ“ wandb login successful\")\n",
    "except Exception as e:\n",
    "    print(f\"wandb login skipped or failed: {e}\")\n",
    "    print(\"Training will continue without wandb logging\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_PROJECT = \"opd-test\"  # Set to None to disable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=TOKEN)\n",
    "\n",
    "\n",
    "HF_HUB_REPO = \"ankitclio/qwen3-0.6b-opd\" ##REPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "x_5x6fL3Ut0X"
   },
   "outputs": [],
   "source": [
    "import os, sys, time, json, random, platform\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import re\n",
    "\n",
    "# --------------------------\n",
    "# Reproducibility & device\n",
    "# --------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "assert DEVICE == \"cuda\", \"A CUDA GPU is required.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1ttYTs9Uv29"
   },
   "source": [
    "Try toying around with these settings. You can also try using a different teacher model, even if it doesn't use the same tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649,
     "referenced_widgets": [
      "828b3bb4d2df4f5c8332bb9c44f561e0",
      "4dfa8b94d60c4496b9fbebd069d39475",
      "cb8b3b2b3a6b4719a80b33c18b685f57",
      "4156f40d18174fd4b051e4e32baf3ca2",
      "cfbd388fabe84093b7f374faff98ba18",
      "de577ae09448450eb1fb05531635081a",
      "9183266e2e324ff48dfb2e532dec11c0",
      "a70223dffdb54c8d8a7a582f614aa101",
      "974d350d09d94fcabb2f461b1272e420",
      "663adf401ada42c382f72c40993d0fba",
      "9cb1f562cb0949ff9b95ca2889a3d020",
      "c14c2eac7d434ffd896e33e3e9bfdc98",
      "d6d2c6e8762f4c6b8dcb51135698512c",
      "80fb6b84377d4875b731db072efe2d3b",
      "16c15a3f10f24b58993f650bbad680cb",
      "cdce465d5040417487caa1c92f514ee4",
      "72453b680ab1435ca3202b7ab3add760",
      "df6506cb7c7344eba9ded9f968740256",
      "3cb56d21813d423080ce0c029df3540e",
      "fe9f2cc368924c518157caee1dcd2dbb",
      "8e6a3f7890094044ac9160b143598584",
      "69f27cb714b34674ae0429abbfd5a8ce",
      "8551488212854178b74385b209b47bf8",
      "766dbcd90d3c4df18d1888edc18cfbf7",
      "3ab1897b9d2047898e3462be07c8ad0a",
      "b528bf6301104662986c6e08cf3e18c0",
      "a1742babdb9f4cab9007835d826a3f31",
      "30f31854469546918a0958677636bf93",
      "80e33a6d33b7454ab5dcbec5bb4c61b8",
      "1b89848e0be04952b77937822a0b02c9",
      "c475518de76a48a1873d6f092a3c9d91",
      "c9a9aa005bb4465ea1af82fcd9d8afd4",
      "7a19b627909d4203b6c50e541c3b3bf3",
      "474533c58b8242d898b2fac4d8869c52",
      "a09ed2a071c9415dbc6db02d46cf4844",
      "1fabddef86694495b2f40a7d31e1efe1",
      "f99b66bc4b084a10a6b2739254bd11cd",
      "92aead18ca9c42fcb5a8126c425a367c",
      "4598b67d1ab34fac97301e37e36e807e",
      "a49fbb6e19d64101a7425006540d5b81",
      "0580907c64fd4d7e8629f48fe2206b7e",
      "1356e073513d40dc986286fb68dc5cde",
      "40ed823de9654f9396c67a75094c6533",
      "478bc78af9ec4b438f99e750db8d2e9a",
      "b79a82873e1548c9b251ab67e7c84de0",
      "b3a36f3d988e4c1f9df346c19efe1960",
      "73f222c0e66a4578b26b28d7603e4617",
      "0fe4f10c1f4a4152a5beb0fbc7fdaa32",
      "354062c80b5d481a813d4befbec2c4d7",
      "66b69ae7def9456a960cb9805c998bb5",
      "6853185373e94a5f96d068d7cb8bdd5f",
      "f9df6d1ea08348d1babd0331dde54419",
      "a3276057dd994c5fbbedcc60a5e9178f",
      "7a6ad553d9854601b6c320f8a95ffc7b",
      "d5087c5ce3334860b747481dd6f2cf7a",
      "dbe521d6446d4cea8cc7af8c702b28c2",
      "63c15f6d94c34468a10cbaaf99307364",
      "9f32cba38ab847a1aa8bc2882a53c1f8",
      "c7071ad65a61477288c42a7fc4fde530",
      "53a1c9cf15ba4dfa8ac9cb4b3b93442a",
      "0b4986bc3b694b5484dcfbbef5e41179",
      "95745730b2314b08b85d83340d961321",
      "a1152b6dc04342b2bc98d670eaf8b9da",
      "f08821d894fa4c06a83eb8d9351f3e75",
      "181662afc9d141f099840fa0fa283423",
      "202b9c3043b44c428bcfaf0da6dda6cc",
      "fdc72c477027405d850c978ef0fbeeaf",
      "5a31fc5df00d449fa50bbfd8a3f1cd50",
      "8fb64fe4bb9c4364a1ea2e34821fdbfb",
      "b418a33e98c542ae8f29f667f3f13ece",
      "b33234068ceb4cb68bf4de79b18c3ac8",
      "29b7ce0e3225450ea4a7031d6be205e8",
      "63b01dbfc5214a42b66a9154d99c515c",
      "6945be98814b49f3881201714d46cf46",
      "309f09862d98463a87e02b1fb9fd05bb",
      "d89cc38eee614b0ab11789d02d76a4d5",
      "b9a3fb7de71040d487410695cb6df583",
      "b9cf70e54cb746bf9c49cfbc61dd5a1b",
      "bff672c5059a4556a7371d478b15fdd2",
      "cc551aa4588b4f4ca2db8be3be14967a",
      "48a91e735230440398740a90d10dd1c4",
      "39581f905d21436187704f1d5274284e",
      "8be8620f01af4fcc85107295cf7afb98",
      "302b00a7b59f413ebf277c12bbad51c3",
      "323d25f425e14b95961b8aac7b217361",
      "fe41146454ba468099fbf4078e7569a2",
      "b9490ccd44144dcc83775ea3f51698e1",
      "6f189547158741b5b12d53f6a48ef9af",
      "f7e25a4b3caf4d1494a1931b2b8a4c4f",
      "7cc2cfca1167456fb4d70cbc820550e6",
      "5baf2b2da1714007be45a58e020d9bfb",
      "395b1475cdce4efb9838e59d062331f2",
      "113cec096d7441ec8b32cd994fe6cb59",
      "9a4e4126fbcc4e1bbae1d2a1914771ca",
      "02633749512a4268808a04005e948c98",
      "7e4c5f209cb3419ca5eb8af7e719ed52",
      "4963343b137d447c8959e9f4f8c72864",
      "58462486ba634d249c8c2dc70b3f4375",
      "1f80a75b69c14ad58d5c87e0d8c952a3",
      "96faaf1fd485441ab9184063f5a3266b",
      "b498d18a3e4f497383298dff00aaada9",
      "94214ef00121486c89e7fded87488625",
      "52f1407c9dc6412cb9d32e1ab096caaf",
      "3131bfe6bcb145149d9c31387f1eaf25",
      "48eec6dad965467ca5e011fbf0952f5e",
      "5007f440b7704dbd87c236afdf53f5ae",
      "4ae9d31c8248443a9b332658503bb107",
      "aeea53225326484a9d1cc4ebcf555623",
      "c0663fa90370493c98d387e02fe4aada",
      "5b9dd6b0b3a743d28aea0d8e7e153642",
      "24eca0174df04bd89f8db4b2712ffe95",
      "42a351903d604cc3afde43b28fe6a9f4",
      "37a6720a163f4d0eb2384b9334385276",
      "6b2b0e2981474fc385cbaebfd492712b",
      "7bcae34b8d0b4691b274caa22402bb9c",
      "d0011c79284b419ea02c328ce1d5ca58",
      "a1f68f0132f5434da6797f4dc70da8b1",
      "cc31e592f52d4feabb571b7067112ae1",
      "4b8d8e32a59048b8a0b6d10cd47592d6",
      "2c436615672f402a9cacac79f27e1486",
      "2ae5a73a5dc6417ea9e7391190a5cc7e",
      "577489ab17364137ad0b6c5d7ee7ecd4",
      "4450371bafa84098bacf7ee6647432fc",
      "44715306855e4306b74b62422d46e391",
      "33c393307c7a4c009ea2f88e7b131e73",
      "327deedb88bb494e8ef10200e0236c6e",
      "58ee3bef989e4630bc631ce216ee020f",
      "c70449070691401b8048f32327a56583",
      "be7531a6c91b4961859b9d090e4af2a6",
      "34ff50e25d024d8da50ad71d9323ab51",
      "521f74b924c04f58b47b4d6d6cb5bcde",
      "8c3e46a9109f4682ad97c5dc7ec53c45",
      "57d2e3e0e6994e5aa752e42f5b90d34d",
      "c2a3efb6ff6644d0ae5e3ac2114755ac",
      "10ea4421b183443795ea483f73c3d280",
      "bf5fb1e5ad0f43b38b6bd966f80df889",
      "8eb9ae07195049e38c363e0f22c2d46c",
      "6439637aeb594e4fbb78886f49abd418",
      "cec31f90c4034a76bfc20e97110d7e6f",
      "4e02d2bc16394610bf8850a83b4570b7",
      "a8cfa49f0f9741569800248fab63bd81",
      "d937f21517884407a91fe0186ee5d19e",
      "a6b842b0e9f04931bd4322b2a9a180ea"
     ]
    },
    "id": "-InQzIboDs2j",
    "outputId": "98d6d0ea-d641-436c-ef48-021e39e0f8d2"
   },
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Config\n",
    "# --------------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Models\n",
    "    student_id: str = \"Qwen/Qwen3-0.6B-Base\"\n",
    "    teacher_id: str = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "    # Prompting\n",
    "    prompt_template: str = (\n",
    "        \"Solve step by step.\\n\"\n",
    "        \"Give ONLY ONE final numeric answer (no units), inside square brackets.\\n\"\n",
    "        \"Problem: {question}\\n\\nSolution:\"\n",
    "    )\n",
    "    max_new_tokens: int = 256\n",
    "\n",
    "    # Generation temps\n",
    "    eval_temperature: float = 0.0   # greedy for eval\n",
    "    train_temperature: float = 0.7  # sampling for on-policy data\n",
    "\n",
    "    # Training schedule\n",
    "    steps: int = 50\n",
    "    batch_prompts: int = 4\n",
    "    samples_per_prompt: int = 4 # increase this if you have a H200/B200\n",
    "    lr: float = 1e-4\n",
    "    weight_decay: float = 0.0\n",
    "    grad_accum: int = 1\n",
    "\n",
    "    # Micro-batching\n",
    "    student_mb: int = 8\n",
    "\n",
    "    # Monitoring\n",
    "    log_every: int = 10\n",
    "    val_every: int = 10\n",
    "    val_sample_n: int = 100\n",
    "    ema_momentum: float = 0.9\n",
    "\n",
    "    # Validation size\n",
    "    val_rows: Optional[int] = None  # if None, uses min(200, len(train))\n",
    "\n",
    "    # Output dir\n",
    "    run_root: str = f\"./run_opd_{int(time.time())}\"\n",
    "\n",
    "cfg = Config()\n",
    "os.makedirs(cfg.run_root, exist_ok=True)\n",
    "\n",
    "def print_env():\n",
    "    import transformers, accelerate, peft, matplotlib\n",
    "    print(\"== Environment ==\")\n",
    "    print({\n",
    "        \"python\": sys.version,\n",
    "        \"torch\": torch.__version__,\n",
    "        \"transformers\": transformers.__version__,\n",
    "        \"accelerate\": accelerate.__version__,\n",
    "        \"peft\": peft.__version__,\n",
    "        \"cuda\": torch.version.cuda if torch.cuda.is_available() else \"cpu\",\n",
    "        \"device_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\",\n",
    "        \"platform\": platform.platform(),\n",
    "        \"seed\": SEED\n",
    "    })\n",
    "print_env()\n",
    "\n",
    "# --------------------------\n",
    "# Data: GSM8K\n",
    "# --------------------------\n",
    "def render_prompt(question: str) -> str:\n",
    "    return cfg.prompt_template.format(question=question)\n",
    "\n",
    "def parse_gold(answer_text: str) -> Optional[str]:\n",
    "    m = re.search(r\"####\\s*(-?\\d+(?:\\.\\d+)?)\", answer_text)\n",
    "    if m: return m.group(1).strip()\n",
    "    nums = re.findall(r\"-?\\d+(?:\\.\\d+)?\", answer_text)\n",
    "    return nums[-1].strip() if nums else None\n",
    "\n",
    "def parse_pred(text: str) -> Optional[str]:\n",
    "    m = re.search(r\"\\[\\s*(-?\\d+(?:\\.\\d+)?)\\s*\\]\", text)\n",
    "    if m: return m.group(1).strip()\n",
    "    nums = re.findall(r\"-?\\d+(?:\\.\\d+)?\", text)\n",
    "    return nums[-1].strip() if nums else None\n",
    "\n",
    "print(\"Loading GSM8Kâ€¦\")\n",
    "ds_train_full = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n",
    "ds_test       = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
    "\n",
    "if cfg.val_rows is None:\n",
    "    val_rows = min(200, len(ds_train_full))\n",
    "else:\n",
    "    val_rows = min(cfg.val_rows, len(ds_train_full))\n",
    "\n",
    "ds_val   = ds_train_full.select(range(val_rows))\n",
    "ds_train = ds_train_full.select(range(val_rows, len(ds_train_full)))\n",
    "\n",
    "print(f\"Splits: {len(ds_train)} train | {len(ds_val)} val | {len(ds_test)} test\")\n",
    "\n",
    "# --------------------------\n",
    "# Tokenizers & Models\n",
    "# --------------------------\n",
    "def load_tokenizers(student_id: str, teacher_id: str):\n",
    "    tok_s = AutoTokenizer.from_pretrained(student_id, use_fast=True)\n",
    "    tok_t = AutoTokenizer.from_pretrained(teacher_id, use_fast=True)\n",
    "    for tok in (tok_s, tok_t):\n",
    "        if tok.pad_token is None and tok.eos_token is not None:\n",
    "            tok.pad_token = tok.eos_token\n",
    "        tok.padding_side = \"left\"  # decoder-only: left padding\n",
    "    return tok_s, tok_t\n",
    "\n",
    "def make_lora_student(model_id: str) -> torch.nn.Module:\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    "    )\n",
    "    base.config.use_cache = False  # off for training\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    )\n",
    "    return get_peft_model(base, lora_cfg)\n",
    "\n",
    "def load_teacher(model_id: str) -> torch.nn.Module:\n",
    "    m = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    "    ).eval()\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad_(False)\n",
    "    return m\n",
    "\n",
    "tok_s, tok_t = load_tokenizers(cfg.student_id, cfg.teacher_id)\n",
    "print(\"Padding sides:\", tok_s.padding_side, tok_t.padding_side)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zd07L4MAVBaT"
   },
   "source": [
    "Let's establish baseline scores on the test set of GSM8K, so we know if we're able to improve the student or not.\n",
    "\n",
    "EM is Exact Match (accuracy score) on the val/test sets of GSM8K. Note that OPD doesn't use answer accuracy to inform weight updates during training! But it's still relevant for us to know since we do care about the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 953,
     "referenced_widgets": [
      "72b8b514a48f4ca0a67b326aa3637771",
      "e565225c22bf46fd830c9aa347e1a6a8",
      "88e2bbeea7a64b1b9ef4814f1ba56265",
      "20685111bc904244953dd37e862a0b94",
      "41aefcb67cd44fcc9ea109e44c634bda",
      "d8fc382cd8b74a639812aa754d2e4892",
      "d00a60f623524613ab067c258792eac8",
      "b14e846853344ef889c9cf4793f6e920",
      "33595340276a44168e654b366eafd3e2",
      "46b1dfdf975c47ef89f6b8bd4d7bb9cd",
      "46c12b05d04148eaab60a9da1173320f",
      "dbc314e21cdf46a7a03cad16457ef48a",
      "8ba6277f82ab42e8a48056062ba9a81f",
      "d6742880c4d141758f69290990682eb3",
      "aa1352423f1e46e0852b708c8bfde7f0",
      "145a45b4adb940d2b3ea87093d2828fa",
      "638bf05ef3554b1c8127f615d11a960b",
      "abb4f1750ea3430cbf35f5676e3c6d35",
      "985cc598cb8a48eb819ae6f87750edc3",
      "969e13906b204dbaab11f69f9b8672d5",
      "58cf0d3d0d1b447ea3605b5843d027ad",
      "f15bd0bdb5fb4c6fa439958c49c12ac2",
      "747786162a9b40edb595a752e163bbfd",
      "84687a3ed5b64274a02ae6ee31ef0e92",
      "858397a66cb4412989c7c81f68d715fe",
      "b9d769bed0f04f2eae41c91b5886dcae",
      "61991f072c8a45469ad81bfea7c10647",
      "6c38ad827a144d41afc40af2ad007ea0",
      "3ade86a91d804158a0d41a082bfdbb83",
      "c41aa0ee9c274c2f89a0b82a519a8984",
      "92743ee5a2bc409f97dff157072b8546",
      "a24c682a4510488484ec35cc7051df93",
      "670c5d1517cb475bb1f56204e36c9300",
      "327d0e946ec147bcaa25192a3724c3ef",
      "4e73fac435dc4babb3b0acaa16dfafb3",
      "3b04bc0be8a7428d85e995eabf58ad98",
      "3a3c682814354662aafa372065e1f7f8",
      "2f066deac70a434a8acb99c7541ded25",
      "8047cf0699e44e918f2edafc8b2949ee",
      "aaac89c673d94a9fa6df18a155751855",
      "4fd3209f80a24a3984f69100c6d1864f",
      "0a2900ee0d1b4e5da83651185d765152",
      "2a1d50658f094af1b38365604d441d44",
      "bbf106cb1cd847688ae0ee7f84502fd8",
      "654d21c66ef6457fb3542127a2b17ce7",
      "f6b2f9280aa34e3a9af8707aebcf0a5c",
      "313fd5d1f9c3427a8e984c6bcf55c6e5",
      "1cbe460ebe00431fbd5d7613ef01629d",
      "e549e16be4114bc28a6215aeecbba54c",
      "4cb1667eaaac41bba521b66c9a24c6a2",
      "75dd6977fde24358a3aec66bccccf922",
      "face3670645e429dbd4a6167c879a3f0",
      "286bebca877741a49615f7a65e26b644",
      "1e9b0a89098d4f59b2980928283d48a2",
      "554e82c41435484886b57451a44d12ec",
      "6161225ac87746089d42eee6f777a67a",
      "10036385709b40adadacec695152b86e",
      "22232cb886e34489908a4afc1209a42f",
      "68bf56984ba14564892b4957ee757481",
      "9fffa449cc2442c6b7f064a0c62a041d",
      "9685b91cec144e0b9e6189fa95e8a311",
      "044e85a64b364e8eab394bf64156f338",
      "77d7e9b3d7514f51b11a7b2b6aeac163",
      "4a0489897a0c43fc909e338de2db2f9d",
      "0ea638797ea942a69e97d49396f302b7",
      "1f87721deb1e49709042ebe285dcce8f",
      "fcd2f948cfc24fd49081d22021f450ff",
      "05143d014b7046a8b71559661e926e26",
      "99eb264f634b47dfa8654ab09b75b839",
      "0227a53c5bc24896a980117c1ac5587b",
      "da1c266b08a24773ab566f5357ce35cf",
      "f7cf4525c1cf463b8f451d5fbe97b749",
      "ba5e3cdf1f86498eab4bac2d07731fd5",
      "73ea9df187134f9485c1705b4427e116",
      "97de14a9f89e4425b370e5a825725409",
      "f66277ce3e2a4e8e8f4f845152f27f0d",
      "806f7d0bf75343c298f16ba120d7fbc8",
      "932117d072dc4976952ddd44b4d857ae",
      "5a8f1773f228411aa55fbaef548957ae",
      "ff31ec78cae74944b0da5a0455e7d1e4",
      "154a9b4cf66a4ae7b172d93268519f82",
      "7540ac9cff284a0ea137d30995c74c64",
      "31fbe096629c4fe6a47340c686abd8dd",
      "8de604c484ee459fb95ec68179b29280",
      "3ebbe2e01b204f8da211e1c85a802b0c",
      "91270d620c4b4f8e95026aa02e840497",
      "2a040fb5c9b04219b6a9ed4ed09d6ba9",
      "064e6f1892c546a293d4a646e913dfd3",
      "f18684d5eecf469ab498336e9f36d23f",
      "965688ca7fde4175b478cba39aba6a8c",
      "084ae7dd0cef46c5a931b76e96b78005",
      "bbc71dab57ae47759c3fd4a5cfd7125e",
      "8f6a18534b7f4d338f7c9fcaeca2b550",
      "63c3b7b18c684ae58a51bb210303db10",
      "7eacbb6c19bc457d821d2f94cbf187cc",
      "e89544b654f144de974d7f8a07a6b790",
      "f534614af70b4e2494c75a752f364a6c",
      "3140d8341db7417a9c8a14b69a543c10",
      "397e6f5cc84844c4a632ec35c5c414a8",
      "00ff538a230e489c92346abec17a6446",
      "f6dbad2f239f411ba6a1b04919ef5e18",
      "a5c390a21731452c85c45c69cd58b0d6",
      "ce1c81f41cf24aa196509f683df889cf",
      "f47e7e34ec4d4d9ab9adef14550126d4",
      "e296d9dcdc43487290702abea4b93c0e",
      "1eef5a94b0794deda9e0a0b28d0653fb",
      "6382f544768a4104bdf76416c864e987",
      "1811f78246ee40b2894275ecc42f837b",
      "b66bc02dbb684e788c522de946bf2fa4",
      "959b656553fc4a64a28464466da0fcf0",
      "cb3b63f7bb364aaab0be66043a4d4fa5",
      "9d6107e082264679b0311586f2b7d524",
      "85212966823a48e4b602e7a8393e5764",
      "7fc214df1aa8443ca5cf91e4dbac821a",
      "3f1cc2d8643d4aec8a73e134dc2c360a",
      "5e2ea541e6ef4bdf92b22fb0f1372c11",
      "767bbfb01e834e8abb404aec39bfffde",
      "25c62b0f02ae43769b631d90b8ea472b",
      "fd98ef81a1cb4625b8a167e7312b6f8e",
      "2b014e1d79584bb0a0ea60b1936a45fa",
      "91ad82dc5db64c1fbe8b9e752f20f536",
      "8bd5ca463fdf45cfb43d788be1d7dbc6",
      "6150491f88694f39b2ff1af4b5de09fe",
      "9b56a1edf4674f82931c6a19ff857bf5",
      "d4bd222988f64eb4b83c06613659732f",
      "984a1714c781437bbd2af2adacdc38f8",
      "6b1bdb9f11134bfcb13d08b66a6962be",
      "a07e306e352c4ae59d427735a866b7ea",
      "67078c2932f7492b881442fd10924ca2",
      "1cda52825fcb4135ad271521db34e655",
      "dc002aabbe5542baafbaa3ee83b75d06",
      "ed66c998da374fae9329c0bb96ee2440",
      "66a0e41f1f864e9c98a73ed426b10419",
      "7fcc714096f14a6d9d560db4b6559d60",
      "0e9f018417ea46dcb6e1939a6f8e87c9",
      "84af1f504a24439ba7ff1d195fb128f4",
      "e14d945061f543d0a025b126dbacf4e6",
      "7aee6d179691471d820a9af73a0f6c54",
      "c95c2e47be504d2881b0001e32bfbc41",
      "399b2269ec9d486bb6b62a735cff7c59",
      "48bce59f7b154bc59a9ca0c7a0b1fd98",
      "0eabc211c3274f8990341133e05e0df5",
      "f5099c05c2744f629ec28998ad1da62c"
     ]
    },
    "id": "ToavEYPqVF6t",
    "outputId": "74a4d12c-002e-4921-e09e-7e6fc0cefc8e"
   },
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------\n",
    "# Evaluation utils\n",
    "# --------------------------\n",
    "def _encode_cuda(tokenizer, texts: List[str], max_length=2048) -> Dict[str, torch.Tensor]:\n",
    "    enc = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    return {k: v.to(\"cuda\") for k, v in enc.items()}\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, tokenizer, dataset, *, num_examples: Optional[int] = None,\n",
    "             temperature: float = 0.0, max_new_tokens: int = 256,\n",
    "             batch_size: int = 32, desc: str = \"Eval\") -> float:\n",
    "    \"\"\"Exact-match accuracy against bracketed numeric answer.\"\"\"\n",
    "    n = len(dataset) if num_examples is None else min(num_examples, len(dataset))\n",
    "    rows = dataset.select(range(n))\n",
    "    correct = 0\n",
    "\n",
    "    was_cache = getattr(model.config, \"use_cache\", True)\n",
    "    model.eval(); model.config.use_cache = True\n",
    "\n",
    "    for i in tqdm(range(0, n, batch_size), desc=desc):\n",
    "        batch = rows.select(range(i, min(i + batch_size, n)))\n",
    "        prompts = [render_prompt(ex[\"question\"]) for ex in batch]\n",
    "        enc = _encode_cuda(tokenizer, prompts)\n",
    "\n",
    "        gen_kwargs = dict(\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True\n",
    "        )\n",
    "        if temperature and temperature > 0.0:\n",
    "            gen_kwargs.update(do_sample=True, temperature=temperature, top_p=0.9)\n",
    "        else:\n",
    "            gen_kwargs.update(do_sample=False)\n",
    "\n",
    "        outs = model.generate(**enc, **gen_kwargs)\n",
    "        txts = tokenizer.batch_decode(outs, skip_special_tokens=True)\n",
    "\n",
    "        for ex, text in zip(batch, txts):\n",
    "            pred = parse_pred(text) or \"\"\n",
    "            gold = parse_gold(ex[\"answer\"]) or \"\"\n",
    "            correct += int(pred == gold)\n",
    "\n",
    "    model.config.use_cache = was_cache\n",
    "    return correct / max(n, 1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def preview(model, tokenizer, dataset, k=2, temperature=0.0, max_new_tokens=256):\n",
    "    model.eval(); model.config.use_cache = True\n",
    "    rows = dataset.select(range(min(k, len(dataset))))\n",
    "    for ex in rows:\n",
    "        prompt = render_prompt(ex[\"question\"])\n",
    "        enc = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "        gen_kwargs = dict(max_new_tokens=max_new_tokens, use_cache=True,\n",
    "                          pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "        if temperature and temperature > 0.0:\n",
    "            gen_kwargs.update(do_sample=True, temperature=temperature, top_p=0.9)\n",
    "        out = model.generate(**enc, **gen_kwargs)\n",
    "        text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        print(\"=\"*80); print(prompt); print(\"-\"*80); print(text)\n",
    "        print(\"-\"*80, f\"\\nParsed: [{parse_pred(text)}] | Gold: [{parse_gold(ex['answer'])}]\")\n",
    "\n",
    "# --------------------------\n",
    "# Baselines (before training)\n",
    "# --------------------------\n",
    "print(\"\\n== Loading models for baseline evals ==\")\n",
    "student_for_eval = make_lora_student(cfg.student_id)\n",
    "teacher = load_teacher(cfg.teacher_id)\n",
    "\n",
    "print(\"\\nPreview (student, greedy)â€¦\")\n",
    "preview(student_for_eval, tok_s, ds_test, k=1, temperature=0.0, max_new_tokens=cfg.max_new_tokens)\n",
    "\n",
    "print(\"\\nComputing baselines (greedy)â€¦\")\n",
    "baseline_student_em = evaluate(student_for_eval, tok_s, ds_test,\n",
    "                               temperature=cfg.eval_temperature, max_new_tokens=cfg.max_new_tokens,\n",
    "                               batch_size=128, desc=\"Student baseline (test)\")\n",
    "baseline_teacher_em = evaluate(teacher, tok_t, ds_test,\n",
    "                               temperature=cfg.eval_temperature, max_new_tokens=cfg.max_new_tokens,\n",
    "                               batch_size=64, desc=\"Teacher baseline (test)\")\n",
    "print(f\"Student (0.6B base) EM: {baseline_student_em:.4f}\")\n",
    "print(f\"Teacher (4B instruct) EM: {baseline_teacher_em:.4f}\")\n",
    "\n",
    "with open(os.path.join(cfg.run_root, \"baselines.json\"), \"w\") as f:\n",
    "    json.dump(dict(student_0p6b=baseline_student_em, teacher_4b=baseline_teacher_em), f, indent=2)\n",
    "\n",
    "# Free the temporary student used just for baselines\n",
    "del student_for_eval; torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gpLOq52TMRl"
   },
   "source": [
    "Now let's actually do the OPD training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJ2g5hwKT24X"
   },
   "source": [
    "As we train, we'll output the following metrics every 10 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69mYi3WcTtnj"
   },
   "source": [
    "| Column        | Meaning                                                                                                                                                                                                         | How to interpret                                                                      |\n",
    "| :------------ | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------ |\n",
    "| **step**      | The current training iteration (out of the total configured `cfg.steps`, e.g. 100). Each step processes one batch of sampled prompts.                                                                           | Training progresses along this axis.                                                  |\n",
    "| **loss**      | The instantaneous batch loss (reverse-KLâ€“style policy-gradient objective). Negative values are expected because we optimize `-E[(log p_T âˆ’ log p_S) Â· log p_S]`; good updates drive this lower (more negative). | Lower (more negative) â†’ better alignment with teacher on that batch.                  |\n",
    "| **loss_ema**  | Exponential moving average (EMA) of the loss across steps (with momentum = `cfg.ema_momentum`, e.g. 0.9). Smooths noise to show trend.                                                                          | Downward trend = overall improvement; steadiness = convergence.                       |\n",
    "| **revkl**     | The *reverse-KL divergence estimate* for this batch: roughly E[ log p_S âˆ’ log p_T ]. It measures how far the studentâ€™s token distribution is from the teacherâ€™s on its own rollouts.                            | Smaller (approaching 0) â†’ student policy is closer to teacher.                        |\n",
    "| **revkl_ema** | EMA-smoothed version of reverse-KL, again for trend stability.                                                                                                                                                  | Should decrease steadily if distillation is working.                                  |\n",
    "| **tokens**    | Cumulative count of *valid graded tokens* seen so far â€” all pre-EOS generated tokens used for loss computation. It grows as training proceeds.                                                                  | Measures total learning signal processed; roughly proportional to compute/throughput. |\n",
    "| **val_em**    | Validation exact-match accuracy (fraction of GSM8K val examples where the parsed numeric answer matches gold). Evaluated every `cfg.val_every` steps on `cfg.val_sample_n` examples.                            | Direct measure of task performance; higher = better reasoning accuracy.               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 995,
     "referenced_widgets": [
      "365d83a1bc2140df97929c63f3c94d9f",
      "a3c10a6f5ec0479cbefa9bc2a01b8be7",
      "e330eaa07e644500955ae051ac8011b4",
      "53f75928a99a48b0b9af904588533326",
      "c8690b0e8e65438788062e181d94a2d6",
      "636e410e70d94503b4c8195a702bdb4e",
      "75ed02a3a6654802929b942d1dd28b2e",
      "56ce961cdb4c46bdb935bf010ad4875b",
      "246775adef3548cab461ec3586a528c3",
      "354256765ff140888bd5602070883643",
      "0f741692af69453a9e7caba4825965a2",
      "e4c887fc8d7a49d08b78de6f2af3f2c1",
      "7c6a69c08ef3499cb5090856fedbcf5d",
      "e3d70f0f33d84485aa322f3cdab3f4e0",
      "d935dfd3489b4621a5d10e3625574a0c",
      "65e5583b39964e1c90c9d9bdb5bb67eb",
      "a4ac88e6956f4774889c1cccd3c29297",
      "81df4caf536a43a39d0a718805b5fc7f",
      "7a31e3f5d3644e7c9bd71fc924fe51a6",
      "9ce1fc521a544627aaa0e85e779b0e23",
      "52132990d5b94deab10b468af8e938d7",
      "23305fc26a9743bd9315e6def42e5a8b",
      "a3fee3b38de949a288b2559a0b67d920",
      "eeb44e0193354d47b58baa48c89a2de7",
      "fa0a2ffaecbf44cf98750381ef6e5b1b",
      "a186ad8e50174791bc61aea8acc18ef0",
      "2efa2be3957d45d683f9eb817f12faff",
      "86d0274eea194b9ca5eae1483f7f7514",
      "a3579ceb44ba425cb0a29069f9344e44",
      "39739a50260a4c6f96def365cfa4b1ee",
      "8a14875674024b379327f9c863ff128a",
      "57c891e3b4034dc990f5414cd3259407",
      "c3c9698ad75a4d49a6efd41c673ae3f6",
      "83567dec2e284805b9afbf20b66b94cd",
      "80279bc7093a4a458c477f6966200dba",
      "b6fbc6349b284cf3ac60c76a396c1536",
      "6aa3e50a613043a5a4d579d9468a3b61",
      "7cf63ec8f716452eb1108b6923c22855",
      "dc386c60f668437ba1143cbfd8f4402b",
      "568d221533b24357945b4bd00e97f862",
      "4302a71ada0b439eab43875684f07db3",
      "3548b948f26e42ffa8ca2b92426ca0a9",
      "898c84f334ab456db5b6d52b32b8d791",
      "001824709b02447097282f556a1397ab",
      "7eddc91e84554878821a1c3a698ebf6c",
      "a8e76861c5214acd9d0798409e7206e6",
      "ffcd33497104439a957b92bee73a053e",
      "4761f9dc5bfd403ca170fcd2752c5761",
      "3e699380bf28412c866b0c1a2d4f5ef5",
      "d234220bb5cc48c58e423940c038155d",
      "a2637fbec04f4777aed0be9ff3119103",
      "2d175be415e449fc8b950748d97613b0",
      "b2d9c1a425be44a4ab6340546e4a3855",
      "52cb195e840244a7bbb1a8dfe8729f76",
      "ab810b6f5fab4576b3b84cd7a4d5ad3e",
      "6aa7ed15260049b683f8ee069873c390",
      "c38c1c86c0ca47408e43905cdda318cd",
      "8397c33566e942788d6ff99f9ff61f90",
      "66937d5026594b64800ed060a1f0c56c",
      "348b524d718e4669afc7330b3bcd1f5a",
      "b673baa42538418f9188033092da6b44",
      "86daea8296134a92aa34ec2ea175cb59",
      "5579fcd49f5c4889aa7cf682b4b55ca0",
      "b38f541499534f1f9a1795b16cbdaf27",
      "7c37b4081fdd4ed89d9b5947bd1db32c",
      "134310ca17944e46bb60582dc0a58831",
      "9827c4cff73a4005810390abb422ffd3",
      "20e6c487889c419bb75dc50a9ee4ea47",
      "5fddd8f0e8da4a03b098405450e989c4",
      "674b83673c424b518d1e18f0dc62f003",
      "002d79afffbb4eb98995b25bbe7e5bdc",
      "6d37d9b236a241b99d43e9d5212da62c",
      "b3d49b863ec94a27a1379bc46313b85f",
      "a0d54f5e6dff4a8dbed4dd3c789702e4",
      "2205e89386794d8aa569f579ee829d70",
      "71ef247ae3ff4b2691c29b63bec062e4",
      "91513f11a5d741e29258133b759a0737",
      "6f6c6e990cb947bb81fcd35b99381afe",
      "9796bdb43bd7448f87dd671f4bd148c3",
      "6e22bda2d4b3456c855ec9ed78754f98",
      "8566e90613004abd941063a56ee4a380",
      "1694eb0f3b894c4db18e5a562739046a",
      "b795df943a454d1f985ed47d5f9375a9",
      "af7a824b26404eef8f68c016760c3bae",
      "78c4efbf98c7433eac32d6aaf774429a",
      "0cf307e959274c56862cd371d84f945a",
      "63bd646e1ffe407c8705c4f8fa12beba",
      "7398c9cf9b464e0eade7794bf50f9f09"
     ]
    },
    "id": "tY7bzIZ6DwLc",
    "outputId": "acfe01bd-4c4e-4da0-96dc-d86062195dc3"
   },
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------\n",
    "# OPD utilities\n",
    "# --------------------------\n",
    "def mask_before_first_eos(next_ids: torch.Tensor, eos_id: int) -> torch.Tensor:\n",
    "    \"\"\"Mask tokens strictly before the first EOS in each sequence.\"\"\"\n",
    "    is_eos = (next_ids == eos_id)\n",
    "    csum = is_eos.cumsum(dim=1)\n",
    "    return csum.eq(0)\n",
    "\n",
    "def student_logp_batched(student, pad_id, full_ids, next_ids, T, micro_bsz):\n",
    "    \"\"\"Return student log p(a_t) for the last T tokens (with grad).\"\"\"\n",
    "    outs = []\n",
    "    for s in range(0, full_ids.size(0), micro_bsz):\n",
    "        chunk = full_ids[s:s+micro_bsz]; nxt = next_ids[s:s+micro_bsz]\n",
    "        out = student(input_ids=chunk[:, :-1],\n",
    "                      attention_mask=(chunk[:, :-1] != pad_id))\n",
    "        logits = out.logits[:, -T:, :]\n",
    "        logp = F.log_softmax(logits, dim=-1).gather(-1, nxt.unsqueeze(-1)).squeeze(-1)\n",
    "        outs.append(logp)\n",
    "        del out, logits\n",
    "    return torch.cat(outs, dim=0)\n",
    "\n",
    "# ---------- Cross-tokenizer teacher scoring ----------\n",
    "def _decode_token_str(tokenizer, token_id: int) -> str:\n",
    "    # Decode one token to text exactly as-is (keep spaces/prefixes).\n",
    "    return tokenizer.decode([int(token_id)],\n",
    "                            skip_special_tokens=False,\n",
    "                            clean_up_tokenization_spaces=False)\n",
    "\n",
    "def _encode_text_ids(tokenizer, text: str):\n",
    "    return tokenizer(text,\n",
    "                     add_special_tokens=False,\n",
    "                     return_tensors=\"pt\").input_ids[0].tolist()\n",
    "\n",
    "@torch.no_grad()\n",
    "def teacher_logp_grouped_by_student_tokens(\n",
    "    teacher, tok_teacher, tok_student, prompts: List[str], next_ids: torch.Tensor, max_len: Optional[int] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    For each sample b and student step t:\n",
    "      1) Decode the student's token id next_ids[b,t] â†’ text piece\n",
    "      2) Tokenize that text with the teacher tokenizer (may become multiple tokens)\n",
    "      3) Sum teacher log-probs over that group\n",
    "    Returns: Tensor [B, T] on CUDA with per-student-step teacher log-probs.\n",
    "    \"\"\"\n",
    "    device = teacher.device if hasattr(teacher, \"device\") else \"cuda\"\n",
    "    B, T = next_ids.shape\n",
    "    out = torch.zeros((B, T), device=device, dtype=torch.float32)\n",
    "\n",
    "    if max_len is None:\n",
    "        max_len = int(getattr(teacher.config, \"max_position_embeddings\", 2048))\n",
    "\n",
    "    for b in range(B):\n",
    "        prompt_text = prompts[b]\n",
    "        ctx_ids = _encode_text_ids(tok_teacher, prompt_text)\n",
    "\n",
    "        groups = []\n",
    "        for t in range(T):\n",
    "            s_tok_id = int(next_ids[b, t].item())\n",
    "            piece = _decode_token_str(tok_student, s_tok_id)\n",
    "            ids_t = _encode_text_ids(tok_teacher, piece)\n",
    "            groups.append(ids_t)\n",
    "\n",
    "        flat_gen = [tid for g in groups for tid in g]\n",
    "        if len(flat_gen) == 0:\n",
    "            continue\n",
    "\n",
    "        # Respect the teacher's context length by trimming left context\n",
    "        total = len(ctx_ids) + len(flat_gen)\n",
    "        if total > max_len:\n",
    "            overflow = total - max_len\n",
    "            ctx_ids = ctx_ids[overflow:]\n",
    "\n",
    "        # Build teacher-forcing inputs; labels are shifted by one token\n",
    "        full = ctx_ids + flat_gen\n",
    "        if len(full) < 2:\n",
    "            continue\n",
    "\n",
    "        input_ids = torch.tensor(full[:-1], device=device).unsqueeze(0)\n",
    "        labels    = torch.tensor(full[1:],  device=device).unsqueeze(0)\n",
    "        attn_mask = torch.ones_like(input_ids, device=device)\n",
    "\n",
    "        outputs = teacher(input_ids=input_ids, attention_mask=attn_mask)\n",
    "        logprobs = F.log_softmax(outputs.logits, dim=-1)\n",
    "        tok_lp = logprobs.gather(-1, labels.unsqueeze(-1)).squeeze(-1)[0]  # [L]\n",
    "\n",
    "        # Labels index full[1:], so generated part starts at k = len(ctx_ids)-1\n",
    "        start = max(len(ctx_ids) - 1, 0)\n",
    "        gen_lp = tok_lp[start : start + len(flat_gen)]\n",
    "\n",
    "        # Sum back per student step\n",
    "        off = 0\n",
    "        for t, g in enumerate(groups):\n",
    "            k = len(g)\n",
    "            if k > 0:\n",
    "                out[b, t] = gen_lp[off : off + k].sum()\n",
    "            off += k\n",
    "\n",
    "    return out\n",
    "\n",
    "class LiveTable:\n",
    "    def __init__(self, title: str = \"Training metrics\", max_rows: int = 200):\n",
    "        self.title = title\n",
    "        self.max_rows = max_rows\n",
    "        self.rows = []\n",
    "        empty = pd.DataFrame(columns=[\"step\",\"loss\",\"loss_ema\",\"revkl\",\"revkl_ema\",\"tokens\",\"val_em\"])\n",
    "        self.handle = display(self._styled(empty), display_id=True)\n",
    "\n",
    "    def _styled(self, df: pd.DataFrame):\n",
    "        styler = (\n",
    "            df.style\n",
    "              .set_caption(self.title)\n",
    "              .format({\n",
    "                  \"loss\": \"{:.4f}\",\n",
    "                  \"loss_ema\": \"{:.4f}\",\n",
    "                  \"revkl\": \"{:.4f}\",\n",
    "                  \"revkl_ema\": \"{:.4f}\",\n",
    "                  \"val_em\": (lambda v: \"\" if pd.isna(v) else f\"{v:.3f}\"),\n",
    "              })\n",
    "        )\n",
    "        try:\n",
    "            styler = styler.hide(axis=\"index\")\n",
    "            return styler\n",
    "        except Exception:\n",
    "            pass\n",
    "        return styler.set_table_styles([\n",
    "            {\"selector\": \"th.row_heading\", \"props\": [(\"display\", \"none\")]},\n",
    "            {\"selector\": \"th.blank\",       \"props\": [(\"display\", \"none\")]},\n",
    "        ])\n",
    "\n",
    "    def update(self, *, step, loss, loss_ema, revkl, revkl_ema, tokens, val_em=None):\n",
    "        self.rows.append(dict(\n",
    "            step=int(step),\n",
    "            loss=float(loss),\n",
    "            loss_ema=(None if loss_ema is None else float(loss_ema)),\n",
    "            revkl=float(revkl),\n",
    "            revkl_ema=(None if revkl_ema is None else float(revkl_ema)),\n",
    "            tokens=int(tokens),\n",
    "            val_em=(None if val_em is None else float(val_em)),\n",
    "        ))\n",
    "        rows = self.rows[-self.max_rows:]\n",
    "        df = pd.DataFrame(rows, columns=[\"step\",\"loss\",\"loss_ema\",\"revkl\",\"revkl_ema\",\"tokens\",\"val_em\"])\n",
    "        self.handle.update(self._styled(df))\n",
    "\n",
    "# --------------------------\n",
    "# Training loop (verbose)\n",
    "# --------------------------\n",
    "def ema(prev, new, beta):\n",
    "    return new if prev is None else (beta * prev + (1 - beta) * new)\n",
    "\n",
    "def run_training(run_dir: str):\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    # Fresh student (LoRA adapters)\n",
    "    student = make_lora_student(cfg.student_id)\n",
    "    optimizer = torch.optim.AdamW(student.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    wandb.watch(student, log=\"all\", log_freq=100)\n",
    "\n",
    "    prompts_all = [render_prompt(x[\"question\"]) for x in ds_train]\n",
    "    ema_loss = None\n",
    "    ema_revkl = None\n",
    "    tokens_graded_cum = 0\n",
    "    logs = []\n",
    "\n",
    "    table = LiveTable(title=\"On-Policy Distillation\")\n",
    "\n",
    "    pbar = tqdm(range(cfg.steps), desc=f\"OPD [{os.path.basename(run_dir)}]\")\n",
    "    for step in pbar:\n",
    "        # Deterministic batch selection per step\n",
    "        rng = np.random.default_rng(SEED + step)\n",
    "        idxs = rng.choice(len(prompts_all), size=cfg.batch_prompts, replace=False)\n",
    "        prompts = [prompts_all[i] for i in idxs]\n",
    "        prompts_rep = sum(([p] * cfg.samples_per_prompt for p in prompts), [])\n",
    "        enc = tok_s(prompts_rep, padding=True, truncation=True, max_length=2048, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        # 1) Student rollouts (no grad) to get sequences and step count\n",
    "        with torch.no_grad():\n",
    "            gen_out = student.generate(\n",
    "                **enc,\n",
    "                do_sample=True, temperature=cfg.train_temperature, top_p=0.9,\n",
    "                max_new_tokens=cfg.max_new_tokens,\n",
    "                eos_token_id=tok_s.eos_token_id, pad_token_id=tok_s.pad_token_id,\n",
    "                return_dict_in_generate=True, output_scores=True\n",
    "            )\n",
    "            seqs = gen_out.sequences\n",
    "            scores_list = list(gen_out.scores)  # per-step logits if you want to inspect\n",
    "            T = len(scores_list)\n",
    "            next_ids = seqs[:, -T:]\n",
    "            valid_mask = mask_before_first_eos(next_ids, eos_id=tok_s.eos_token_id).float()\n",
    "\n",
    "        # 2) Student log-probs with grad\n",
    "        student.train(); student.config.use_cache = False\n",
    "        logp_s = student_logp_batched(\n",
    "            student, tok_s.pad_token_id, seqs, next_ids, T, cfg.student_mb\n",
    "        )\n",
    "\n",
    "        # 3) Teacher log-probs (no grad), robust to tokenizer mismatches\n",
    "        teacher.eval()\n",
    "        logp_t = teacher_logp_grouped_by_student_tokens(\n",
    "            teacher=teacher,\n",
    "            tok_teacher=tok_t,\n",
    "            tok_student=tok_s,\n",
    "            prompts=prompts_rep,                  # the prompts used for these rollouts\n",
    "            next_ids=next_ids,                    # [B, T] student IDs\n",
    "            max_len=getattr(teacher.config, \"max_position_embeddings\", 2048),\n",
    "        )\n",
    "\n",
    "        # 4) Reverse-KL-style policy gradient\n",
    "        adv = (logp_t - logp_s).clamp(-5, 5)  # detach below for stability\n",
    "        denom = valid_mask.sum().clamp_min(1.0)\n",
    "        loss = - ((valid_mask * adv.detach()) * logp_s).sum() / denom\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rev_kl = ((logp_s - logp_t) * valid_mask).sum().item() / float(denom)\n",
    "            tokens_graded_cum += int(denom.item())\n",
    "            ema_loss = ema(ema_loss, float(loss.item()), cfg.ema_momentum)\n",
    "            ema_revkl = ema(ema_revkl, float(rev_kl), cfg.ema_momentum)\n",
    "\n",
    "        # Periodic validation EM (greedy)\n",
    "        val_em = None\n",
    "        if (step % cfg.val_every == 0) or (step == cfg.steps - 1):\n",
    "            student.eval(); student.config.use_cache = True\n",
    "            val_em = evaluate(student, tok_s, ds_val,\n",
    "                              num_examples=min(cfg.val_sample_n, len(ds_val)),\n",
    "                              temperature=0.0, max_new_tokens=cfg.max_new_tokens,\n",
    "                              batch_size=32, desc=\"VAL EM\")\n",
    "            student.train(); student.config.use_cache = False\n",
    "\n",
    "        row = dict(\n",
    "            step=int(step),\n",
    "            train_loss=float(loss.item()),\n",
    "            train_loss_ema=float(ema_loss) if ema_loss is not None else None,\n",
    "            train_revkl=float(rev_kl),\n",
    "            train_revkl_ema=float(ema_revkl) if ema_revkl is not None else None,\n",
    "            tokens_graded=int(tokens_graded_cum),\n",
    "            **({\"val_em\": float(val_em)} if val_em is not None else {})\n",
    "        )\n",
    "        logs.append(row)\n",
    "        wandb.log(row)\n",
    "\n",
    "        if (step % cfg.log_every == 0) or (val_em is not None):\n",
    "            table.update(\n",
    "                step=row[\"step\"],\n",
    "                loss=row[\"train_loss\"],\n",
    "                loss_ema=row[\"train_loss_ema\"],\n",
    "                revkl=row[\"train_revkl\"],\n",
    "                revkl_ema=row[\"train_revkl_ema\"],\n",
    "                tokens=row[\"tokens_graded\"],\n",
    "                val_em=row.get(\"val_em\", None),\n",
    "            )\n",
    "\n",
    "        postfix = {\n",
    "            \"loss\": f\"{loss.item():.3f}\",\n",
    "            \"ema\": f\"{(ema_loss if ema_loss is not None else loss.item()):.3f}\",\n",
    "            \"rkl\": f\"{rev_kl:.3f}\",\n",
    "            \"toks\": tokens_graded_cum\n",
    "        }\n",
    "        if val_em is not None:\n",
    "            postfix[\"val\"] = f\"{val_em:.3f}\"\n",
    "        pbar.set_postfix(**postfix)\n",
    "\n",
    "        del scores_list\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Save logs\n",
    "    try:\n",
    "        pd.DataFrame(logs).to_csv(os.path.join(run_dir, \"train_logs.csv\"), index=False)\n",
    "    except Exception:\n",
    "        with open(os.path.join(run_dir, \"train_logs.jsonl\"), \"w\") as f:\n",
    "            for r in logs:\n",
    "                f.write(json.dumps(r) + \"\\n\")\n",
    "\n",
    "    # Final test EM (greedy)\n",
    "    student.eval(); student.config.use_cache = True\n",
    "    test_em = evaluate(student, tok_s, ds_test,\n",
    "                       temperature=0.0, max_new_tokens=cfg.max_new_tokens,\n",
    "                       batch_size=64, desc=f\"Test EM [{os.path.basename(run_dir)}]\")\n",
    "\n",
    "    wandb.summary[\"final_test_em\"] = test_em\n",
    "\n",
    "    # Save adapters and summary\n",
    "    save_dir = os.path.join(run_dir, \"adapters_lora\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    student.save_pretrained(save_dir)\n",
    "    \n",
    "    print(\"Merging LoRA adapters into the base model...\")\n",
    "    # This returns a new, standard Hugging Face model\n",
    "    merged_model = student.merge_and_unload()\n",
    "    print(\"Merge complete.\")\n",
    "    \n",
    "    merged_model.push_to_hub(\n",
    "        repo_id=HF_HUB_REPO,\n",
    "        commit_message=f\"End of training for wandb run {wandb.run.id}. Test EM: {test_em:.4f}\",\n",
    "        private=True # Set to False if you want the model to be public\n",
    "    )\n",
    "    tok_s.push_to_hub(\n",
    "        repo_id=HF_HUB_REPO,\n",
    "        commit_message=f\"End of training for wandb run {wandb.run.id}. Test EM: {test_em:.4f}\",\n",
    "        private=True # Set to False if you want the model to be public\n",
    "    )\n",
    "\n",
    "    summary = dict(\n",
    "        steps=cfg.steps,\n",
    "        batch_prompts=cfg.batch_prompts,\n",
    "        samples_per_prompt=cfg.samples_per_prompt,\n",
    "        max_new_tokens=cfg.max_new_tokens,\n",
    "        train_tokens_graded=tokens_graded_cum,\n",
    "        test_em=float(test_em)\n",
    "    )\n",
    "    wandb.log(summary)\n",
    "    # --- W&B: Log model adapters and log files as artifacts ---\n",
    "    # Log the trained LoRA adapters\n",
    "    adapters_artifact = wandb.Artifact(name=f\"lora-adapters-{wandb.run.id}\", type=\"model\")\n",
    "    adapters_artifact.add_dir(save_dir)\n",
    "    wandb.log_artifact(adapters_artifact)\n",
    "\n",
    "    with open(os.path.join(run_dir, \"summary.json\"), \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    # Free GPU\n",
    "    del student; torch.cuda.empty_cache()\n",
    "    return summary, logs\n",
    "\n",
    "# --------------------------\n",
    "# Run training (single run)\n",
    "# --------------------------\n",
    "print(\"\\n== Training (OPD) ==\")\n",
    "run_dir = os.path.join(cfg.run_root, \"opd\")\n",
    "\n",
    "\n",
    "# Wrap the training execution with wandb.init\n",
    "# This will start a new run and automatically sync all data.\n",
    "with wandb.init(\n",
    "    project=\"on-policy-distillation\", # CHANGE: Your project name\n",
    "    name=f\"opd_run_{os.path.basename(cfg.run_root)}\", # Optional: A custom run name\n",
    "    job_type=\"training\",\n",
    "    config=vars(cfg) # Log all hyperparameters from your config object\n",
    ") as run:\n",
    "    summary, _ = run_training(run_dir)\n",
    "\n",
    "\n",
    "print(\"\\n== Final Results ==\")\n",
    "print(json.dumps(dict(\n",
    "    env=dict(\n",
    "        python=sys.version,\n",
    "        torch=torch.__version__,\n",
    "        cuda=torch.version.cuda if torch.cuda.is_available() else \"cpu\",\n",
    "        device=torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\",\n",
    "    ),\n",
    "    prompt_template=cfg.prompt_template,\n",
    "    baselines=dict(student_0p6b=baseline_student_em, teacher_4b=baseline_teacher_em),\n",
    "    final=summary,\n",
    "), indent=2))\n",
    "print(\"Artifacts saved to:\", cfg.run_root)\n",
    "\n",
    "print(\"Artifacts saved to:\", cfg.run_root)\n",
    "print(f\"Find your run online at: {wandb.run.get_url()}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
