{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Multi-Teacher On-Policy Distillation\n",
    "\n",
    "This notebook demonstrates **multi-teacher on-policy distillation (OPD)**, an extension of standard on-policy distillation where a student model learns from **multiple teacher models** simultaneously.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "Unlike single-teacher distillation, multi-teacher distillation allows:\n",
    "- **Diverse knowledge sources**: Each teacher may excel at different aspects of the task\n",
    "- **Ensemble benefits**: Combining multiple teachers can produce better guidance than any single teacher\n",
    "- **Specialized teachers**: Different teachers can be trained on different datasets or with different objectives\n",
    "\n",
    "In this notebook, you'll:\n",
    "1. Train a small student model (Qwen3-0.6B) using two teacher models\n",
    "2. Teacher 1: Qwen3-4B-Instruct-2507 (strong reasoning)\n",
    "3. Teacher 2: Qwen3-3B-Instruct (faster, different training)\n",
    "4. Learn on [GSM8K](https://huggingface.co/datasets/openai/gsm8k) math problems\n",
    "\n",
    "## Requirements\n",
    "\n",
    "You'll need an **A100 GPU (40 GB)** or better. With smaller GPUs, reduce `samples_per_prompt` and `max_new_tokens`.\n",
    "\n",
    "## References\n",
    "\n",
    "- Blog: [Thinking Machines - On-Policy Distillation](https://thinkingmachines.ai/blog/on-policy-distillation/)\n",
    "- Paper: [Agarwal et al. 2024](https://arxiv.org/abs/2306.13649)\n",
    "- Code: [tinker-cookbook multi-teacher implementation](https://github.com/thinking-machines-lab/tinker-cookbook/blob/main/tinker_cookbook/recipes/distillation/on_policy_multi_teacher.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ðŸ› ï¸ Setup\n",
    "!nvidia-smi -L || true\n",
    "\n",
    "import os, sys, random, numpy as np, torch, json, time, platform, math\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Qwen3 requires Transformers >= 4.51\n",
    "try:\n",
    "    get_ipython().run_line_magic(\"uv\", \"pip -q install transformers==4.51.3 accelerate==1.4.0 peft==0.14.0 datasets==3.3.2 evaluate==0.4.3 sentencepiece protobuf tqdm matplotlib > /dev/null\")\n",
    "except Exception:\n",
    "    get_ipython().run_line_magic(\"pip\", \"-q install transformers==4.51.3 accelerate==1.4.0 peft==0.14.0 datasets==3.3.2 evaluate==0.4.3 sentencepiece protobuf tqdm matplotlib > /dev/null\")\n",
    "\n",
    "import transformers, datasets, peft, accelerate, matplotlib\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Accelerate:\", accelerate.__version__)\n",
    "print(\"PEFT:\", peft.__version__)\n",
    "print(\"matplotlib:\", matplotlib.__version__)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "assert DEVICE == \"cuda\", \"Please connect a GPU (A100+ recommended).\"\n",
    "\n",
    "def print_header():\n",
    "    print(\"== Environment ==\")\n",
    "    print(dict(\n",
    "        python=sys.version,\n",
    "        torch=torch.__version__,\n",
    "        transformers=transformers.__version__,\n",
    "        accelerate=accelerate.__version__,\n",
    "        peft=peft.__version__,\n",
    "        cuda=torch.version.cuda if torch.cuda.is_available() else \"cpu\",\n",
    "        device_name=torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\",\n",
    "        platform=platform.platform(),\n",
    "        seed=SEED\n",
    "    ))\n",
    "print_header()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, json, random, platform\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import re\n",
    "\n",
    "# --------------------------\n",
    "# Reproducibility & device\n",
    "# --------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "assert DEVICE == \"cuda\", \"A CUDA GPU is required.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "The key difference from single-teacher OPD is that we now specify **multiple teachers**. Each teacher contributes to the distillation with a configurable weight.\n",
    "\n",
    "### Teacher Weighting Strategies\n",
    "\n",
    "1. **Uniform**: Equal weight to all teachers (default)\n",
    "2. **Performance-based**: Weight by baseline accuracy\n",
    "3. **Dynamic**: Adjust weights during training based on agreement\n",
    "\n",
    "For this demo, we use **uniform weighting** for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Multi-Teacher Config\n",
    "# --------------------------\n",
    "@dataclass\n",
    "class TeacherConfig:\n",
    "    \"\"\"Configuration for a single teacher model.\"\"\"\n",
    "    model_id: str\n",
    "    weight: float = 1.0  # Contribution weight (will be normalized)\n",
    "    description: str = \"\"\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Student model\n",
    "    student_id: str = \"Qwen/Qwen3-0.6B-Base\"\n",
    "    \n",
    "    # Multiple teachers with their configurations\n",
    "    teachers: List[TeacherConfig] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.teachers is None:\n",
    "            # Default: two teachers with equal weights\n",
    "            self.teachers = [\n",
    "                TeacherConfig(\n",
    "                    model_id=\"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "                    weight=1.0,\n",
    "                    description=\"Strong reasoning, latest version\"\n",
    "                ),\n",
    "                TeacherConfig(\n",
    "                    model_id=\"Qwen/Qwen3-3B-Instruct\",\n",
    "                    weight=1.0,\n",
    "                    description=\"Faster, different training distribution\"\n",
    "                ),\n",
    "            ]\n",
    "        # Normalize weights\n",
    "        total_weight = sum(t.weight for t in self.teachers)\n",
    "        for t in self.teachers:\n",
    "            t.weight = t.weight / total_weight\n",
    "\n",
    "    # Prompting\n",
    "    prompt_template: str = (\n",
    "        \"Solve step by step.\\n\"\n",
    "        \"Give ONLY ONE final numeric answer (no units), inside square brackets.\\n\"\n",
    "        \"Problem: {question}\\n\\nSolution:\"\n",
    "    )\n",
    "    max_new_tokens: int = 256\n",
    "\n",
    "    # Generation temps\n",
    "    eval_temperature: float = 0.0   # greedy for eval\n",
    "    train_temperature: float = 0.7  # sampling for on-policy data\n",
    "\n",
    "    # Training schedule\n",
    "    steps: int = 50\n",
    "    batch_prompts: int = 4\n",
    "    samples_per_prompt: int = 4\n",
    "    lr: float = 1e-4\n",
    "    weight_decay: float = 0.0\n",
    "    grad_accum: int = 1\n",
    "\n",
    "    # Micro-batching\n",
    "    student_mb: int = 8\n",
    "\n",
    "    # Monitoring\n",
    "    log_every: int = 10\n",
    "    val_every: int = 10\n",
    "    val_sample_n: int = 100\n",
    "    ema_momentum: float = 0.9\n",
    "\n",
    "    # Validation size\n",
    "    val_rows: Optional[int] = None\n",
    "\n",
    "    # Output dir\n",
    "    run_root: str = f\"./run_multi_teacher_{int(time.time())}\"\n",
    "\n",
    "cfg = Config()\n",
    "os.makedirs(cfg.run_root, exist_ok=True)\n",
    "\n",
    "print(\"\\n=== Multi-Teacher Configuration ===\")\n",
    "print(f\"Student: {cfg.student_id}\")\n",
    "print(f\"\\nTeachers ({len(cfg.teachers)}):\")\n",
    "for i, t in enumerate(cfg.teachers, 1):\n",
    "    print(f\"  {i}. {t.model_id}\")\n",
    "    print(f\"     Weight: {t.weight:.3f}\")\n",
    "    print(f\"     Description: {t.description}\")\n",
    "print(f\"\\nTraining steps: {cfg.steps}\")\n",
    "print(f\"Batch size: {cfg.batch_prompts} prompts Ã— {cfg.samples_per_prompt} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Dataset: GSM8K\n",
    "\n",
    "We use the same dataset and parsing utilities as single-teacher OPD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Data: GSM8K\n",
    "# --------------------------\n",
    "def render_prompt(question: str) -> str:\n",
    "    return cfg.prompt_template.format(question=question)\n",
    "\n",
    "def parse_gold(answer_text: str) -> Optional[str]:\n",
    "    m = re.search(r\"####\\s*(-?\\d+(?:\\.\\d+)?)\", answer_text)\n",
    "    if m: return m.group(1).strip()\n",
    "    nums = re.findall(r\"-?\\d+(?:\\.\\d+)?\", answer_text)\n",
    "    return nums[-1].strip() if nums else None\n",
    "\n",
    "def parse_pred(text: str) -> Optional[str]:\n",
    "    m = re.search(r\"\\[\\s*(-?\\d+(?:\\.\\d+)?)\\s*\\]\", text)\n",
    "    if m: return m.group(1).strip()\n",
    "    nums = re.findall(r\"-?\\d+(?:\\.\\d+)?\", text)\n",
    "    return nums[-1].strip() if nums else None\n",
    "\n",
    "print(\"Loading GSM8Kâ€¦\")\n",
    "ds_train_full = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n",
    "ds_test       = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
    "\n",
    "if cfg.val_rows is None:\n",
    "    val_rows = min(200, len(ds_train_full))\n",
    "else:\n",
    "    val_rows = min(cfg.val_rows, len(ds_train_full))\n",
    "\n",
    "ds_val   = ds_train_full.select(range(val_rows))\n",
    "ds_train = ds_train_full.select(range(val_rows, len(ds_train_full)))\n",
    "\n",
    "print(f\"Splits: {len(ds_train)} train | {len(ds_val)} val | {len(ds_test)} test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Load Models\n",
    "\n",
    "Key difference: we now load **multiple teacher models** and store them in a list with their tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Tokenizers & Models\n",
    "# --------------------------\n",
    "def load_tokenizer(model_id: str):\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "    if tok.pad_token is None and tok.eos_token is not None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    tok.padding_side = \"left\"  # decoder-only: left padding\n",
    "    return tok\n",
    "\n",
    "def make_lora_student(model_id: str) -> torch.nn.Module:\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    "    )\n",
    "    base.config.use_cache = False  # off for training\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    )\n",
    "    return get_peft_model(base, lora_cfg)\n",
    "\n",
    "def load_teacher(model_id: str) -> torch.nn.Module:\n",
    "    m = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    "    ).eval()\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad_(False)\n",
    "    return m\n",
    "\n",
    "# Load student\n",
    "print(f\"Loading student: {cfg.student_id}\")\n",
    "tok_s = load_tokenizer(cfg.student_id)\n",
    "\n",
    "# Load all teachers\n",
    "print(f\"\\nLoading {len(cfg.teachers)} teachers...\")\n",
    "teachers = []\n",
    "teacher_tokenizers = []\n",
    "for i, t_cfg in enumerate(cfg.teachers, 1):\n",
    "    print(f\"  [{i}/{len(cfg.teachers)}] {t_cfg.model_id}\")\n",
    "    teacher_model = load_teacher(t_cfg.model_id)\n",
    "    teacher_tok = load_tokenizer(t_cfg.model_id)\n",
    "    teachers.append(teacher_model)\n",
    "    teacher_tokenizers.append(teacher_tok)\n",
    "    print(f\"       Loaded. Weight: {t_cfg.weight:.3f}\")\n",
    "\n",
    "print(f\"\\nAll models loaded. Student padding: {tok_s.padding_side}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Baseline Evaluation\n",
    "\n",
    "Let's evaluate all models (student + all teachers) before training to establish baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Evaluation utils\n",
    "# --------------------------\n",
    "def _encode_cuda(tokenizer, texts: List[str], max_length=2048) -> Dict[str, torch.Tensor]:\n",
    "    enc = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    return {k: v.to(\"cuda\") for k, v in enc.items()}\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, tokenizer, dataset, *, num_examples: Optional[int] = None,\n",
    "             temperature: float = 0.0, max_new_tokens: int = 256,\n",
    "             batch_size: int = 32, desc: str = \"Eval\") -> float:\n",
    "    \"\"\"Exact-match accuracy against bracketed numeric answer.\"\"\"\n",
    "    n = len(dataset) if num_examples is None else min(num_examples, len(dataset))\n",
    "    rows = dataset.select(range(n))\n",
    "    correct = 0\n",
    "\n",
    "    was_cache = getattr(model.config, \"use_cache\", True)\n",
    "    model.eval(); model.config.use_cache = True\n",
    "\n",
    "    for i in tqdm(range(0, n, batch_size), desc=desc):\n",
    "        batch = rows.select(range(i, min(i + batch_size, n)))\n",
    "        prompts = [render_prompt(ex[\"question\"]) for ex in batch]\n",
    "        enc = _encode_cuda(tokenizer, prompts)\n",
    "\n",
    "        gen_kwargs = dict(\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True\n",
    "        )\n",
    "        if temperature and temperature > 0.0:\n",
    "            gen_kwargs.update(do_sample=True, temperature=temperature, top_p=0.9)\n",
    "        else:\n",
    "            gen_kwargs.update(do_sample=False)\n",
    "\n",
    "        outs = model.generate(**enc, **gen_kwargs)\n",
    "        txts = tokenizer.batch_decode(outs, skip_special_tokens=True)\n",
    "\n",
    "        for ex, text in zip(batch, txts):\n",
    "            pred = parse_pred(text) or \"\"\n",
    "            gold = parse_gold(ex[\"answer\"]) or \"\"\n",
    "            correct += int(pred == gold)\n",
    "\n",
    "    model.config.use_cache = was_cache\n",
    "    return correct / max(n, 1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def preview(model, tokenizer, dataset, k=2, temperature=0.0, max_new_tokens=256):\n",
    "    model.eval(); model.config.use_cache = True\n",
    "    rows = dataset.select(range(min(k, len(dataset))))\n",
    "    for ex in rows:\n",
    "        prompt = render_prompt(ex[\"question\"])\n",
    "        enc = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "        gen_kwargs = dict(max_new_tokens=max_new_tokens, use_cache=True,\n",
    "                          pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "        if temperature and temperature > 0.0:\n",
    "            gen_kwargs.update(do_sample=True, temperature=temperature, top_p=0.9)\n",
    "        out = model.generate(**enc, **gen_kwargs)\n",
    "        text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        print(\"=\"*80); print(prompt); print(\"-\"*80); print(text)\n",
    "        print(\"-\"*80, f\"\\nParsed: [{parse_pred(text)}] | Gold: [{parse_gold(ex['answer'])}]\")\n",
    "\n",
    "# --------------------------\n",
    "# Baselines (before training)\n",
    "# --------------------------\n",
    "print(\"\\n== Computing Baselines ==\")\n",
    "student_for_eval = make_lora_student(cfg.student_id)\n",
    "\n",
    "print(\"\\nPreview (student, greedy):\")\n",
    "preview(student_for_eval, tok_s, ds_test, k=1, temperature=0.0, max_new_tokens=cfg.max_new_tokens)\n",
    "\n",
    "print(\"\\nEvaluating student...\")\n",
    "baseline_student_em = evaluate(student_for_eval, tok_s, ds_test,\n",
    "                               temperature=cfg.eval_temperature, max_new_tokens=cfg.max_new_tokens,\n",
    "                               batch_size=128, desc=\"Student baseline (test)\")\n",
    "print(f\"Student (0.6B base) EM: {baseline_student_em:.4f}\")\n",
    "\n",
    "# Evaluate all teachers\n",
    "teacher_baselines = []\n",
    "for i, (teacher, tok_t, t_cfg) in enumerate(zip(teachers, teacher_tokenizers, cfg.teachers), 1):\n",
    "    print(f\"\\nEvaluating teacher {i}/{len(teachers)}: {t_cfg.model_id}\")\n",
    "    em = evaluate(teacher, tok_t, ds_test,\n",
    "                  temperature=cfg.eval_temperature, max_new_tokens=cfg.max_new_tokens,\n",
    "                  batch_size=64, desc=f\"Teacher {i} baseline\")\n",
    "    teacher_baselines.append(em)\n",
    "    print(f\"Teacher {i} EM: {em:.4f}\")\n",
    "\n",
    "print(\"\\n=== Baseline Summary ===\")\n",
    "print(f\"Student: {baseline_student_em:.4f}\")\n",
    "for i, (em, t_cfg) in enumerate(zip(teacher_baselines, cfg.teachers), 1):\n",
    "    print(f\"Teacher {i} ({t_cfg.model_id.split('/')[-1]}): {em:.4f} (weight: {t_cfg.weight:.3f})\")\n",
    "\n",
    "baselines_dict = {\n",
    "    \"student\": baseline_student_em,\n",
    "    **{f\"teacher_{i}\": em for i, em in enumerate(teacher_baselines, 1)}\n",
    "}\n",
    "with open(os.path.join(cfg.run_root, \"baselines.json\"), \"w\") as f:\n",
    "    json.dump(baselines_dict, f, indent=2)\n",
    "\n",
    "# Free the temporary student\n",
    "del student_for_eval; torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Multi-Teacher Training Utilities\n",
    "\n",
    "The key innovation is in the `multi_teacher_logp` function, which:\n",
    "1. Computes log probabilities from each teacher\n",
    "2. Combines them using weighted averaging (or other aggregation strategies)\n",
    "3. Returns a single aggregated teacher signal for the student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Multi-Teacher OPD utilities\n",
    "# --------------------------\n",
    "def mask_before_first_eos(next_ids: torch.Tensor, eos_id: int) -> torch.Tensor:\n",
    "    \"\"\"Mask tokens strictly before the first EOS in each sequence.\"\"\"\n",
    "    is_eos = (next_ids == eos_id)\n",
    "    csum = is_eos.cumsum(dim=1)\n",
    "    return csum.eq(0)\n",
    "\n",
    "def student_logp_batched(student, pad_id, full_ids, next_ids, T, micro_bsz):\n",
    "    \"\"\"Return student log p(a_t) for the last T tokens (with grad).\"\"\"\n",
    "    outs = []\n",
    "    for s in range(0, full_ids.size(0), micro_bsz):\n",
    "        chunk = full_ids[s:s+micro_bsz]; nxt = next_ids[s:s+micro_bsz]\n",
    "        out = student(input_ids=chunk[:, :-1],\n",
    "                      attention_mask=(chunk[:, :-1] != pad_id))\n",
    "        logits = out.logits[:, -T:, :]\n",
    "        logp = F.log_softmax(logits, dim=-1).gather(-1, nxt.unsqueeze(-1)).squeeze(-1)\n",
    "        outs.append(logp)\n",
    "        del out, logits\n",
    "    return torch.cat(outs, dim=0)\n",
    "\n",
    "# ---------- Cross-tokenizer teacher scoring ----------\n",
    "def _decode_token_str(tokenizer, token_id: int) -> str:\n",
    "    return tokenizer.decode([int(token_id)],\n",
    "                            skip_special_tokens=False,\n",
    "                            clean_up_tokenization_spaces=False)\n",
    "\n",
    "def _encode_text_ids(tokenizer, text: str):\n",
    "    return tokenizer(text,\n",
    "                     add_special_tokens=False,\n",
    "                     return_tensors=\"pt\").input_ids[0].tolist()\n",
    "\n",
    "@torch.no_grad()\n",
    "def teacher_logp_grouped_by_student_tokens(\n",
    "    teacher, tok_teacher, tok_student, prompts: List[str], next_ids: torch.Tensor, max_len: Optional[int] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    For each sample b and student step t:\n",
    "      1) Decode the student's token id next_ids[b,t] â†’ text piece\n",
    "      2) Tokenize that text with the teacher tokenizer (may become multiple tokens)\n",
    "      3) Sum teacher log-probs over that group\n",
    "    Returns: Tensor [B, T] on CUDA with per-student-step teacher log-probs.\n",
    "    \"\"\"\n",
    "    device = teacher.device if hasattr(teacher, \"device\") else \"cuda\"\n",
    "    B, T = next_ids.shape\n",
    "    out = torch.zeros((B, T), device=device, dtype=torch.float32)\n",
    "\n",
    "    if max_len is None:\n",
    "        max_len = int(getattr(teacher.config, \"max_position_embeddings\", 2048))\n",
    "\n",
    "    for b in range(B):\n",
    "        prompt_text = prompts[b]\n",
    "        ctx_ids = _encode_text_ids(tok_teacher, prompt_text)\n",
    "\n",
    "        groups = []\n",
    "        for t in range(T):\n",
    "            s_tok_id = int(next_ids[b, t].item())\n",
    "            piece = _decode_token_str(tok_student, s_tok_id)\n",
    "            ids_t = _encode_text_ids(tok_teacher, piece)\n",
    "            groups.append(ids_t)\n",
    "\n",
    "        flat_gen = [tid for g in groups for tid in g]\n",
    "        if len(flat_gen) == 0:\n",
    "            continue\n",
    "\n",
    "        # Respect the teacher's context length\n",
    "        total = len(ctx_ids) + len(flat_gen)\n",
    "        if total > max_len:\n",
    "            overflow = total - max_len\n",
    "            ctx_ids = ctx_ids[overflow:]\n",
    "\n",
    "        full = ctx_ids + flat_gen\n",
    "        if len(full) < 2:\n",
    "            continue\n",
    "\n",
    "        input_ids = torch.tensor(full[:-1], device=device).unsqueeze(0)\n",
    "        labels    = torch.tensor(full[1:],  device=device).unsqueeze(0)\n",
    "        attn_mask = torch.ones_like(input_ids, device=device)\n",
    "\n",
    "        outputs = teacher(input_ids=input_ids, attention_mask=attn_mask)\n",
    "        logprobs = F.log_softmax(outputs.logits, dim=-1)\n",
    "        tok_lp = logprobs.gather(-1, labels.unsqueeze(-1)).squeeze(-1)[0]\n",
    "\n",
    "        start = max(len(ctx_ids) - 1, 0)\n",
    "        gen_lp = tok_lp[start : start + len(flat_gen)]\n",
    "\n",
    "        # Sum back per student step\n",
    "        off = 0\n",
    "        for t, g in enumerate(groups):\n",
    "            k = len(g)\n",
    "            if k > 0:\n",
    "                out[b, t] = gen_lp[off : off + k].sum()\n",
    "            off += k\n",
    "\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def multi_teacher_logp(\n",
    "    teachers: List[torch.nn.Module],\n",
    "    teacher_toks: List[AutoTokenizer],\n",
    "    teacher_weights: List[float],\n",
    "    tok_student: AutoTokenizer,\n",
    "    prompts: List[str],\n",
    "    next_ids: torch.Tensor,\n",
    "    aggregation: str = \"weighted_avg\"  # or \"max\", \"mixture\"\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute aggregated teacher log-probs from multiple teachers.\n",
    "    \n",
    "    Args:\n",
    "        teachers: List of teacher models\n",
    "        teacher_toks: List of teacher tokenizers\n",
    "        teacher_weights: Normalized weights for each teacher\n",
    "        tok_student: Student tokenizer\n",
    "        prompts: List of prompt strings\n",
    "        next_ids: Student-generated token IDs [B, T]\n",
    "        aggregation: How to combine teacher signals\n",
    "            - \"weighted_avg\": Weighted average of log-probs (default)\n",
    "            - \"max\": Take maximum log-prob at each position\n",
    "            - \"mixture\": Log of weighted probability mixture\n",
    "    \n",
    "    Returns:\n",
    "        Aggregated log-probs [B, T]\n",
    "    \"\"\"\n",
    "    B, T = next_ids.shape\n",
    "    device = next_ids.device\n",
    "    \n",
    "    # Collect log-probs from all teachers\n",
    "    all_logps = []\n",
    "    for teacher, tok_t in zip(teachers, teacher_toks):\n",
    "        logp_t = teacher_logp_grouped_by_student_tokens(\n",
    "            teacher=teacher,\n",
    "            tok_teacher=tok_t,\n",
    "            tok_student=tok_student,\n",
    "            prompts=prompts,\n",
    "            next_ids=next_ids,\n",
    "            max_len=getattr(teacher.config, \"max_position_embeddings\", 2048)\n",
    "        )\n",
    "        all_logps.append(logp_t)\n",
    "    \n",
    "    # Stack: [num_teachers, B, T]\n",
    "    stacked = torch.stack(all_logps, dim=0)\n",
    "    \n",
    "    # Aggregate\n",
    "    if aggregation == \"weighted_avg\":\n",
    "        # Weighted average in log space (not exact, but simple)\n",
    "        weights = torch.tensor(teacher_weights, device=device).view(-1, 1, 1)\n",
    "        result = (stacked * weights).sum(dim=0)\n",
    "    elif aggregation == \"max\":\n",
    "        # Take the maximum log-prob at each position\n",
    "        result = stacked.max(dim=0)[0]\n",
    "    elif aggregation == \"mixture\":\n",
    "        # Proper mixture: log(sum_i w_i * exp(logp_i))\n",
    "        weights = torch.tensor(teacher_weights, device=device).view(-1, 1, 1)\n",
    "        # Use logsumexp for numerical stability\n",
    "        # log(sum w_i exp(logp_i)) = log(sum exp(log(w_i) + logp_i))\n",
    "        log_weights = torch.log(weights)\n",
    "        result = torch.logsumexp(log_weights + stacked, dim=0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown aggregation: {aggregation}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "class LiveTable:\n",
    "    def __init__(self, title: str = \"Training metrics\", max_rows: int = 200):\n",
    "        self.title = title\n",
    "        self.max_rows = max_rows\n",
    "        self.rows = []\n",
    "        empty = pd.DataFrame(columns=[\"step\",\"loss\",\"loss_ema\",\"revkl\",\"revkl_ema\",\"tokens\",\"val_em\"])\n",
    "        self.handle = display(self._styled(empty), display_id=True)\n",
    "\n",
    "    def _styled(self, df: pd.DataFrame):\n",
    "        styler = (\n",
    "            df.style\n",
    "              .set_caption(self.title)\n",
    "              .format({\n",
    "                  \"loss\": \"{:.4f}\",\n",
    "                  \"loss_ema\": \"{:.4f}\",\n",
    "                  \"revkl\": \"{:.4f}\",\n",
    "                  \"revkl_ema\": \"{:.4f}\",\n",
    "                  \"val_em\": (lambda v: \"\" if pd.isna(v) else f\"{v:.3f}\"),\n",
    "              })\n",
    "        )\n",
    "        try:\n",
    "            styler = styler.hide(axis=\"index\")\n",
    "            return styler\n",
    "        except Exception:\n",
    "            pass\n",
    "        return styler.set_table_styles([\n",
    "            {\"selector\": \"th.row_heading\", \"props\": [(\"display\", \"none\")]},\n",
    "            {\"selector\": \"th.blank\",       \"props\": [(\"display\", \"none\")]},\n",
    "        ])\n",
    "\n",
    "    def update(self, *, step, loss, loss_ema, revkl, revkl_ema, tokens, val_em=None):\n",
    "        self.rows.append(dict(\n",
    "            step=int(step),\n",
    "            loss=float(loss),\n",
    "            loss_ema=(None if loss_ema is None else float(loss_ema)),\n",
    "            revkl=float(revkl),\n",
    "            revkl_ema=(None if revkl_ema is None else float(revkl_ema)),\n",
    "            tokens=int(tokens),\n",
    "            val_em=(None if val_em is None else float(val_em)),\n",
    "        ))\n",
    "        rows = self.rows[-self.max_rows:]\n",
    "        df = pd.DataFrame(rows, columns=[\"step\",\"loss\",\"loss_ema\",\"revkl\",\"revkl_ema\",\"tokens\",\"val_em\"])\n",
    "        self.handle.update(self._styled(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Training Metrics Explanation\n",
    "\n",
    "| Column        | Meaning                                                                                                                                                                                                         | How to interpret                                                                      |\n",
    "| :------------ | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------ |\n",
    "| **step**      | The current training iteration (out of the total configured `cfg.steps`). Each step processes one batch of sampled prompts.                                                                                     | Training progresses along this axis.                                                  |\n",
    "| **loss**      | The instantaneous batch loss (reverse-KLâ€“style policy-gradient objective). Negative values are expected because we optimize `-E[(log p_T âˆ’ log p_S) Â· log p_S]`; good updates drive this lower (more negative). | Lower (more negative) â†’ better alignment with teachers on that batch.                 |\n",
    "| **loss_ema**  | Exponential moving average (EMA) of the loss across steps. Smooths noise to show trend.                                                                                                                         | Downward trend = overall improvement; steadiness = convergence.                       |\n",
    "| **revkl**     | The *reverse-KL divergence estimate* for this batch: roughly E[ log p_S âˆ’ log p_T ]. It measures how far the student's distribution is from the **aggregated teachers'** distribution.                          | Smaller (approaching 0) â†’ student policy is closer to teacher ensemble.               |\n",
    "| **revkl_ema** | EMA-smoothed version of reverse-KL.                                                                                                                                                                             | Should decrease steadily if multi-teacher distillation is working.                    |\n",
    "| **tokens**    | Cumulative count of *valid graded tokens* seen so far â€” all pre-EOS generated tokens used for loss computation.                                                                                                 | Measures total learning signal processed; roughly proportional to compute/throughput. |\n",
    "| **val_em**    | Validation exact-match accuracy (fraction of GSM8K val examples where the parsed numeric answer matches gold). Evaluated every `cfg.val_every` steps.                                                           | Direct measure of task performance; higher = better reasoning accuracy.               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Multi-Teacher Training Loop\n",
    "# --------------------------\n",
    "def ema(prev, new, beta):\n",
    "    return new if prev is None else (beta * prev + (1 - beta) * new)\n",
    "\n",
    "def run_multi_teacher_training(run_dir: str, aggregation: str = \"weighted_avg\"):\n",
    "    \"\"\"\n",
    "    Main multi-teacher on-policy distillation training loop.\n",
    "    \n",
    "    Args:\n",
    "        run_dir: Directory to save outputs\n",
    "        aggregation: How to combine teacher signals (\"weighted_avg\", \"max\", \"mixture\")\n",
    "    \"\"\"\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    # Fresh student (LoRA adapters)\n",
    "    student = make_lora_student(cfg.student_id)\n",
    "    optimizer = torch.optim.AdamW(student.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    prompts_all = [render_prompt(x[\"question\"]) for x in ds_train]\n",
    "    ema_loss = None\n",
    "    ema_revkl = None\n",
    "    tokens_graded_cum = 0\n",
    "    logs = []\n",
    "\n",
    "    table = LiveTable(title=f\"Multi-Teacher OPD ({len(cfg.teachers)} teachers, {aggregation})\")\n",
    "\n",
    "    # Extract normalized weights\n",
    "    teacher_weights = [t.weight for t in cfg.teachers]\n",
    "\n",
    "    pbar = tqdm(range(cfg.steps), desc=f\"Multi-Teacher OPD [{os.path.basename(run_dir)}]\")\n",
    "    for step in pbar:\n",
    "        # Deterministic batch selection per step\n",
    "        rng = np.random.default_rng(SEED + step)\n",
    "        idxs = rng.choice(len(prompts_all), size=cfg.batch_prompts, replace=False)\n",
    "        prompts = [prompts_all[i] for i in idxs]\n",
    "        prompts_rep = sum(([p] * cfg.samples_per_prompt for p in prompts), [])\n",
    "        enc = tok_s(prompts_rep, padding=True, truncation=True, max_length=2048, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        # 1) Student rollouts (no grad) to get sequences and step count\n",
    "        with torch.no_grad():\n",
    "            gen_out = student.generate(\n",
    "                **enc,\n",
    "                do_sample=True, temperature=cfg.train_temperature, top_p=0.9,\n",
    "                max_new_tokens=cfg.max_new_tokens,\n",
    "                eos_token_id=tok_s.eos_token_id, pad_token_id=tok_s.pad_token_id,\n",
    "                return_dict_in_generate=True, output_scores=True\n",
    "            )\n",
    "            seqs = gen_out.sequences\n",
    "            scores_list = list(gen_out.scores)\n",
    "            T = len(scores_list)\n",
    "            next_ids = seqs[:, -T:]\n",
    "            valid_mask = mask_before_first_eos(next_ids, eos_id=tok_s.eos_token_id).float()\n",
    "\n",
    "        # 2) Student log-probs with grad\n",
    "        student.train(); student.config.use_cache = False\n",
    "        logp_s = student_logp_batched(\n",
    "            student, tok_s.pad_token_id, seqs, next_ids, T, cfg.student_mb\n",
    "        )\n",
    "\n",
    "        # 3) MULTI-TEACHER aggregated log-probs (no grad)\n",
    "        logp_t = multi_teacher_logp(\n",
    "            teachers=teachers,\n",
    "            teacher_toks=teacher_tokenizers,\n",
    "            teacher_weights=teacher_weights,\n",
    "            tok_student=tok_s,\n",
    "            prompts=prompts_rep,\n",
    "            next_ids=next_ids,\n",
    "            aggregation=aggregation\n",
    "        )\n",
    "\n",
    "        # 4) Reverse-KL-style policy gradient\n",
    "        adv = (logp_t - logp_s).clamp(-5, 5)\n",
    "        denom = valid_mask.sum().clamp_min(1.0)\n",
    "        loss = - ((valid_mask * adv.detach()) * logp_s).sum() / denom\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rev_kl = ((logp_s - logp_t) * valid_mask).sum().item() / float(denom)\n",
    "            tokens_graded_cum += int(denom.item())\n",
    "            ema_loss = ema(ema_loss, float(loss.item()), cfg.ema_momentum)\n",
    "            ema_revkl = ema(ema_revkl, float(rev_kl), cfg.ema_momentum)\n",
    "\n",
    "        # Periodic validation EM (greedy)\n",
    "        val_em = None\n",
    "        if (step % cfg.val_every == 0) or (step == cfg.steps - 1):\n",
    "            student.eval(); student.config.use_cache = True\n",
    "            val_em = evaluate(student, tok_s, ds_val,\n",
    "                              num_examples=min(cfg.val_sample_n, len(ds_val)),\n",
    "                              temperature=0.0, max_new_tokens=cfg.max_new_tokens,\n",
    "                              batch_size=32, desc=\"VAL EM\")\n",
    "            student.train(); student.config.use_cache = False\n",
    "\n",
    "        row = dict(\n",
    "            step=int(step),\n",
    "            train_loss=float(loss.item()),\n",
    "            train_loss_ema=float(ema_loss) if ema_loss is not None else None,\n",
    "            train_revkl=float(rev_kl),\n",
    "            train_revkl_ema=float(ema_revkl) if ema_revkl is not None else None,\n",
    "            tokens_graded=int(tokens_graded_cum),\n",
    "            **({\"val_em\": float(val_em)} if val_em is not None else {})\n",
    "        )\n",
    "        logs.append(row)\n",
    "\n",
    "        if (step % cfg.log_every == 0) or (val_em is not None):\n",
    "            table.update(\n",
    "                step=row[\"step\"],\n",
    "                loss=row[\"train_loss\"],\n",
    "                loss_ema=row[\"train_loss_ema\"],\n",
    "                revkl=row[\"train_revkl\"],\n",
    "                revkl_ema=row[\"train_revkl_ema\"],\n",
    "                tokens=row[\"tokens_graded\"],\n",
    "                val_em=row.get(\"val_em\", None),\n",
    "            )\n",
    "\n",
    "        postfix = {\n",
    "            \"loss\": f\"{loss.item():.3f}\",\n",
    "            \"ema\": f\"{(ema_loss if ema_loss is not None else loss.item()):.3f}\",\n",
    "            \"rkl\": f\"{rev_kl:.3f}\",\n",
    "            \"toks\": tokens_graded_cum\n",
    "        }\n",
    "        if val_em is not None:\n",
    "            postfix[\"val\"] = f\"{val_em:.3f}\"\n",
    "        pbar.set_postfix(**postfix)\n",
    "\n",
    "        del scores_list\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Save logs\n",
    "    try:\n",
    "        pd.DataFrame(logs).to_csv(os.path.join(run_dir, \"train_logs.csv\"), index=False)\n",
    "    except Exception:\n",
    "        with open(os.path.join(run_dir, \"train_logs.jsonl\"), \"w\") as f:\n",
    "            for r in logs:\n",
    "                f.write(json.dumps(r) + \"\\n\")\n",
    "\n",
    "    # Final test EM (greedy)\n",
    "    student.eval(); student.config.use_cache = True\n",
    "    test_em = evaluate(student, tok_s, ds_test,\n",
    "                       temperature=0.0, max_new_tokens=cfg.max_new_tokens,\n",
    "                       batch_size=64, desc=f\"Test EM [{os.path.basename(run_dir)}]\")\n",
    "\n",
    "    # Save adapters and summary\n",
    "    save_dir = os.path.join(run_dir, \"adapters_lora\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    student.save_pretrained(save_dir)\n",
    "\n",
    "    summary = dict(\n",
    "        num_teachers=len(cfg.teachers),\n",
    "        aggregation=aggregation,\n",
    "        teacher_weights=[t.weight for t in cfg.teachers],\n",
    "        steps=cfg.steps,\n",
    "        batch_prompts=cfg.batch_prompts,\n",
    "        samples_per_prompt=cfg.samples_per_prompt,\n",
    "        max_new_tokens=cfg.max_new_tokens,\n",
    "        train_tokens_graded=tokens_graded_cum,\n",
    "        test_em=float(test_em)\n",
    "    )\n",
    "    with open(os.path.join(run_dir, \"summary.json\"), \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    # Free GPU\n",
    "    del student; torch.cuda.empty_cache()\n",
    "    return summary, logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Run Multi-Teacher Training\n",
    "\n",
    "Now let's train the student with all teachers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Run training\n",
    "# --------------------------\n",
    "print(\"\\n== Multi-Teacher Training ==\")\n",
    "print(f\"Aggregation strategy: weighted_avg\")\n",
    "print(f\"Number of teachers: {len(cfg.teachers)}\")\n",
    "\n",
    "run_dir = os.path.join(cfg.run_root, \"multi_teacher_opd\")\n",
    "summary, _ = run_multi_teacher_training(run_dir, aggregation=\"weighted_avg\")\n",
    "\n",
    "print(\"\\n== Final Results ==\")\n",
    "final_report = dict(\n",
    "    env=dict(\n",
    "        python=sys.version,\n",
    "        torch=torch.__version__,\n",
    "        cuda=torch.version.cuda if torch.cuda.is_available() else \"cpu\",\n",
    "        device=torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\",\n",
    "    ),\n",
    "    config=dict(\n",
    "        student=cfg.student_id,\n",
    "        teachers=[t.model_id for t in cfg.teachers],\n",
    "        teacher_weights=[t.weight for t in cfg.teachers],\n",
    "        prompt_template=cfg.prompt_template,\n",
    "    ),\n",
    "    baselines=baselines_dict,\n",
    "    final=summary,\n",
    "    improvement=float(summary[\"test_em\"] - baseline_student_em)\n",
    ")\n",
    "\n",
    "print(json.dumps(final_report, indent=2))\n",
    "\n",
    "print(f\"\\n=== Summary ===\")\n",
    "print(f\"Student baseline: {baseline_student_em:.4f}\")\n",
    "print(f\"After multi-teacher OPD: {summary['test_em']:.4f}\")\n",
    "print(f\"Improvement: {final_report['improvement']:.4f}\")\n",
    "print(f\"\\nArtifacts saved to: {cfg.run_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Comparison: Different Aggregation Strategies (Optional)\n",
    "\n",
    "You can experiment with different ways to combine teacher signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Compare aggregation strategies\n",
    "# Uncomment to run multiple experiments\n",
    "\n",
    "# aggregation_strategies = [\"weighted_avg\", \"max\", \"mixture\"]\n",
    "# results = {}\n",
    "\n",
    "# for agg in aggregation_strategies:\n",
    "#     print(f\"\\n{'='*80}\")\n",
    "#     print(f\"Training with aggregation: {agg}\")\n",
    "#     print(f\"{'='*80}\")\n",
    "#     \n",
    "#     run_dir = os.path.join(cfg.run_root, f\"multi_teacher_{agg}\")\n",
    "#     summary, _ = run_multi_teacher_training(run_dir, aggregation=agg)\n",
    "#     results[agg] = summary\n",
    "#     \n",
    "#     print(f\"\\nTest EM ({agg}): {summary['test_em']:.4f}\")\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "# # Compare results\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"COMPARISON: Aggregation Strategies\")\n",
    "# print(\"=\"*80)\n",
    "# print(f\"Student baseline: {baseline_student_em:.4f}\")\n",
    "# for agg, summary in results.items():\n",
    "#     improvement = summary['test_em'] - baseline_student_em\n",
    "#     print(f\"{agg:15s}: {summary['test_em']:.4f} (+{improvement:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Multi-Teacher vs Single-Teacher OPD\n",
    "\n",
    "**Advantages:**\n",
    "1. **Ensemble benefits**: Multiple teachers provide more robust guidance\n",
    "2. **Diverse knowledge**: Different teachers may excel at different aspects\n",
    "3. **Flexibility**: Can weight teachers by performance or other criteria\n",
    "\n",
    "**Considerations:**\n",
    "1. **Compute cost**: Must score student rollouts with multiple teachers (linear in number of teachers)\n",
    "2. **Memory**: All teachers must fit in GPU memory (though can be on separate GPUs)\n",
    "3. **Aggregation strategy**: Choice of how to combine teachers affects results\n",
    "\n",
    "### Aggregation Strategies\n",
    "\n",
    "1. **Weighted Average** (default): Simple, interpretable, works well in practice\n",
    "2. **Max**: Takes best teacher signal at each step, but ignores weights\n",
    "3. **Mixture**: Proper probabilistic mixture, theoretically sound but more complex\n",
    "\n",
    "### Extension Ideas\n",
    "\n",
    "1. **Dynamic weighting**: Adjust teacher weights during training based on agreement\n",
    "2. **Specialized teachers**: Use different teachers for different data distributions\n",
    "3. **Curriculum**: Start with easier teachers, gradually add harder ones\n",
    "4. **Teacher selection**: Per-batch teacher selection based on uncertainty\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "For the production implementation shown in the tinker-cookbook:\n",
    "- Uses much larger models (8B student, 32B and 235B teachers)\n",
    "- Separate datasets for each teacher (DeepMath, Tulu3)\n",
    "- More sophisticated infrastructure (distributed training, checkpointing)\n",
    "- Consider cost vs. benefit: Does the improvement justify 2-3x compute?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
