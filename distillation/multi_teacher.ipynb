{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Multi-Teacher On-Policy Distillation with Dataset Specialization\n",
    "\n",
    "This notebook demonstrates **multi-teacher on-policy distillation (OPD)** with **dataset-teacher pairing**, where each teacher model specializes on a specific dataset/domain.\n",
    "\n",
    "## Key Concept: Dataset-Teacher Pairing\n",
    "\n",
    "Unlike naive multi-teacher approaches that aggregate multiple teachers on the same data, the production approach pairs each teacher with its specialty:\n",
    "\n",
    "- **Teacher 1 (Math Expert)**: Qwen3-4B-Instruct-2507 â†’ GSM8K (math problems)\n",
    "- **Teacher 2 (Code Expert)**: Qwen3-3B-Instruct â†’ MBPP (code problems)\n",
    "\n",
    "During training, we:\n",
    "1. **Sample** a (dataset, teacher) pair\n",
    "2. **Get prompts** from that dataset\n",
    "3. **Student generates** solutions\n",
    "4. **Only that teacher** (the specialist) evaluates the rollouts\n",
    "5. **Student learns** from specialized feedback\n",
    "\n",
    "## Why This Approach?\n",
    "\n",
    "âœ… **Specialization**: Each teacher evaluates only its domain of expertise\n",
    "\n",
    "âœ… **Diversity**: Student learns multiple skills (math, code, etc.)\n",
    "\n",
    "âœ… **Efficiency**: Don't waste compute running all teachers on all data\n",
    "\n",
    "âœ… **Production-ready**: Matches tinker-cookbook implementation pattern\n",
    "\n",
    "## Requirements\n",
    "\n",
    "**GPU Auto-Detection**: This notebook automatically detects your GPU and adjusts precision/batch sizes.\n",
    "\n",
    "Recommended: **A100 (40GB+)**, **H100**, **L40S**, or similar. Also works on V100, T4 with reduced settings.\n",
    "\n",
    "## References\n",
    "\n",
    "- Blog: [Thinking Machines - On-Policy Distillation](https://thinkingmachines.ai/blog/on-policy-distillation/)\n",
    "- Paper: [Agarwal et al. 2024](https://arxiv.org/abs/2306.13649)\n",
    "- Code: [tinker-cookbook multi-teacher](https://github.com/thinking-machines-lab/tinker-cookbook/blob/main/tinker_cookbook/recipes/distillation/on_policy_multi_teacher.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ðŸ› ï¸ Setup\n",
    "!nvidia-smi -L || true\n",
    "\n",
    "import os, sys, random, numpy as np, torch, json, time, platform, math\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Qwen3 requires Transformers >= 4.51\n",
    "try:\n",
    "    get_ipython().run_line_magic(\"uv\", \"pip -q install transformers==4.51.3 accelerate==1.4.0 peft==0.14.0 datasets==3.3.2 evaluate==0.4.3 sentencepiece protobuf tqdm matplotlib > /dev/null\")\n",
    "except Exception:\n",
    "    get_ipython().run_line_magic(\"pip\", \"-q install transformers==4.51.3 accelerate==1.4.0 peft==0.14.0 datasets==3.3.2 evaluate==0.4.3 sentencepiece protobuf tqdm matplotlib > /dev/null\")\n",
    "\n",
    "import transformers, datasets, peft, accelerate, matplotlib\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Accelerate:\", accelerate.__version__)\n",
    "print(\"PEFT:\", peft.__version__)\n",
    "print(\"matplotlib:\", matplotlib.__version__)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "assert DEVICE == \"cuda\", \"Please connect a GPU (A100+ recommended).\"\n",
    "\n",
    "def print_header():\n",
    "    print(\"== Environment ==\")\n",
    "    print(dict(\n",
    "        python=sys.version,\n",
    "        torch=torch.__version__,\n",
    "        transformers=transformers.__version__,\n",
    "        accelerate=accelerate.__version__,\n",
    "        peft=peft.__version__,\n",
    "        cuda=torch.version.cuda if torch.cuda.is_available() else \"cpu\",\n",
    "        device_name=torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\",\n",
    "        platform=platform.platform(),\n",
    "        seed=SEED\n",
    "    ))\n",
    "print_header()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, json, random, platform\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Dict, Tuple, Callable, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import re\n",
    "\n",
    "# --------------------------\n",
    "# Reproducibility & device\n",
    "# --------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "assert DEVICE == \"cuda\", \"A CUDA GPU is required.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-gpu-detection-header",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ GPU Auto-Detection & Configuration\n",
    "\n",
    "Automatically detects your GPU and selects optimal settings:\n",
    "- **Precision**: fp8 (H100), bf16 (A100, L40S), fp16 (V100, T4)\n",
    "- **Batch sizes**: Based on VRAM (80GB/40GB/24GB/16GB)\n",
    "- **LoRA rank**: Adjusted for GPU capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-gpu-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# GPU Detection & Auto-Configuration\n",
    "# --------------------------\n",
    "@dataclass\n",
    "class GPUConfig:\n",
    "    \"\"\"Auto-detected GPU configuration.\"\"\"\n",
    "    name: str\n",
    "    vram_gb: float\n",
    "    compute_capability: Tuple[int, int]\n",
    "    \n",
    "    # Auto-selected settings\n",
    "    dtype: torch.dtype\n",
    "    dtype_str: str\n",
    "    batch_prompts: int\n",
    "    samples_per_prompt: int\n",
    "    student_mb: int\n",
    "    lora_rank: int\n",
    "    use_flash_attention: bool = False\n",
    "    \n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"GPU: {self.name}\\n\"\n",
    "            f\"VRAM: {self.vram_gb:.1f} GB\\n\"\n",
    "            f\"Compute: {self.compute_capability[0]}.{self.compute_capability[1]}\\n\"\n",
    "            f\"Precision: {self.dtype_str}\\n\"\n",
    "            f\"Batch: {self.batch_prompts} prompts Ã— {self.samples_per_prompt} samples\\n\"\n",
    "            f\"LoRA rank: {self.lora_rank}\\n\"\n",
    "            f\"Flash Attention: {self.use_flash_attention}\"\n",
    "        )\n",
    "\n",
    "def detect_gpu() -> GPUConfig:\n",
    "    \"\"\"Detect GPU and automatically select optimal configuration.\"\"\"\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"No CUDA GPU detected!\")\n",
    "    \n",
    "    # Get GPU info\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_bytes = torch.cuda.get_device_properties(0).total_memory\n",
    "    vram_gb = vram_bytes / (1024 ** 3)\n",
    "    compute_cap = torch.cuda.get_device_capability(0)\n",
    "    \n",
    "    # Check for bf16 support (compute capability >= 8.0)\n",
    "    supports_bf16 = (compute_cap[0] >= 8) or (compute_cap[0] == 7 and compute_cap[1] >= 5)\n",
    "    \n",
    "    # Check for Hopper (H100, compute capability 9.0) - supports fp8\n",
    "    is_hopper = (compute_cap[0] >= 9)\n",
    "    \n",
    "    # Check for Ampere+ (A100, RTX 30/40 series) - supports bf16 natively\n",
    "    is_ampere_plus = (compute_cap[0] >= 8)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸ” GPU DETECTION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Name: {gpu_name}\")\n",
    "    print(f\"VRAM: {vram_gb:.1f} GB\")\n",
    "    print(f\"Compute Capability: {compute_cap[0]}.{compute_cap[1]}\")\n",
    "    print(f\"BF16 Support: {supports_bf16}\")\n",
    "    print(f\"Hopper (FP8): {is_hopper}\")\n",
    "    \n",
    "    # --------------------------\n",
    "    # Select precision (dtype)\n",
    "    # --------------------------\n",
    "    if is_hopper:\n",
    "        # H100: Use bf16 (fp8 requires additional setup)\n",
    "        # Note: fp8 would require transformers 4.43+ and special quantization\n",
    "        dtype = torch.bfloat16\n",
    "        dtype_str = \"bf16 (fp8 available but not auto-enabled)\"\n",
    "        print(f\"\\nâœ… Precision: bfloat16 (Hopper GPU)\")\n",
    "        print(f\"   Note: FP8 available but requires additional setup\")\n",
    "    elif supports_bf16:\n",
    "        # A100, RTX 30/40 series, L40S: Use bf16\n",
    "        dtype = torch.bfloat16\n",
    "        dtype_str = \"bf16\"\n",
    "        print(f\"\\nâœ… Precision: bfloat16 (Ampere+ GPU)\")\n",
    "    else:\n",
    "        # V100, T4, older GPUs: Use fp16\n",
    "        dtype = torch.float16\n",
    "        dtype_str = \"fp16\"\n",
    "        print(f\"\\nâš ï¸  Precision: float16 (older GPU, no native bf16)\")\n",
    "    \n",
    "    # --------------------------\n",
    "    # Auto-tune batch sizes based on VRAM\n",
    "    # --------------------------\n",
    "    if vram_gb >= 75:  # H100 80GB, A100 80GB\n",
    "        batch_prompts = 8\n",
    "        samples_per_prompt = 8\n",
    "        student_mb = 16\n",
    "        lora_rank = 32\n",
    "        tier = \"High (80GB+)\"\n",
    "    elif vram_gb >= 35:  # A100 40GB, L40S 48GB\n",
    "        batch_prompts = 4\n",
    "        samples_per_prompt = 4\n",
    "        student_mb = 8\n",
    "        lora_rank = 16\n",
    "        tier = \"Medium (40GB+)\"\n",
    "    elif vram_gb >= 20:  # RTX 4090 24GB, RTX A5000 24GB\n",
    "        batch_prompts = 2\n",
    "        samples_per_prompt = 4\n",
    "        student_mb = 4\n",
    "        lora_rank = 16\n",
    "        tier = \"Low (24GB)\"\n",
    "    else:  # T4 16GB, V100 16GB\n",
    "        batch_prompts = 2\n",
    "        samples_per_prompt = 2\n",
    "        student_mb = 2\n",
    "        lora_rank = 8\n",
    "        tier = \"Minimal (16GB)\"\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Memory Tier: {tier}\")\n",
    "    print(f\"   Batch: {batch_prompts} prompts Ã— {samples_per_prompt} samples\")\n",
    "    print(f\"   Student micro-batch: {student_mb}\")\n",
    "    print(f\"   LoRA rank: {lora_rank}\")\n",
    "    \n",
    "    # --------------------------\n",
    "    # Flash Attention (if available)\n",
    "    # --------------------------\n",
    "    use_flash_attention = is_ampere_plus\n",
    "    if use_flash_attention:\n",
    "        print(f\"\\nâš¡ Flash Attention: Enabled (Ampere+ GPU)\")\n",
    "    else:\n",
    "        print(f\"\\nâš¡ Flash Attention: Not available (requires Ampere+ GPU)\")\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return GPUConfig(\n",
    "        name=gpu_name,\n",
    "        vram_gb=vram_gb,\n",
    "        compute_capability=compute_cap,\n",
    "        dtype=dtype,\n",
    "        dtype_str=dtype_str,\n",
    "        batch_prompts=batch_prompts,\n",
    "        samples_per_prompt=samples_per_prompt,\n",
    "        student_mb=student_mb,\n",
    "        lora_rank=lora_rank,\n",
    "        use_flash_attention=use_flash_attention,\n",
    "    )\n",
    "\n",
    "# Detect GPU and get optimal configuration\n",
    "gpu_config = detect_gpu()\n",
    "\n",
    "print(\"\\n=== Selected Configuration ===\")\n",
    "print(gpu_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Configuration: Teacher-Dataset Pairs\n",
    "\n",
    "The core innovation: each `TeacherDatasetPair` links a teacher model to its specialized dataset.\n",
    "\n",
    "### Example Production Setup (tinker-cookbook)\n",
    "```python\n",
    "Teacher 1: Qwen3-32B + DeepMath dataset (math specialist)\n",
    "Teacher 2: Qwen3-235B + Tulu3 dataset (instruction specialist)\n",
    "```\n",
    "\n",
    "### Our Demo Setup\n",
    "```python\n",
    "Teacher 1: Qwen3-4B-Instruct + GSM8K (math problems)\n",
    "Teacher 2: Qwen3-3B-Instruct + MBPP (code problems)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Teacher-Dataset Pair Configuration\n",
    "# --------------------------\n",
    "@dataclass\n",
    "class TeacherDatasetPair:\n",
    "    \"\"\"Configuration for a teacher model paired with a specific dataset.\"\"\"\n",
    "    \n",
    "    # Teacher model\n",
    "    teacher_id: str\n",
    "    description: str = \"\"\n",
    "    \n",
    "    # Dataset configuration\n",
    "    dataset_name: str = \"\"\n",
    "    dataset_config: Optional[str] = None\n",
    "    dataset_split: str = \"train\"\n",
    "    \n",
    "    # Prompting functions (dataset-specific)\n",
    "    prompt_fn: Optional[Callable] = None  # (example) -> prompt_str\n",
    "    parse_gold_fn: Optional[Callable] = None  # (example) -> gold_answer\n",
    "    parse_pred_fn: Optional[Callable] = None  # (generated_text) -> predicted_answer\n",
    "    \n",
    "    # Sampling weight (normalized automatically)\n",
    "    weight: float = 1.0\n",
    "    \n",
    "    # Dataset cache\n",
    "    _dataset: Optional[Dataset] = field(default=None, repr=False)\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Student model\n",
    "    student_id: str = \"Qwen/Qwen3-0.6B-Base\"\n",
    "    \n",
    "    # Teacher-dataset pairs\n",
    "    pairs: List[TeacherDatasetPair] = field(default_factory=list)\n",
    "    \n",
    "    # GPU-auto-configured parameters (will be overridden)\n",
    "    dtype: torch.dtype = torch.bfloat16\n",
    "    batch_prompts: int = 4\n",
    "    samples_per_prompt: int = 4\n",
    "    student_mb: int = 8\n",
    "    lora_rank: int = 16\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if not self.pairs:\n",
    "            # Default: two specialized teachers\n",
    "            self.pairs = [\n",
    "                TeacherDatasetPair(\n",
    "                    teacher_id=\"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "                    description=\"Math expert (4B, latest)\",\n",
    "                    dataset_name=\"openai/gsm8k\",\n",
    "                    dataset_config=\"main\",\n",
    "                    dataset_split=\"train\",\n",
    "                    weight=1.0,\n",
    "                ),\n",
    "                TeacherDatasetPair(\n",
    "                    teacher_id=\"Qwen/Qwen3-3B-Instruct\",\n",
    "                    description=\"Code expert (3B)\",\n",
    "                    dataset_name=\"mbpp\",\n",
    "                    dataset_config=\"sanitized\",\n",
    "                    dataset_split=\"train\",\n",
    "                    weight=1.0,\n",
    "                ),\n",
    "            ]\n",
    "        \n",
    "        # Normalize weights\n",
    "        total_weight = sum(p.weight for p in self.pairs)\n",
    "        for p in self.pairs:\n",
    "            p.weight = p.weight / total_weight\n",
    "\n",
    "    # Generation parameters\n",
    "    max_new_tokens: int = 256\n",
    "    eval_temperature: float = 0.0\n",
    "    train_temperature: float = 0.7\n",
    "\n",
    "    # Training schedule\n",
    "    steps: int = 100\n",
    "    lr: float = 1e-4\n",
    "    weight_decay: float = 0.0\n",
    "\n",
    "    # Monitoring\n",
    "    log_every: int = 10\n",
    "    val_every: int = 20\n",
    "    val_sample_n: int = 100\n",
    "    ema_momentum: float = 0.9\n",
    "\n",
    "    # Validation size per dataset\n",
    "    val_rows_per_dataset: int = 200\n",
    "\n",
    "    # Output dir\n",
    "    run_root: str = f\"./run_multi_teacher_{int(time.time())}\"\n",
    "\n",
    "# Create config and apply GPU-detected settings\n",
    "cfg = Config(\n",
    "    dtype=gpu_config.dtype,\n",
    "    batch_prompts=gpu_config.batch_prompts,\n",
    "    samples_per_prompt=gpu_config.samples_per_prompt,\n",
    "    student_mb=gpu_config.student_mb,\n",
    "    lora_rank=gpu_config.lora_rank,\n",
    ")\n",
    "os.makedirs(cfg.run_root, exist_ok=True)\n",
    "\n",
    "print(\"\\n=== Multi-Teacher Configuration (Dataset-Specialized) ===\")\n",
    "print(f\"Student: {cfg.student_id}\")\n",
    "print(f\"Precision: {gpu_config.dtype_str}\")\n",
    "print(f\"\\nTeacher-Dataset Pairs ({len(cfg.pairs)}):\")\n",
    "for i, p in enumerate(cfg.pairs, 1):\n",
    "    print(f\"\\n  Pair {i}:\")\n",
    "    print(f\"    Teacher: {p.teacher_id}\")\n",
    "    print(f\"    Dataset: {p.dataset_name} ({p.dataset_config or 'default'})\")\n",
    "    print(f\"    Description: {p.description}\")\n",
    "    print(f\"    Weight: {p.weight:.3f}\")\n",
    "print(f\"\\nTraining: {cfg.steps} steps, {cfg.batch_prompts} prompts Ã— {cfg.samples_per_prompt} samples per step\")\n",
    "print(f\"LoRA rank: {cfg.lora_rank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Dataset Loading & Parsing\n",
    "\n",
    "Each dataset needs:\n",
    "1. **Prompt function**: Convert example â†’ prompt string\n",
    "2. **Gold parser**: Extract ground truth answer\n",
    "3. **Pred parser**: Extract model's predicted answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Dataset-specific parsers\n",
    "# --------------------------\n",
    "\n",
    "# GSM8K (Math)\n",
    "def gsm8k_prompt(example: dict) -> str:\n",
    "    return (\n",
    "        \"Solve step by step.\\n\"\n",
    "        \"Give ONLY ONE final numeric answer (no units), inside square brackets.\\n\"\n",
    "        f\"Problem: {example['question']}\\n\\nSolution:\"\n",
    "    )\n",
    "\n",
    "def gsm8k_parse_gold(example: dict) -> Optional[str]:\n",
    "    answer_text = example['answer']\n",
    "    m = re.search(r\"####\\s*(-?\\d+(?:\\.\\d+)?)\", answer_text)\n",
    "    if m: return m.group(1).strip()\n",
    "    nums = re.findall(r\"-?\\d+(?:\\.\\d+)?\", answer_text)\n",
    "    return nums[-1].strip() if nums else None\n",
    "\n",
    "def gsm8k_parse_pred(text: str) -> Optional[str]:\n",
    "    m = re.search(r\"\\[\\s*(-?\\d+(?:\\.\\d+)?)\\s*\\]\", text)\n",
    "    if m: return m.group(1).strip()\n",
    "    nums = re.findall(r\"-?\\d+(?:\\.\\d+)?\", text)\n",
    "    return nums[-1].strip() if nums else None\n",
    "\n",
    "# MBPP (Code)\n",
    "def mbpp_prompt(example: dict) -> str:\n",
    "    return (\n",
    "        \"Write a Python function to solve the following problem.\\n\"\n",
    "        \"Return ONLY the function code, starting with 'def'.\\n\"\n",
    "        f\"Problem: {example['text']}\\n\\nCode:\"\n",
    "    )\n",
    "\n",
    "def mbpp_parse_gold(example: dict) -> Optional[str]:\n",
    "    # MBPP has test cases, not direct answers\n",
    "    # For simplicity, we'll use the code itself as \"gold\"\n",
    "    return example.get('code', '').strip()\n",
    "\n",
    "def mbpp_parse_pred(text: str) -> Optional[str]:\n",
    "    # Extract code block (everything after the prompt)\n",
    "    # Look for 'def ' to find function start\n",
    "    match = re.search(r'(def \\w+.*)', text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return text.strip()\n",
    "\n",
    "# Assign parsers to pairs\n",
    "cfg.pairs[0].prompt_fn = gsm8k_prompt\n",
    "cfg.pairs[0].parse_gold_fn = gsm8k_parse_gold\n",
    "cfg.pairs[0].parse_pred_fn = gsm8k_parse_pred\n",
    "\n",
    "cfg.pairs[1].prompt_fn = mbpp_prompt\n",
    "cfg.pairs[1].parse_gold_fn = mbpp_parse_gold\n",
    "cfg.pairs[1].parse_pred_fn = mbpp_parse_pred\n",
    "\n",
    "# --------------------------\n",
    "# Load datasets\n",
    "# --------------------------\n",
    "print(\"\\n=== Loading Datasets ===\")\n",
    "for i, pair in enumerate(cfg.pairs, 1):\n",
    "    print(f\"\\nPair {i}: {pair.dataset_name}\")\n",
    "    \n",
    "    # Load full dataset\n",
    "    if pair.dataset_config:\n",
    "        ds_full = load_dataset(pair.dataset_name, pair.dataset_config, split=pair.dataset_split)\n",
    "    else:\n",
    "        ds_full = load_dataset(pair.dataset_name, split=pair.dataset_split)\n",
    "    \n",
    "    # Split into train/val\n",
    "    val_size = min(cfg.val_rows_per_dataset, len(ds_full))\n",
    "    pair._dataset = {\n",
    "        'val': ds_full.select(range(val_size)),\n",
    "        'train': ds_full.select(range(val_size, len(ds_full))),\n",
    "        'full': ds_full\n",
    "    }\n",
    "    \n",
    "    print(f\"  Loaded: {len(pair._dataset['train'])} train, {len(pair._dataset['val'])} val\")\n",
    "\n",
    "print(\"\\nAll datasets loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Load Models\n",
    "\n",
    "We load:\n",
    "- 1 student model (shared across all tasks)\n",
    "- N teacher models (one per dataset)\n",
    "\n",
    "**Auto-configured precision** based on your GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Model loading utilities (GPU-aware)\n",
    "# --------------------------\n",
    "def load_tokenizer(model_id: str):\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "    if tok.pad_token is None and tok.eos_token is not None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    tok.padding_side = \"left\"\n",
    "    return tok\n",
    "\n",
    "def make_lora_student(model_id: str) -> torch.nn.Module:\n",
    "    \"\"\"Load student with GPU-detected dtype and LoRA rank.\"\"\"\n",
    "    print(f\"Loading student in {gpu_config.dtype_str} precision...\")\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, torch_dtype=cfg.dtype, device_map=\"auto\"\n",
    "    )\n",
    "    base.config.use_cache = False\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=cfg.lora_rank,\n",
    "        lora_alpha=cfg.lora_rank * 2,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    )\n",
    "    return get_peft_model(base, lora_cfg)\n",
    "\n",
    "def load_teacher(model_id: str) -> torch.nn.Module:\n",
    "    \"\"\"Load teacher with GPU-detected dtype.\"\"\"\n",
    "    m = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, torch_dtype=cfg.dtype, device_map=\"auto\"\n",
    "    ).eval()\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad_(False)\n",
    "    return m\n",
    "\n",
    "# Load student\n",
    "print(f\"\\n=== Loading Models ({gpu_config.dtype_str}) ===\")\n",
    "print(f\"Student: {cfg.student_id}\")\n",
    "tok_student = load_tokenizer(cfg.student_id)\n",
    "\n",
    "# Load teachers (one per pair)\n",
    "print(f\"\\nLoading {len(cfg.pairs)} teachers...\")\n",
    "for i, pair in enumerate(cfg.pairs, 1):\n",
    "    print(f\"  [{i}/{len(cfg.pairs)}] {pair.teacher_id}\")\n",
    "    pair._teacher_model = load_teacher(pair.teacher_id)\n",
    "    pair._teacher_tokenizer = load_tokenizer(pair.teacher_id)\n",
    "    print(f\"       âœ“ Loaded for {pair.dataset_name}\")\n",
    "\n",
    "print(f\"\\nâœ“ All models loaded in {gpu_config.dtype_str}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Baseline Evaluation\n",
    "\n",
    "Evaluate each teacher on its **own specialized dataset** (and student on all datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Evaluation utilities\n",
    "# --------------------------\n",
    "def _encode_cuda(tokenizer, texts: List[str], max_length=2048) -> Dict[str, torch.Tensor]:\n",
    "    enc = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    return {k: v.to(\"cuda\") for k, v in enc.items()}\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_on_dataset(\n",
    "    model, tokenizer, dataset, prompt_fn, parse_gold_fn, parse_pred_fn,\n",
    "    num_examples: Optional[int] = None, temperature: float = 0.0,\n",
    "    max_new_tokens: int = 256, batch_size: int = 32, desc: str = \"Eval\"\n",
    ") -> float:\n",
    "    \"\"\"Exact-match accuracy on a specific dataset.\"\"\"\n",
    "    n = len(dataset) if num_examples is None else min(num_examples, len(dataset))\n",
    "    rows = dataset.select(range(n))\n",
    "    correct = 0\n",
    "\n",
    "    was_cache = getattr(model.config, \"use_cache\", True)\n",
    "    model.eval(); model.config.use_cache = True\n",
    "\n",
    "    for i in tqdm(range(0, n, batch_size), desc=desc):\n",
    "        batch = rows.select(range(i, min(i + batch_size, n)))\n",
    "        prompts = [prompt_fn(ex) for ex in batch]\n",
    "        enc = _encode_cuda(tokenizer, prompts)\n",
    "\n",
    "        gen_kwargs = dict(\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True\n",
    "        )\n",
    "        if temperature and temperature > 0.0:\n",
    "            gen_kwargs.update(do_sample=True, temperature=temperature, top_p=0.9)\n",
    "        else:\n",
    "            gen_kwargs.update(do_sample=False)\n",
    "\n",
    "        outs = model.generate(**enc, **gen_kwargs)\n",
    "        txts = tokenizer.batch_decode(outs, skip_special_tokens=True)\n",
    "\n",
    "        for ex, text in zip(batch, txts):\n",
    "            pred = parse_pred_fn(text) or \"\"\n",
    "            gold = parse_gold_fn(ex) or \"\"\n",
    "            correct += int(pred == gold)\n",
    "\n",
    "    model.config.use_cache = was_cache\n",
    "    return correct / max(n, 1)\n",
    "\n",
    "# --------------------------\n",
    "# Compute baselines\n",
    "# --------------------------\n",
    "print(\"\\n=== Computing Baselines ===\")\n",
    "\n",
    "# Create temporary student for baseline\n",
    "student_eval = make_lora_student(cfg.student_id)\n",
    "\n",
    "baselines = {}\n",
    "\n",
    "for i, pair in enumerate(cfg.pairs, 1):\n",
    "    dataset_name = pair.dataset_name.split('/')[-1]\n",
    "    val_ds = pair._dataset['val']\n",
    "    \n",
    "    print(f\"\\n--- Pair {i}: {dataset_name} ---\")\n",
    "    \n",
    "    # Student baseline on this dataset\n",
    "    print(f\"Evaluating student on {dataset_name}...\")\n",
    "    student_em = evaluate_on_dataset(\n",
    "        student_eval, tok_student, val_ds,\n",
    "        pair.prompt_fn, pair.parse_gold_fn, pair.parse_pred_fn,\n",
    "        temperature=cfg.eval_temperature, max_new_tokens=cfg.max_new_tokens,\n",
    "        batch_size=64, desc=f\"Student on {dataset_name}\"\n",
    "    )\n",
    "    baselines[f\"student_{dataset_name}\"] = student_em\n",
    "    print(f\"  Student EM: {student_em:.4f}\")\n",
    "    \n",
    "    # Teacher baseline on its specialized dataset\n",
    "    print(f\"Evaluating teacher (specialist) on {dataset_name}...\")\n",
    "    teacher_em = evaluate_on_dataset(\n",
    "        pair._teacher_model, pair._teacher_tokenizer, val_ds,\n",
    "        pair.prompt_fn, pair.parse_gold_fn, pair.parse_pred_fn,\n",
    "        temperature=cfg.eval_temperature, max_new_tokens=cfg.max_new_tokens,\n",
    "        batch_size=32, desc=f\"Teacher {i} on {dataset_name}\"\n",
    "    )\n",
    "    baselines[f\"teacher_{i}_{dataset_name}\"] = teacher_em\n",
    "    print(f\"  Teacher EM: {teacher_em:.4f}\")\n",
    "\n",
    "print(\"\\n=== Baseline Summary ===\")\n",
    "for key, val in baselines.items():\n",
    "    print(f\"{key:30s}: {val:.4f}\")\n",
    "\n",
    "with open(os.path.join(cfg.run_root, \"baselines.json\"), \"w\") as f:\n",
    "    json.dump(baselines, f, indent=2)\n",
    "\n",
    "del student_eval; torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Multi-Teacher Training Utilities\n",
    "\n",
    "Key change: We **sample** a (dataset, teacher) pair each step, not aggregate all teachers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Training utilities\n",
    "# --------------------------\n",
    "def mask_before_first_eos(next_ids: torch.Tensor, eos_id: int) -> torch.Tensor:\n",
    "    is_eos = (next_ids == eos_id)\n",
    "    csum = is_eos.cumsum(dim=1)\n",
    "    return csum.eq(0)\n",
    "\n",
    "def student_logp_batched(student, pad_id, full_ids, next_ids, T, micro_bsz):\n",
    "    outs = []\n",
    "    for s in range(0, full_ids.size(0), micro_bsz):\n",
    "        chunk = full_ids[s:s+micro_bsz]; nxt = next_ids[s:s+micro_bsz]\n",
    "        out = student(input_ids=chunk[:, :-1],\n",
    "                      attention_mask=(chunk[:, :-1] != pad_id))\n",
    "        logits = out.logits[:, -T:, :]\n",
    "        logp = F.log_softmax(logits, dim=-1).gather(-1, nxt.unsqueeze(-1)).squeeze(-1)\n",
    "        outs.append(logp)\n",
    "        del out, logits\n",
    "    return torch.cat(outs, dim=0)\n",
    "\n",
    "def _decode_token_str(tokenizer, token_id: int) -> str:\n",
    "    return tokenizer.decode([int(token_id)],\n",
    "                            skip_special_tokens=False,\n",
    "                            clean_up_tokenization_spaces=False)\n",
    "\n",
    "def _encode_text_ids(tokenizer, text: str):\n",
    "    return tokenizer(text, add_special_tokens=False,\n",
    "                     return_tensors=\"pt\").input_ids[0].tolist()\n",
    "\n",
    "@torch.no_grad()\n",
    "def teacher_logp_grouped(\n",
    "    teacher, tok_teacher, tok_student, prompts: List[str],\n",
    "    next_ids: torch.Tensor, max_len: Optional[int] = None\n",
    "):\n",
    "    \"\"\"Cross-tokenizer teacher scoring.\"\"\"\n",
    "    device = teacher.device if hasattr(teacher, \"device\") else \"cuda\"\n",
    "    B, T = next_ids.shape\n",
    "    out = torch.zeros((B, T), device=device, dtype=torch.float32)\n",
    "\n",
    "    if max_len is None:\n",
    "        max_len = int(getattr(teacher.config, \"max_position_embeddings\", 2048))\n",
    "\n",
    "    for b in range(B):\n",
    "        prompt_text = prompts[b]\n",
    "        ctx_ids = _encode_text_ids(tok_teacher, prompt_text)\n",
    "\n",
    "        groups = []\n",
    "        for t in range(T):\n",
    "            s_tok_id = int(next_ids[b, t].item())\n",
    "            piece = _decode_token_str(tok_student, s_tok_id)\n",
    "            ids_t = _encode_text_ids(tok_teacher, piece)\n",
    "            groups.append(ids_t)\n",
    "\n",
    "        flat_gen = [tid for g in groups for tid in g]\n",
    "        if len(flat_gen) == 0:\n",
    "            continue\n",
    "\n",
    "        total = len(ctx_ids) + len(flat_gen)\n",
    "        if total > max_len:\n",
    "            overflow = total - max_len\n",
    "            ctx_ids = ctx_ids[overflow:]\n",
    "\n",
    "        full = ctx_ids + flat_gen\n",
    "        if len(full) < 2:\n",
    "            continue\n",
    "\n",
    "        input_ids = torch.tensor(full[:-1], device=device).unsqueeze(0)\n",
    "        labels    = torch.tensor(full[1:],  device=device).unsqueeze(0)\n",
    "        attn_mask = torch.ones_like(input_ids, device=device)\n",
    "\n",
    "        outputs = teacher(input_ids=input_ids, attention_mask=attn_mask)\n",
    "        logprobs = F.log_softmax(outputs.logits, dim=-1)\n",
    "        tok_lp = logprobs.gather(-1, labels.unsqueeze(-1)).squeeze(-1)[0]\n",
    "\n",
    "        start = max(len(ctx_ids) - 1, 0)\n",
    "        gen_lp = tok_lp[start : start + len(flat_gen)]\n",
    "\n",
    "        off = 0\n",
    "        for t, g in enumerate(groups):\n",
    "            k = len(g)\n",
    "            if k > 0:\n",
    "                out[b, t] = gen_lp[off : off + k].sum()\n",
    "            off += k\n",
    "\n",
    "    return out\n",
    "\n",
    "class LiveTable:\n",
    "    def __init__(self, title: str = \"Training metrics\", max_rows: int = 200):\n",
    "        self.title = title\n",
    "        self.max_rows = max_rows\n",
    "        self.rows = []\n",
    "        empty = pd.DataFrame(columns=[\"step\",\"pair\",\"loss\",\"loss_ema\",\"revkl\",\"revkl_ema\",\"tokens\",\"val_em\"])\n",
    "        self.handle = display(self._styled(empty), display_id=True)\n",
    "\n",
    "    def _styled(self, df: pd.DataFrame):\n",
    "        styler = (\n",
    "            df.style\n",
    "              .set_caption(self.title)\n",
    "              .format({\n",
    "                  \"loss\": \"{:.4f}\",\n",
    "                  \"loss_ema\": \"{:.4f}\",\n",
    "                  \"revkl\": \"{:.4f}\",\n",
    "                  \"revkl_ema\": \"{:.4f}\",\n",
    "                  \"val_em\": (lambda v: \"\" if pd.isna(v) else f\"{v:.3f}\"),\n",
    "              })\n",
    "        )\n",
    "        try:\n",
    "            styler = styler.hide(axis=\"index\")\n",
    "            return styler\n",
    "        except Exception:\n",
    "            pass\n",
    "        return styler\n",
    "\n",
    "    def update(self, *, step, pair_name, loss, loss_ema, revkl, revkl_ema, tokens, val_em=None):\n",
    "        self.rows.append(dict(\n",
    "            step=int(step),\n",
    "            pair=str(pair_name),\n",
    "            loss=float(loss),\n",
    "            loss_ema=(None if loss_ema is None else float(loss_ema)),\n",
    "            revkl=float(revkl),\n",
    "            revkl_ema=(None if revkl_ema is None else float(revkl_ema)),\n",
    "            tokens=int(tokens),\n",
    "            val_em=(None if val_em is None else float(val_em)),\n",
    "        ))\n",
    "        rows = self.rows[-self.max_rows:]\n",
    "        df = pd.DataFrame(rows, columns=[\"step\",\"pair\",\"loss\",\"loss_ema\",\"revkl\",\"revkl_ema\",\"tokens\",\"val_em\"])\n",
    "        self.handle.update(self._styled(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Training Loop: Alternating Dataset-Teacher Pairs\n",
    "\n",
    "Each step:\n",
    "1. **Sample** a (dataset, teacher) pair based on weights\n",
    "2. Get prompts from **that dataset**\n",
    "3. Student generates solutions\n",
    "4. **Only that teacher** (the specialist) scores the rollouts\n",
    "5. Student learns from specialized feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Multi-Teacher Training Loop (Dataset-Specialized)\n",
    "# --------------------------\n",
    "def ema(prev, new, beta):\n",
    "    return new if prev is None else (beta * prev + (1 - beta) * new)\n",
    "\n",
    "def run_multi_teacher_training(run_dir: str):\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    # Fresh student\n",
    "    student = make_lora_student(cfg.student_id)\n",
    "    optimizer = torch.optim.AdamW(student.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    # Prepare sampling weights\n",
    "    weights = np.array([p.weight for p in cfg.pairs])\n",
    "    \n",
    "    ema_loss = None\n",
    "    ema_revkl = None\n",
    "    tokens_graded_cum = 0\n",
    "    logs = []\n",
    "\n",
    "    table = LiveTable(title=f\"Multi-Teacher OPD ({len(cfg.pairs)} pairs, {gpu_config.dtype_str})\")\n",
    "\n",
    "    pbar = tqdm(range(cfg.steps), desc=f\"Multi-Teacher OPD\")\n",
    "    for step in pbar:\n",
    "        # 1) Sample a (dataset, teacher) pair\n",
    "        rng = np.random.default_rng(SEED + step)\n",
    "        pair_idx = rng.choice(len(cfg.pairs), p=weights)\n",
    "        pair = cfg.pairs[pair_idx]\n",
    "        \n",
    "        dataset_name = pair.dataset_name.split('/')[-1]\n",
    "        train_ds = pair._dataset['train']\n",
    "        \n",
    "        # 2) Sample prompts from THIS dataset\n",
    "        prompt_indices = rng.choice(len(train_ds), size=cfg.batch_prompts, replace=False)\n",
    "        examples = train_ds.select(prompt_indices.tolist())\n",
    "        prompts = [pair.prompt_fn(ex) for ex in examples]\n",
    "        prompts_rep = sum(([p] * cfg.samples_per_prompt for p in prompts), [])\n",
    "        \n",
    "        enc = tok_student(prompts_rep, padding=True, truncation=True,\n",
    "                          max_length=2048, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        # 3) Student rollouts\n",
    "        with torch.no_grad():\n",
    "            gen_out = student.generate(\n",
    "                **enc,\n",
    "                do_sample=True, temperature=cfg.train_temperature, top_p=0.9,\n",
    "                max_new_tokens=cfg.max_new_tokens,\n",
    "                eos_token_id=tok_student.eos_token_id,\n",
    "                pad_token_id=tok_student.pad_token_id,\n",
    "                return_dict_in_generate=True, output_scores=True\n",
    "            )\n",
    "            seqs = gen_out.sequences\n",
    "            scores_list = list(gen_out.scores)\n",
    "            T = len(scores_list)\n",
    "            next_ids = seqs[:, -T:]\n",
    "            valid_mask = mask_before_first_eos(next_ids, eos_id=tok_student.eos_token_id).float()\n",
    "\n",
    "        # 4) Student log-probs (with grad)\n",
    "        student.train(); student.config.use_cache = False\n",
    "        logp_s = student_logp_batched(\n",
    "            student, tok_student.pad_token_id, seqs, next_ids, T, cfg.student_mb\n",
    "        )\n",
    "\n",
    "        # 5) Teacher log-probs - ONLY the specialist for this dataset\n",
    "        logp_t = teacher_logp_grouped(\n",
    "            teacher=pair._teacher_model,\n",
    "            tok_teacher=pair._teacher_tokenizer,\n",
    "            tok_student=tok_student,\n",
    "            prompts=prompts_rep,\n",
    "            next_ids=next_ids,\n",
    "            max_len=getattr(pair._teacher_model.config, \"max_position_embeddings\", 2048)\n",
    "        )\n",
    "\n",
    "        # 6) Reverse-KL policy gradient\n",
    "        adv = (logp_t - logp_s).clamp(-5, 5)\n",
    "        denom = valid_mask.sum().clamp_min(1.0)\n",
    "        loss = - ((valid_mask * adv.detach()) * logp_s).sum() / denom\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rev_kl = ((logp_s - logp_t) * valid_mask).sum().item() / float(denom)\n",
    "            tokens_graded_cum += int(denom.item())\n",
    "            ema_loss = ema(ema_loss, float(loss.item()), cfg.ema_momentum)\n",
    "            ema_revkl = ema(ema_revkl, float(rev_kl), cfg.ema_momentum)\n",
    "\n",
    "        # Periodic validation\n",
    "        val_em = None\n",
    "        if (step % cfg.val_every == 0) or (step == cfg.steps - 1):\n",
    "            student.eval(); student.config.use_cache = True\n",
    "            # Evaluate on this pair's validation set\n",
    "            val_em = evaluate_on_dataset(\n",
    "                student, tok_student, pair._dataset['val'],\n",
    "                pair.prompt_fn, pair.parse_gold_fn, pair.parse_pred_fn,\n",
    "                num_examples=min(cfg.val_sample_n, len(pair._dataset['val'])),\n",
    "                temperature=0.0, max_new_tokens=cfg.max_new_tokens,\n",
    "                batch_size=32, desc=f\"VAL {dataset_name}\"\n",
    "            )\n",
    "            student.train(); student.config.use_cache = False\n",
    "\n",
    "        row = dict(\n",
    "            step=int(step),\n",
    "            pair_idx=int(pair_idx),\n",
    "            pair_name=dataset_name,\n",
    "            train_loss=float(loss.item()),\n",
    "            train_loss_ema=float(ema_loss) if ema_loss is not None else None,\n",
    "            train_revkl=float(rev_kl),\n",
    "            train_revkl_ema=float(ema_revkl) if ema_revkl is not None else None,\n",
    "            tokens_graded=int(tokens_graded_cum),\n",
    "            **({\"val_em\": float(val_em)} if val_em is not None else {})\n",
    "        )\n",
    "        logs.append(row)\n",
    "\n",
    "        if (step % cfg.log_every == 0) or (val_em is not None):\n",
    "            table.update(\n",
    "                step=row[\"step\"],\n",
    "                pair_name=row[\"pair_name\"],\n",
    "                loss=row[\"train_loss\"],\n",
    "                loss_ema=row[\"train_loss_ema\"],\n",
    "                revkl=row[\"train_revkl\"],\n",
    "                revkl_ema=row[\"train_revkl_ema\"],\n",
    "                tokens=row[\"tokens_graded\"],\n",
    "                val_em=row.get(\"val_em\", None),\n",
    "            )\n",
    "\n",
    "        postfix = {\n",
    "            \"pair\": dataset_name[:4],\n",
    "            \"loss\": f\"{loss.item():.3f}\",\n",
    "            \"ema\": f\"{(ema_loss if ema_loss is not None else loss.item()):.3f}\",\n",
    "            \"rkl\": f\"{rev_kl:.3f}\",\n",
    "        }\n",
    "        if val_em is not None:\n",
    "            postfix[\"val\"] = f\"{val_em:.3f}\"\n",
    "        pbar.set_postfix(**postfix)\n",
    "\n",
    "        del scores_list\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Save logs\n",
    "    pd.DataFrame(logs).to_csv(os.path.join(run_dir, \"train_logs.csv\"), index=False)\n",
    "\n",
    "    # Final evaluation on ALL datasets\n",
    "    student.eval(); student.config.use_cache = True\n",
    "    final_results = {}\n",
    "    \n",
    "    for i, pair in enumerate(cfg.pairs, 1):\n",
    "        dataset_name = pair.dataset_name.split('/')[-1]\n",
    "        val_em = evaluate_on_dataset(\n",
    "            student, tok_student, pair._dataset['val'],\n",
    "            pair.prompt_fn, pair.parse_gold_fn, pair.parse_pred_fn,\n",
    "            temperature=0.0, max_new_tokens=cfg.max_new_tokens,\n",
    "            batch_size=64, desc=f\"Final {dataset_name}\"\n",
    "        )\n",
    "        final_results[f\"final_{dataset_name}\"] = float(val_em)\n",
    "\n",
    "    # Save adapters and summary\n",
    "    save_dir = os.path.join(run_dir, \"adapters_lora\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    student.save_pretrained(save_dir)\n",
    "\n",
    "    summary = dict(\n",
    "        gpu=gpu_config.name,\n",
    "        vram_gb=gpu_config.vram_gb,\n",
    "        precision=gpu_config.dtype_str,\n",
    "        num_pairs=len(cfg.pairs),\n",
    "        steps=cfg.steps,\n",
    "        tokens_graded=tokens_graded_cum,\n",
    "        **final_results\n",
    "    )\n",
    "    with open(os.path.join(run_dir, \"summary.json\"), \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    del student; torch.cuda.empty_cache()\n",
    "    return summary, logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Multi-Teacher Training (Dataset-Specialized) ===\")\n",
    "print(f\"GPU: {gpu_config.name} ({gpu_config.vram_gb:.1f} GB, {gpu_config.dtype_str})\")\n",
    "print(f\"Training for {cfg.steps} steps\")\n",
    "print(f\"Pairs: {[p.dataset_name.split('/')[-1] for p in cfg.pairs]}\")\n",
    "\n",
    "run_dir = os.path.join(cfg.run_root, \"multi_teacher_opd\")\n",
    "summary, _ = run_multi_teacher_training(run_dir)\n",
    "\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(json.dumps(summary, indent=2))\n",
    "\n",
    "print(f\"\\n=== Improvements ===\")\n",
    "for pair in cfg.pairs:\n",
    "    dataset_name = pair.dataset_name.split('/')[-1]\n",
    "    baseline_key = f\"student_{dataset_name}\"\n",
    "    final_key = f\"final_{dataset_name}\"\n",
    "    if baseline_key in baselines and final_key in summary:\n",
    "        improvement = summary[final_key] - baselines[baseline_key]\n",
    "        print(f\"{dataset_name:15s}: {baselines[baseline_key]:.4f} â†’ {summary[final_key]:.4f} (+{improvement:.4f})\")\n",
    "\n",
    "print(f\"\\nArtifacts saved to: {cfg.run_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Why Dataset-Teacher Pairing?\n",
    "\n",
    "**âœ… Specialization**\n",
    "- Each teacher evaluates only its domain of expertise\n",
    "- Math teacher grades math; code teacher grades code\n",
    "- No wasted compute on out-of-domain evaluation\n",
    "\n",
    "**âœ… True Diversity**\n",
    "- Student exposed to multiple task distributions\n",
    "- Learns diverse skills from specialized experts\n",
    "- More robust than single-domain training\n",
    "\n",
    "**âœ… Production-Ready Pattern**\n",
    "- Matches tinker-cookbook implementation\n",
    "- Scales to many dataset-teacher pairs\n",
    "- Easy to add new domains\n",
    "\n",
    "**âœ… GPU Auto-Configuration**\n",
    "- Automatic precision selection (fp8/bf16/fp16)\n",
    "- Batch sizes adapted to VRAM\n",
    "- LoRA rank tuned for GPU capabilities\n",
    "\n",
    "### GPU-Aware Optimizations\n",
    "\n",
    "| GPU | VRAM | Precision | Batch | LoRA Rank |\n",
    "|-----|------|-----------|-------|--------|\n",
    "| **H100** | 80GB | bf16 (fp8 ready) | 8Ã—8 | 32 |\n",
    "| **A100 80GB** | 80GB | bf16 | 8Ã—8 | 32 |\n",
    "| **A100 40GB** | 40GB | bf16 | 4Ã—4 | 16 |\n",
    "| **L40S** | 48GB | bf16 | 4Ã—4 | 16 |\n",
    "| **RTX 4090** | 24GB | bf16 | 2Ã—4 | 16 |\n",
    "| **V100** | 16GB | fp16 | 2Ã—2 | 8 |\n",
    "| **T4** | 16GB | fp16 | 2Ã—2 | 8 |\n",
    "\n",
    "### vs. Naive Multi-Teacher Aggregation\n",
    "\n",
    "| Approach | Dataset-Teacher Pairing | Naive Aggregation |\n",
    "|----------|------------------------|-------------------|\n",
    "| **Datasets** | Multiple, diverse | Single |\n",
    "| **Teachers per step** | 1 (the specialist) | All teachers |\n",
    "| **Compute** | Linear in teachers | Linear in teachers |\n",
    "| **Diversity** | High (different domains) | Low (same data) |\n",
    "| **Specialization** | Each teacher in its domain | Redundant on same task |\n",
    "\n",
    "### Extension Ideas\n",
    "\n",
    "1. **Dynamic weighting**: Adjust pair weights based on student performance\n",
    "2. **Curriculum**: Start with easier datasets, add harder ones\n",
    "3. **More domains**: Add reasoning, dialog, translation, etc.\n",
    "4. **Per-dataset teachers**: Multiple teachers per dataset with voting\n",
    "5. **FP8 quantization**: For H100, enable fp8 for even faster training\n",
    "\n",
    "### Production Scale (tinker-cookbook)\n",
    "\n",
    "```python\n",
    "Pair 1: Qwen3-32B + DeepMath (deep math reasoning)\n",
    "Pair 2: Qwen3-235B + Tulu3 (instruction following)\n",
    "Student: Qwen3-8B\n",
    "Result: Student learns BOTH math and instruction skills\n",
    "```\n",
    "\n",
    "This is the **correct** multi-teacher approach with **GPU auto-configuration**! ðŸŽ¯âš¡"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
