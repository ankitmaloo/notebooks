{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Without Regret: Parameter-Efficient RL Fine-Tuning\n",
    "\n",
    "This notebook demonstrates **LoRA without regret** \u2014 a technique for doing reinforcement learning (RL) fine-tuning with Low-Rank Adaptation (LoRA) adapters while maintaining the efficiency and performance benefits of LoRA.\n",
    "\n",
    "## What is LoRA Without Regret?\n",
    "\n",
    "Standard LoRA freezes the base model and only trains small adapter matrices. However, for RL tasks, we need to:\n",
    "1. **Update LoRA parameters** based on reward signals\n",
    "2. **Generate rollouts** efficiently for policy optimization\n",
    "3. **Not freeze layers** \u2014 LoRA adapters are actively updated during training\n",
    "\n",
    "This notebook uses **GRPO (Group Relative Policy Optimization)** to train LoRA adapters on GSM8K math problems with reward-based learning.\n",
    "\n",
    "## Key Differences from Standard LoRA:\n",
    "- **LoRA layers are NOT frozen** \u2014 they're the trainable parameters\n",
    "- Uses policy gradient methods (REINFORCE/GRPO) instead of supervised learning\n",
    "- Integrates with inference engines (vLLM, sglang) for efficient rollout generation\n",
    "\n",
    "You'll need an **A100 GPU (40 GB)** or better for this notebook.\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "- [LoRA Without Regret (GitHub)](https://github.com/michaelbzhu/lora-without-regret)\n",
    "- [Thinking Machines LoRA Guide](https://thinkingmachines.ai/blog/lora/)\n",
    "- [Karpathy's NanoChat RL](https://github.com/karpathy/nanochat/blob/master/scripts/chat_rl.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udee0\ufe0f Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup\n",
    "!nvidia-smi -L || true\n",
    "\n",
    "import os, sys, random, numpy as np, torch, json, time, platform\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Install dependencies\n",
    "try:\n",
    "    get_ipython().run_line_magic(\"uv\", \"pip -q install transformers==4.51.3 accelerate==1.4.0 peft==0.14.0 datasets==3.3.2 evaluate==0.4.3 trl==0.13.0 sentencepiece protobuf tqdm matplotlib > /dev/null\")\n",
    "except Exception:\n",
    "    get_ipython().run_line_magic(\"pip\", \"-q install transformers==4.51.3 accelerate==1.4.0 peft==0.14.0 datasets==3.3.2 evaluate==0.4.3 trl==0.13.0 sentencepiece protobuf tqdm matplotlib > /dev/null\")\n",
    "\n",
    "import transformers, datasets, peft, accelerate, matplotlib\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Accelerate:\", accelerate.__version__)\n",
    "print(\"PEFT:\", peft.__version__)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "assert DEVICE == \"cuda\", \"Please connect a GPU (A100+ recommended).\"\n",
    "\n",
    "print(f\"\\n\u2713 Running on {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2699\ufe0f Configuration Parameters\n",
    "\n",
    "All hyperparameters are defined here. You can experiment with these settings to see how they affect LoRA training with RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, List\n",
    "import re\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # ========== Model Settings ==========\n",
    "    model_id: str = \"Qwen/Qwen3-0.6B-Base\"\n",
    "    \n",
    "    # ========== LoRA Settings ==========\n",
    "    # IMPORTANT: These LoRA parameters are NOT frozen - they're actively trained!\n",
    "    lora_r: int = 16                    # LoRA rank (1-64, higher = more capacity)\n",
    "    lora_alpha: int = 32                # LoRA scaling factor\n",
    "    lora_dropout: float = 0.05          # Dropout for LoRA layers\n",
    "    lora_target_modules: List[str] = None  # Will be set to all attention + MLP\n",
    "    \n",
    "    # ========== RL Training Settings (GRPO) ==========\n",
    "    n_grpo_steps: int = 100             # Number of RL training steps\n",
    "    prompts_per_step: int = 32          # How many prompts to sample per step\n",
    "    rollouts_per_prompt: int = 8        # Group size (K samples per prompt)\n",
    "    \n",
    "    learning_rate: float = 9e-5         # Learning rate for LoRA parameters\n",
    "    weight_decay: float = 0.0           # Weight decay\n",
    "    \n",
    "    micro_batch_size: int = 2           # Micro-batch for gradient accumulation\n",
    "    gradient_accumulation_steps: int = 128  # Accumulate gradients over this many steps\n",
    "    epochs_per_step: int = 1            # Training epochs per GRPO step\n",
    "    max_grad_norm: float = 1.0          # Gradient clipping\n",
    "    \n",
    "    # ========== Generation Settings ==========\n",
    "    max_new_tokens: int = 256           # Max tokens to generate\n",
    "    temperature: float = 0.7            # Sampling temperature for rollouts\n",
    "    top_p: float = 0.9                  # Nucleus sampling\n",
    "    eval_temperature: float = 0.0       # Greedy decoding for evaluation\n",
    "    \n",
    "    # ========== Task Settings (GSM8K) ==========\n",
    "    prompt_template: str = (\n",
    "        \"Solve this math problem step by step.\\n\"\n",
    "        \"Give ONLY ONE final numeric answer (no units), inside square brackets.\\n\"\n",
    "        \"Problem: {question}\\n\\nSolution:\"\n",
    "    )\n",
    "    \n",
    "    # ========== Monitoring & Logging ==========\n",
    "    log_every: int = 5                  # Log metrics every N steps\n",
    "    eval_every: int = 10                # Evaluate on validation set every N steps\n",
    "    eval_samples: int = 100             # Number of validation samples\n",
    "    save_every: int = 25                # Save checkpoint every N steps\n",
    "    \n",
    "    # ========== Output ==========\n",
    "    run_name: str = f\"lora_rl_{int(time.time())}\"\n",
    "    output_dir: str = \"./lora_rl_runs\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # Default to all attention + MLP projection layers\n",
    "        if self.lora_target_modules is None:\n",
    "            self.lora_target_modules = [\n",
    "                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention\n",
    "                \"gate_proj\", \"up_proj\", \"down_proj\"      # MLP\n",
    "            ]\n",
    "        self.run_dir = os.path.join(self.output_dir, self.run_name)\n",
    "        os.makedirs(self.run_dir, exist_ok=True)\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Configuration:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {cfg.model_id}\")\n",
    "print(f\"LoRA rank: {cfg.lora_r}, alpha: {cfg.lora_alpha}\")\n",
    "print(f\"GRPO steps: {cfg.n_grpo_steps}\")\n",
    "print(f\"Prompts/step: {cfg.prompts_per_step}, Rollouts/prompt: {cfg.rollouts_per_prompt}\")\n",
    "print(f\"Learning rate: {cfg.learning_rate}\")\n",
    "print(f\"Output: {cfg.run_dir}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Load Dataset (GSM8K)\n",
    "\n",
    "We'll use GSM8K grade school math problems as our RL task. The reward is binary: 1 if the answer is correct, 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def render_prompt(question: str) -> str:\n",
    "    return cfg.prompt_template.format(question=question)\n",
    "\n",
    "def parse_gold_answer(answer_text: str) -> Optional[str]:\n",
    "    \"\"\"Extract numeric answer from GSM8K format (e.g., '#### 42')\"\"\"\n",
    "    m = re.search(r\"####\\s*(-?\\d+(?:\\.\\d+)?)\", answer_text)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    # Fallback: last number in text\n",
    "    nums = re.findall(r\"-?\\d+(?:\\.\\d+)?\", answer_text)\n",
    "    return nums[-1].strip() if nums else None\n",
    "\n",
    "def parse_pred_answer(text: str) -> Optional[str]:\n",
    "    \"\"\"Extract answer from model output (looks for [number])\"\"\"\n",
    "    m = re.search(r\"\\[\\s*(-?\\d+(?:\\.\\d+)?)\\s*\\]\", text)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    # Fallback: last number in text\n",
    "    nums = re.findall(r\"-?\\d+(?:\\.\\d+)?\", text)\n",
    "    return nums[-1].strip() if nums else None\n",
    "\n",
    "def compute_reward(generated_text: str, gold_answer: str) -> float:\n",
    "    \"\"\"Binary reward: 1.0 if correct, 0.0 otherwise\"\"\"\n",
    "    pred = parse_pred_answer(generated_text)\n",
    "    gold = parse_gold_answer(gold_answer)\n",
    "    if pred is None or gold is None:\n",
    "        return 0.0\n",
    "    return 1.0 if pred == gold else 0.0\n",
    "\n",
    "print(\"Loading GSM8K...\")\n",
    "ds_train = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n",
    "ds_test = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
    "\n",
    "# Split train into train/val\n",
    "val_size = min(200, len(ds_train))\n",
    "ds_val = ds_train.select(range(val_size))\n",
    "ds_train = ds_train.select(range(val_size, len(ds_train)))\n",
    "\n",
    "print(f\"Splits: {len(ds_train)} train | {len(ds_val)} val | {len(ds_test)} test\")\n",
    "\n",
    "# Example\n",
    "ex = ds_train[0]\n",
    "print(\"\\nExample:\")\n",
    "print(f\"Q: {ex['question']}\")\n",
    "print(f\"A: {ex['answer']}\")\n",
    "print(f\"Gold: [{parse_gold_answer(ex['answer'])}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfd7\ufe0f Load Model with LoRA\n",
    "\n",
    "We load the base model and add LoRA adapters. **Crucially, only the LoRA parameters are trainable** \u2014 the base model weights are frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "def load_model_with_lora(model_id: str, lora_config: LoraConfig):\n",
    "    \"\"\"Load base model and apply LoRA adapters\"\"\"\n",
    "    print(f\"Loading base model: {model_id}...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model.config.use_cache = False  # Disable for training\n",
    "    \n",
    "    print(\"Applying LoRA adapters...\")\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Print trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable params: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_id, use_fast=True, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"  # For decoder-only models\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=cfg.lora_r,\n",
    "    lora_alpha=cfg.lora_alpha,\n",
    "    lora_dropout=cfg.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=cfg.lora_target_modules,\n",
    ")\n",
    "\n",
    "# Load model with LoRA\n",
    "model = load_model_with_lora(cfg.model_id, lora_config)\n",
    "\n",
    "print(\"\\n\u2713 Model ready for RL training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcc8 Baseline Evaluation\n",
    "\n",
    "Let's see how well the model performs before RL training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_accuracy(model, tokenizer, dataset, num_samples: int = 100, temperature: float = 0.0, batch_size: int = 16):\n",
    "    \"\"\"Evaluate exact match accuracy on dataset\"\"\"\n",
    "    model.eval()\n",
    "    model.config.use_cache = True\n",
    "    \n",
    "    n = min(num_samples, len(dataset))\n",
    "    correct = 0\n",
    "    \n",
    "    for i in tqdm(range(0, n, batch_size), desc=\"Evaluating\"):\n",
    "        batch = dataset.select(range(i, min(i + batch_size, n)))\n",
    "        prompts = [render_prompt(ex[\"question\"]) for ex in batch]\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(prompts, padding=True, truncation=True, max_length=2048, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        # Generate\n",
    "        gen_kwargs = dict(\n",
    "            max_new_tokens=cfg.max_new_tokens,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True\n",
    "        )\n",
    "        if temperature > 0.0:\n",
    "            gen_kwargs.update(do_sample=True, temperature=temperature, top_p=cfg.top_p)\n",
    "        else:\n",
    "            gen_kwargs.update(do_sample=False)\n",
    "        \n",
    "        outputs = model.generate(**inputs, **gen_kwargs)\n",
    "        texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        \n",
    "        # Check correctness\n",
    "        for ex, text in zip(batch, texts):\n",
    "            reward = compute_reward(text, ex[\"answer\"])\n",
    "            correct += int(reward)\n",
    "    \n",
    "    model.config.use_cache = False\n",
    "    accuracy = correct / n\n",
    "    return accuracy\n",
    "\n",
    "print(\"Computing baseline accuracy...\")\n",
    "baseline_acc = evaluate_accuracy(model, tokenizer, ds_test, num_samples=100, temperature=0.0)\n",
    "print(f\"\\nBaseline Test Accuracy: {baseline_acc:.1%}\")\n",
    "\n",
    "# Save baseline\n",
    "with open(os.path.join(cfg.run_dir, \"baseline.json\"), \"w\") as f:\n",
    "    json.dump({\"baseline_accuracy\": baseline_acc}, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf GRPO Training Loop\n",
    "\n",
    "This is the core RL training loop using **Group Relative Policy Optimization (GRPO)**:\n",
    "\n",
    "1. Sample prompts from the training set\n",
    "2. Generate K rollouts per prompt (group)\n",
    "3. Compute rewards for each rollout\n",
    "4. Normalize advantages within each group (subtract mean)\n",
    "5. Update LoRA parameters using policy gradients\n",
    "\n",
    "**Key insight**: We only train on the generated tokens (response), not the prompt. This is enforced via masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "def create_response_mask(input_ids: torch.Tensor, prompt_lengths: List[int], eos_token_id: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create mask that's 1 for generated tokens (response) and 0 for prompt tokens.\n",
    "    Also masks out tokens after EOS.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    mask = torch.zeros_like(input_ids, dtype=torch.float32)\n",
    "    \n",
    "    for i, prompt_len in enumerate(prompt_lengths):\n",
    "        # Find first EOS in generated part\n",
    "        generated_ids = input_ids[i, prompt_len:]\n",
    "        eos_positions = (generated_ids == eos_token_id).nonzero(as_tuple=True)[0]\n",
    "        \n",
    "        if len(eos_positions) > 0:\n",
    "            first_eos = eos_positions[0].item()\n",
    "            mask[i, prompt_len:prompt_len + first_eos] = 1.0\n",
    "        else:\n",
    "            mask[i, prompt_len:] = 1.0\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def compute_policy_logprobs(model, input_ids: torch.Tensor, response_mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute log probabilities under current policy for generated tokens.\n",
    "    Returns: [batch_size, seq_len] log probs (0 for masked positions)\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    outputs = model(input_ids=input_ids[:, :-1], attention_mask=(input_ids[:, :-1] != tokenizer.pad_token_id))\n",
    "    logits = outputs.logits  # [batch, seq_len-1, vocab]\n",
    "    \n",
    "    # Get log probs for actual next tokens\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    next_tokens = input_ids[:, 1:].unsqueeze(-1)  # [batch, seq_len-1, 1]\n",
    "    token_log_probs = log_probs.gather(-1, next_tokens).squeeze(-1)  # [batch, seq_len-1]\n",
    "    \n",
    "    # Pad to match input_ids length and apply mask\n",
    "    token_log_probs = F.pad(token_log_probs, (1, 0), value=0.0)  # [batch, seq_len]\n",
    "    token_log_probs = token_log_probs * response_mask\n",
    "    \n",
    "    return token_log_probs\n",
    "\n",
    "class MetricsTable:\n",
    "    \"\"\"Live updating table for training metrics\"\"\"\n",
    "    def __init__(self):\n",
    "        self.rows = []\n",
    "        self.df = pd.DataFrame(columns=[\"step\", \"loss\", \"avg_reward\", \"kl_div\", \"val_acc\"])\n",
    "        self.handle = display(self.df, display_id=True)\n",
    "    \n",
    "    def update(self, step, loss=None, avg_reward=None, kl_div=None, val_acc=None):\n",
    "        row = {\"step\": step}\n",
    "        if loss is not None:\n",
    "            row[\"loss\"] = f\"{loss:.4f}\"\n",
    "        if avg_reward is not None:\n",
    "            row[\"avg_reward\"] = f\"{avg_reward:.3f}\"\n",
    "        if kl_div is not None:\n",
    "            row[\"kl_div\"] = f\"{kl_div:.4f}\"\n",
    "        if val_acc is not None:\n",
    "            row[\"val_acc\"] = f\"{val_acc:.1%}\"\n",
    "        \n",
    "        self.rows.append(row)\n",
    "        self.df = pd.DataFrame(self.rows)\n",
    "        self.handle.update(self.df)\n",
    "\n",
    "def save_lora_checkpoint(model, step: int, output_dir: str):\n",
    "    \"\"\"Save LoRA adapter weights\"\"\"\n",
    "    save_path = os.path.join(output_dir, f\"checkpoint_step_{step}\")\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    model.save_pretrained(save_path)\n",
    "    print(f\"Saved checkpoint to {save_path}\")\n",
    "    return save_path\n",
    "\n",
    "def train_grpo():\n",
    "    \"\"\"Main GRPO training loop\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Starting GRPO Training\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Setup\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
    "    metrics_table = MetricsTable()\n",
    "    train_prompts = [render_prompt(ex[\"question\"]) for ex in ds_train]\n",
    "    train_answers = [ex[\"answer\"] for ex in ds_train]\n",
    "    \n",
    "    logs = []\n",
    "    \n",
    "    for step in tqdm(range(cfg.n_grpo_steps), desc=\"GRPO Steps\"):\n",
    "        # ========== 1. Sample prompts ==========\n",
    "        rng = np.random.default_rng(SEED + step)\n",
    "        indices = rng.choice(len(train_prompts), size=cfg.prompts_per_step, replace=False)\n",
    "        prompts_batch = [train_prompts[i] for i in indices]\n",
    "        answers_batch = [train_answers[i] for i in indices]\n",
    "        \n",
    "        # Repeat each prompt K times for K rollouts\n",
    "        prompts_repeated = sum([[p] * cfg.rollouts_per_prompt for p in prompts_batch], [])\n",
    "        answers_repeated = sum([[a] * cfg.rollouts_per_prompt for a in answers_batch], [])\n",
    "        \n",
    "        # ========== 2. Generate rollouts (no grad) ==========\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(prompts_repeated, padding=True, truncation=True, max_length=2048, return_tensors=\"pt\").to(model.device)\n",
    "            prompt_lengths = (inputs.input_ids != tokenizer.pad_token_id).sum(dim=1).tolist()\n",
    "            \n",
    "            # Generate with sampling\n",
    "            gen_outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=cfg.max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=cfg.temperature,\n",
    "                top_p=cfg.top_p,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=False\n",
    "            )\n",
    "            \n",
    "            full_sequences = gen_outputs.sequences  # [batch, seq_len]\n",
    "            generated_texts = tokenizer.batch_decode(full_sequences, skip_special_tokens=True)\n",
    "            \n",
    "            # ========== 3. Compute rewards ==========\n",
    "            rewards = torch.tensor(\n",
    "                [compute_reward(text, ans) for text, ans in zip(generated_texts, answers_repeated)],\n",
    "                dtype=torch.float32,\n",
    "                device=model.device\n",
    "            )  # [batch]\n",
    "            \n",
    "            # Reshape to [num_prompts, rollouts_per_prompt]\n",
    "            rewards_grouped = rewards.view(cfg.prompts_per_step, cfg.rollouts_per_prompt)\n",
    "            \n",
    "            # ========== 4. Compute advantages (group normalization) ==========\n",
    "            # Subtract mean within each group\n",
    "            group_means = rewards_grouped.mean(dim=1, keepdim=True)  # [num_prompts, 1]\n",
    "            advantages = rewards_grouped - group_means  # [num_prompts, rollouts_per_prompt]\n",
    "            advantages = advantages.view(-1)  # Flatten back to [batch]\n",
    "            \n",
    "            # Store old log probs for KL estimation\n",
    "            response_mask = create_response_mask(full_sequences, prompt_lengths, tokenizer.eos_token_id)\n",
    "            old_logprobs = compute_policy_logprobs(model, full_sequences, response_mask)\n",
    "        \n",
    "        # ========== 5. Policy gradient update ==========\n",
    "        model.train()\n",
    "        model.config.use_cache = False\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Train for multiple epochs with gradient accumulation\n",
    "        for epoch in range(cfg.epochs_per_step):\n",
    "            # Shuffle data\n",
    "            perm = torch.randperm(full_sequences.size(0))\n",
    "            \n",
    "            for i in range(0, full_sequences.size(0), cfg.micro_batch_size):\n",
    "                indices = perm[i:i + cfg.micro_batch_size]\n",
    "                \n",
    "                # Micro-batch\n",
    "                mb_sequences = full_sequences[indices]\n",
    "                mb_mask = response_mask[indices]\n",
    "                mb_advantages = advantages[indices]\n",
    "                \n",
    "                # Compute current policy log probs\n",
    "                curr_logprobs = compute_policy_logprobs(model, mb_sequences, mb_mask)\n",
    "                \n",
    "                # Policy gradient loss: -E[advantage * log_prob]\n",
    "                # Sum over tokens, then weight by advantage per sequence\n",
    "                sequence_logprobs = curr_logprobs.sum(dim=1)  # [micro_batch]\n",
    "                loss = -(mb_advantages * sequence_logprobs).mean()\n",
    "                \n",
    "                # Backprop with gradient accumulation\n",
    "                loss = loss / cfg.gradient_accumulation_steps\n",
    "                loss.backward()\n",
    "                \n",
    "                total_loss += loss.item() * cfg.gradient_accumulation_steps\n",
    "                num_batches += 1\n",
    "                \n",
    "                # Optimizer step every accumulation_steps\n",
    "                if (num_batches % cfg.gradient_accumulation_steps) == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "        \n",
    "        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "        \n",
    "        # ========== 6. Compute metrics ==========\n",
    "        with torch.no_grad():\n",
    "            # Approximate KL divergence\n",
    "            new_logprobs = compute_policy_logprobs(model, full_sequences, response_mask)\n",
    "            kl_div = ((old_logprobs - new_logprobs) * response_mask).sum() / response_mask.sum()\n",
    "            kl_div = kl_div.item()\n",
    "            \n",
    "            avg_reward = rewards.mean().item()\n",
    "        \n",
    "        # ========== 7. Logging & Evaluation ==========\n",
    "        log_entry = {\n",
    "            \"step\": step,\n",
    "            \"loss\": avg_loss,\n",
    "            \"avg_reward\": avg_reward,\n",
    "            \"kl_div\": kl_div,\n",
    "        }\n",
    "        \n",
    "        if (step % cfg.log_every == 0) or (step == cfg.n_grpo_steps - 1):\n",
    "            # Validation\n",
    "            val_acc = None\n",
    "            if (step % cfg.eval_every == 0) or (step == cfg.n_grpo_steps - 1):\n",
    "                val_acc = evaluate_accuracy(model, tokenizer, ds_val, num_samples=cfg.eval_samples)\n",
    "                log_entry[\"val_acc\"] = val_acc\n",
    "            \n",
    "            metrics_table.update(\n",
    "                step=step,\n",
    "                loss=avg_loss,\n",
    "                avg_reward=avg_reward,\n",
    "                kl_div=kl_div,\n",
    "                val_acc=val_acc\n",
    "            )\n",
    "        \n",
    "        logs.append(log_entry)\n",
    "        \n",
    "        # ========== 8. Save checkpoint ==========\n",
    "        if (step % cfg.save_every == 0 and step > 0) or (step == cfg.n_grpo_steps - 1):\n",
    "            save_lora_checkpoint(model, step, cfg.run_dir)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Save logs\n",
    "    pd.DataFrame(logs).to_csv(os.path.join(cfg.run_dir, \"training_logs.csv\"), index=False)\n",
    "    \n",
    "    print(\"\\n\u2713 Training complete!\")\n",
    "    return logs\n",
    "\n",
    "# Run training\n",
    "training_logs = train_grpo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running final test evaluation...\")\n",
    "final_test_acc = evaluate_accuracy(model, tokenizer, ds_test, num_samples=len(ds_test), temperature=0.0, batch_size=32)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Baseline Test Accuracy: {baseline_acc:.1%}\")\n",
    "print(f\"Final Test Accuracy:    {final_test_acc:.1%}\")\n",
    "print(f\"Improvement:            {(final_test_acc - baseline_acc):.1%}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save final results\n",
    "with open(os.path.join(cfg.run_dir, \"final_results.json\"), \"w\") as f:\n",
    "    json.dump({\n",
    "        \"baseline_accuracy\": baseline_acc,\n",
    "        \"final_test_accuracy\": final_test_acc,\n",
    "        \"improvement\": final_test_acc - baseline_acc\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nAll artifacts saved to: {cfg.run_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\ude80 Usage with Inference Engines (vLLM, sglang)\n",
    "\n",
    "After training, you can use your LoRA adapters with fast inference engines like **vLLM** or **sglang** for production deployment.\n",
    "\n",
    "### Option 1: vLLM\n",
    "\n",
    "vLLM supports dynamic LoRA adapter loading for efficient serving:\n",
    "\n",
    "```python\n",
    "# Install vLLM\n",
    "# pip install vllm\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "# Initialize vLLM with base model\n",
    "llm = LLM(\n",
    "    model=\"Qwen/Qwen3-0.6B-Base\",\n",
    "    enable_lora=True,\n",
    "    max_lora_rank=16,  # Must match your LoRA rank\n",
    "    tensor_parallel_size=1\n",
    ")\n",
    "\n",
    "# Path to your trained LoRA adapter\n",
    "lora_path = \"./lora_rl_runs/lora_rl_123456/checkpoint_step_100\"\n",
    "\n",
    "# Create LoRA request\n",
    "lora_request = LoRARequest(\n",
    "    lora_name=\"math_lora\",\n",
    "    lora_int_id=1,\n",
    "    lora_local_path=lora_path\n",
    ")\n",
    "\n",
    "# Sampling params\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.0,\n",
    "    max_tokens=256,\n",
    "    stop=[tokenizer.eos_token]\n",
    ")\n",
    "\n",
    "# Generate with LoRA\n",
    "prompts = [render_prompt(\"What is 25 * 17?\")]\n",
    "outputs = llm.generate(\n",
    "    prompts,\n",
    "    sampling_params,\n",
    "    lora_request=lora_request\n",
    ")\n",
    "\n",
    "for output in outputs:\n",
    "    print(output.outputs[0].text)\n",
    "```\n",
    "\n",
    "**Benefits of vLLM:**\n",
    "- PagedAttention for memory efficiency\n",
    "- Continuous batching for high throughput\n",
    "- Multiple LoRA adapters can be served simultaneously\n",
    "- Supports tensor parallelism for large models\n",
    "\n",
    "---\n",
    "\n",
    "### Option 2: sglang\n",
    "\n",
    "sglang (Structured Generation Language) provides structured generation with LoRA support:\n",
    "\n",
    "```python\n",
    "# Install sglang\n",
    "# pip install sglang[all]\n",
    "\n",
    "import sglang as sgl\n",
    "\n",
    "# Start sglang server with LoRA\n",
    "# In terminal:\n",
    "# python -m sglang.launch_server \\\n",
    "#   --model-path Qwen/Qwen3-0.6B-Base \\\n",
    "#   --lora-paths ./lora_rl_runs/lora_rl_123456/checkpoint_step_100 \\\n",
    "#   --port 30000\n",
    "\n",
    "# Client code\n",
    "import sglang as sgl\n",
    "\n",
    "@sgl.function\n",
    "def solve_math(s, question):\n",
    "    s += render_prompt(question)\n",
    "    s += sgl.gen(\"answer\", max_tokens=256, temperature=0.0)\n",
    "\n",
    "# Connect to server\n",
    "sgl.set_default_backend(sgl.RuntimeEndpoint(\"http://localhost:30000\"))\n",
    "\n",
    "# Generate\n",
    "state = solve_math.run(question=\"What is 25 * 17?\")\n",
    "print(state[\"answer\"])\n",
    "```\n",
    "\n",
    "**Benefits of sglang:**\n",
    "- Structured generation primitives (constrained decoding)\n",
    "- Easier to write complex prompting logic\n",
    "- RadixAttention for prefix caching\n",
    "- Good for agent-based applications\n",
    "\n",
    "---\n",
    "\n",
    "### Option 3: Merge LoRA weights into base model\n",
    "\n",
    "If you don't need dynamic adapter switching, merge the LoRA weights:\n",
    "\n",
    "```python\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen3-0.6B-Base\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load LoRA adapter\n",
    "lora_path = \"./lora_rl_runs/lora_rl_123456/checkpoint_step_100\"\n",
    "model = PeftModel.from_pretrained(base_model, lora_path)\n",
    "\n",
    "# Merge and unload\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Save merged model\n",
    "merged_model.save_pretrained(\"./merged_model\")\n",
    "tokenizer.save_pretrained(\"./merged_model\")\n",
    "\n",
    "# Now you can use this with any inference engine as a regular model\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Performance Comparison\n",
    "\n",
    "| Engine | Throughput | Memory | LoRA Support | Best For |\n",
    "|--------|-----------|---------|--------------|----------|\n",
    "| **vLLM** | \u2b50\u2b50\u2b50\u2b50\u2b50 | \u2b50\u2b50\u2b50\u2b50\u2b50 | Multi-adapter | High-throughput serving |\n",
    "| **sglang** | \u2b50\u2b50\u2b50\u2b50 | \u2b50\u2b50\u2b50\u2b50 | Single adapter | Structured generation, agents |\n",
    "| **Transformers** | \u2b50\u2b50 | \u2b50\u2b50 | Native PEFT | Development, experimentation |\n",
    "| **Merged** | \u2b50\u2b50\u2b50\u2b50\u2b50 | \u2b50\u2b50\u2b50 | N/A (merged) | Single model deployment |\n",
    "\n",
    "**Recommendation:** Use vLLM for production serving with multiple LoRA adapters, sglang for agent-based applications, or merge weights for simple deployments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd2c Advanced: Custom Reward Functions\n",
    "\n",
    "You can easily adapt this notebook for other tasks by changing the reward function:\n",
    "\n",
    "```python\n",
    "# Example 1: Length-based reward (encourage conciseness)\n",
    "def length_reward(text: str, target_length: int = 100) -> float:\n",
    "    length = len(text.split())\n",
    "    return 1.0 - abs(length - target_length) / target_length\n",
    "\n",
    "# Example 2: Sentiment reward (for RLHF-style training)\n",
    "from transformers import pipeline\n",
    "sentiment_classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "def sentiment_reward(text: str, target_sentiment: str = \"POSITIVE\") -> float:\n",
    "    result = sentiment_classifier(text)[0]\n",
    "    return 1.0 if result[\"label\"] == target_sentiment else 0.0\n",
    "\n",
    "# Example 3: Learned reward model\n",
    "class RewardModel(torch.nn.Module):\n",
    "    # Your reward model architecture\n",
    "    pass\n",
    "\n",
    "reward_model = RewardModel.from_pretrained(\"path/to/reward/model\")\n",
    "\n",
    "def learned_reward(text: str, prompt: str) -> float:\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(prompt + text, return_tensors=\"pt\").to(device)\n",
    "        reward = reward_model(**inputs).item()\n",
    "    return reward\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcda Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **LoRA for RL** \u2014 How to train LoRA adapters with reinforcement learning\n",
    "2. **GRPO Algorithm** \u2014 Group-based policy optimization with advantage normalization\n",
    "3. **Not freezing layers** \u2014 LoRA parameters are updated, not frozen, during RL training\n",
    "4. **Production deployment** \u2014 Using vLLM and sglang for efficient inference\n",
    "\n",
    "**Key takeaways:**\n",
    "- LoRA makes RL training memory-efficient (only ~1% of parameters are trainable)\n",
    "- GRPO normalizes rewards within groups for stable training\n",
    "- Response masking ensures we only train on generated tokens\n",
    "- Fast inference engines like vLLM enable production deployment\n",
    "\n",
    "**Next steps:**\n",
    "- Try different LoRA ranks and learning rates\n",
    "- Experiment with other reward functions\n",
    "- Deploy your trained adapter with vLLM or sglang\n",
    "- Combine with other techniques (PPO, DPO, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}