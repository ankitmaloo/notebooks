{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-Scale LoRA RL: Qwen3-8B on DeepMath-103K\n",
    "\n",
    "This notebook validates **LoRA's effectiveness in reasoning RL** at a larger scale:\n",
    "\n",
    "- **Model**: Qwen3-8B-Base (13x larger than 0.6B experiments)\n",
    "- **Dataset**: DeepMath-103K ‚Äî 103K challenging mathematical problems (levels 5-10)\n",
    "- **Sequence Length**: 8192 tokens (allows backtracking and reasoning)\n",
    "- **Training**: GRPO with LoRA adapters\n",
    "\n",
    "## Why DeepMath-103K?\n",
    "\n",
    "DeepMath-103K is significantly more challenging than MATH or GSM8K:\n",
    "- **Larger scale**: 103K problems vs. 7.5K in MATH\n",
    "- **Higher difficulty**: Primarily levels 5-9 (vs. levels 1-5 in MATH)\n",
    "- **Decontaminated**: Rigorous decontamination against numerous benchmarks\n",
    "- **Verifiable answers**: Rule-based RL reward computation\n",
    "- **Multiple solutions**: 3 R1-generated reasoning paths per problem\n",
    "\n",
    "## 8192 Token Limit\n",
    "\n",
    "We restrict samples to 8192 tokens for:\n",
    "- **Computational efficiency**: Faster experiments\n",
    "- **Backtracking & reasoning**: Enough space for chain-of-thought\n",
    "- **Performance trade-off**: Limits absolute performance vs. longer sequences\n",
    "\n",
    "---\n",
    "\n",
    "**Requirements**: H100/A100 80GB recommended (8B model + 8192 context + LoRA training)\n",
    "\n",
    "**Reference**: He et al., 2025 - DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup\n",
    "!nvidia-smi -L || true\n",
    "\n",
    "import os, sys, random, numpy as np, torch, json, time, platform\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Install dependencies\n",
    "try:\n",
    "    get_ipython().run_line_magic(\"uv\", \"pip -q install transformers==4.51.3 accelerate==1.4.0 peft==0.14.0 datasets==3.3.2 evaluate==0.4.3 trl==0.13.0 flash-attn --no-build-isolation sentencepiece protobuf tqdm matplotlib > /dev/null\")\n",
    "except Exception:\n",
    "    get_ipython().run_line_magic(\"pip\", \"-q install transformers==4.51.3 accelerate==1.4.0 peft==0.14.0 datasets==3.3.2 evaluate==0.4.3 trl==0.13.0 flash-attn --no-build-isolation sentencepiece protobuf tqdm matplotlib > /dev/null\")\n",
    "\n",
    "import transformers, datasets, peft, accelerate, matplotlib\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Accelerate:\", accelerate.__version__)\n",
    "print(\"PEFT:\", peft.__version__)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "assert DEVICE == \"cuda\", \"Please connect a GPU (H100/A100 80GB recommended).\"\n",
    "\n",
    "print(f\"\\n‚úì Running on {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration Parameters\n",
    "\n",
    "**Key differences from base experiment:**\n",
    "- Model: Qwen3-8B-Base (larger)\n",
    "- Dataset: DeepMath-103K (harder, more diverse)\n",
    "- Max length: 8192 tokens (vs. 2048)\n",
    "- Adjusted batch sizes for memory efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from dataclasses import dataclass\nfrom typing import Optional, List\nimport re\n\n@dataclass\nclass Config:\n    # ========== Model Settings ==========\n    model_id: str = \"Qwen/Qwen2.5-8B-Base\"  # 8B base model for large-scale experiments\n    \n    # ========== LoRA Settings ==========\n    # LoRA parameters are NOT frozen - actively trained with RL!\n    lora_r: int = 32                    # Higher rank for 8B model (more capacity)\n    lora_alpha: int = 64                # Scaling factor\n    lora_dropout: float = 0.05          # Dropout for LoRA layers\n    lora_target_modules: List[str] = None  # Will target all attention + MLP\n    \n    # ========== Sequence Length ==========\n    max_seq_length: int = 8192          # 8192 token limit for training/eval\n    max_new_tokens: int = 2048          # Max tokens to generate (leaves room for reasoning)\n    \n    # ========== RL Training Settings (GRPO) ==========\n    n_grpo_steps: int = 500             # More steps for larger dataset\n    prompts_per_step: int = 16          # Reduced for memory (8B model + long context)\n    rollouts_per_prompt: int = 4        # Group size (K samples per prompt)\n    \n    learning_rate: float = 5e-5         # Lower LR for larger model\n    weight_decay: float = 0.01          # Small weight decay\n    \n    micro_batch_size: int = 1           # Very small due to 8192 context length\n    gradient_accumulation_steps: int = 64  # Accumulate to effective batch size\n    epochs_per_step: int = 1            # Training epochs per GRPO step\n    max_grad_norm: float = 1.0          # Gradient clipping\n    \n    # ========== Generation Settings ==========\n    temperature: float = 0.7            # Sampling temperature for rollouts\n    top_p: float = 0.9                  # Nucleus sampling\n    eval_temperature: float = 0.0       # Greedy decoding for evaluation\n    \n    # ========== Dataset Settings (DeepMath-103K) ==========\n    dataset_id: str = \"zwhe99/DeepMath-103K\"\n    difficulty_filter: Optional[tuple] = None  # (min, max) difficulty, None = all\n    topic_filter: Optional[List[str]] = None   # Filter by topics, None = all\n    \n    prompt_template: str = (\n        \"Solve this mathematical problem step by step. \"\n        \"Show your reasoning and provide the final answer.\\n\\n\"\n        \"Problem: {question}\\n\\n\"\n        \"Solution:\"\n    )\n    \n    # ========== Memory Optimization ==========\n    use_flash_attention: bool = True    # FlashAttention-2 for efficiency\n    use_gradient_checkpointing: bool = True  # Save memory during training\n    bf16: bool = True                   # BFloat16 training\n    \n    # ========== Monitoring & Logging ==========\n    log_every: int = 10                 # Log metrics every N steps\n    eval_every: int = 25                # Evaluate every N steps\n    eval_samples: int = 200             # Validation samples\n    save_every: int = 50                # Save checkpoint every N steps\n    \n    # ========== Data Splits ==========\n    train_samples: Optional[int] = None  # None = use all (103K)\n    val_samples: int = 500              # Validation set size\n    test_samples: int = 1000            # Test set size\n    \n    # ========== Output ==========\n    run_name: str = f\"lora_8b_deepmath_{int(time.time())}\"\n    output_dir: str = \"./lora_8b_runs\"\n    \n    def __post_init__(self):\n        if self.lora_target_modules is None:\n            self.lora_target_modules = [\n                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                \"gate_proj\", \"up_proj\", \"down_proj\"\n            ]\n        self.run_dir = os.path.join(self.output_dir, self.run_name)\n        os.makedirs(self.run_dir, exist_ok=True)\n\ncfg = Config()\n\nprint(\"=\"*70)\nprint(\"Large-Scale LoRA RL Configuration\")\nprint(\"=\"*70)\nprint(f\"Model:           {cfg.model_id}\")\nprint(f\"Dataset:         {cfg.dataset_id}\")\nprint(f\"Max seq length:  {cfg.max_seq_length} tokens\")\nprint(f\"Max new tokens:  {cfg.max_new_tokens} tokens\")\nprint(f\"LoRA rank:       {cfg.lora_r}, alpha: {cfg.lora_alpha}\")\nprint(f\"GRPO steps:      {cfg.n_grpo_steps}\")\nprint(f\"Batch config:    {cfg.prompts_per_step} prompts √ó {cfg.rollouts_per_prompt} rollouts\")\nprint(f\"Effective batch: {cfg.micro_batch_size} √ó {cfg.gradient_accumulation_steps} = {cfg.micro_batch_size * cfg.gradient_accumulation_steps}\")\nprint(f\"Learning rate:   {cfg.learning_rate}\")\nprint(f\"Flash Attention: {cfg.use_flash_attention}\")\nprint(f\"Gradient ckpt:   {cfg.use_gradient_checkpointing}\")\nprint(f\"Output:          {cfg.run_dir}\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Load DeepMath-103K Dataset\n",
    "\n",
    "DeepMath-103K contains:\n",
    "- `question`: Mathematical problem\n",
    "- `final_answer`: Verifiable answer (for RL reward)\n",
    "- `difficulty`: Float score (enables curriculum learning)\n",
    "- `topic`: Hierarchical classification\n",
    "- `r1_solution_1/2/3`: Three reasoning paths from DeepSeek-R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def render_prompt(question: str) -> str:\n",
    "    return cfg.prompt_template.format(question=question)\n",
    "\n",
    "def normalize_answer(answer: str) -> str:\n",
    "    \"\"\"Normalize mathematical answer for comparison\"\"\"\n",
    "    if answer is None:\n",
    "        return \"\"\n",
    "    # Remove whitespace, convert to lowercase\n",
    "    answer = answer.strip().lower()\n",
    "    # Remove common LaTeX delimiters\n",
    "    answer = answer.replace(\"$\", \"\").replace(\"\\\\boxed{\", \"\").replace(\"}\", \"\")\n",
    "    answer = answer.replace(\"\\\\text{\", \"\").replace(\"\\\\(\", \"\").replace(\"\\\\)\", \"\")\n",
    "    return answer.strip()\n",
    "\n",
    "def compute_reward(generated_text: str, gold_answer: str) -> float:\n",
    "    \"\"\"\n",
    "    Binary reward based on final answer match.\n",
    "    Looks for common answer patterns: boxed, brackets, or last mathematical expression.\n",
    "    \"\"\"\n",
    "    # Try to extract answer from generated text\n",
    "    # Pattern 1: \\boxed{answer}\n",
    "    boxed_match = re.search(r\"\\\\boxed\\{([^}]+)\\}\", generated_text)\n",
    "    if boxed_match:\n",
    "        pred_answer = boxed_match.group(1)\n",
    "    else:\n",
    "        # Pattern 2: [answer] or (answer)\n",
    "        bracket_match = re.search(r\"\\[([^\\]]+)\\]|\\(([^\\)]+)\\)\", generated_text)\n",
    "        if bracket_match:\n",
    "            pred_answer = bracket_match.group(1) or bracket_match.group(2)\n",
    "        else:\n",
    "            # Pattern 3: \"Final answer: ...\" or \"Answer: ...\"\n",
    "            answer_match = re.search(r\"(?:final\\s+)?answer\\s*:?\\s*(.+?)(?:\\.|$)\", generated_text.lower())\n",
    "            if answer_match:\n",
    "                pred_answer = answer_match.group(1)\n",
    "            else:\n",
    "                # Fallback: last line\n",
    "                lines = [l.strip() for l in generated_text.split(\"\\n\") if l.strip()]\n",
    "                pred_answer = lines[-1] if lines else \"\"\n",
    "    \n",
    "    # Normalize both answers\n",
    "    pred_norm = normalize_answer(pred_answer)\n",
    "    gold_norm = normalize_answer(gold_answer)\n",
    "    \n",
    "    # Exact match\n",
    "    if pred_norm == gold_norm:\n",
    "        return 1.0\n",
    "    \n",
    "    # Partial match (contained)\n",
    "    if pred_norm and gold_norm and (pred_norm in gold_norm or gold_norm in pred_norm):\n",
    "        return 0.5\n",
    "    \n",
    "    return 0.0\n",
    "\n",
    "print(\"Loading DeepMath-103K...\")\n",
    "ds_full = load_dataset(cfg.dataset_id, split=\"train\")\n",
    "\n",
    "# Filter by difficulty if specified\n",
    "if cfg.difficulty_filter is not None:\n",
    "    min_diff, max_diff = cfg.difficulty_filter\n",
    "    ds_full = ds_full.filter(lambda x: min_diff <= x[\"difficulty\"] <= max_diff)\n",
    "    print(f\"Filtered to difficulty {min_diff}-{max_diff}: {len(ds_full)} samples\")\n",
    "\n",
    "# Filter by topic if specified\n",
    "if cfg.topic_filter is not None:\n",
    "    ds_full = ds_full.filter(lambda x: any(t in x[\"topic\"] for t in cfg.topic_filter))\n",
    "    print(f\"Filtered to topics {cfg.topic_filter}: {len(ds_full)} samples\")\n",
    "\n",
    "# Filter by length (8192 token limit)\n",
    "from transformers import AutoTokenizer\n",
    "print(\"Loading tokenizer for length filtering...\")\n",
    "temp_tokenizer = AutoTokenizer.from_pretrained(cfg.model_id, trust_remote_code=True)\n",
    "\n",
    "def is_valid_length(example):\n",
    "    \"\"\"Check if example fits within 8192 token budget\"\"\"\n",
    "    prompt = render_prompt(example[\"question\"])\n",
    "    # Account for prompt + max generation + buffer\n",
    "    prompt_len = len(temp_tokenizer.encode(prompt))\n",
    "    return prompt_len + cfg.max_new_tokens < cfg.max_seq_length\n",
    "\n",
    "print(\"Filtering by 8192 token limit (this may take a few minutes)...\")\n",
    "ds_full = ds_full.filter(is_valid_length, desc=\"Length filter\")\n",
    "print(f\"After 8192 token filtering: {len(ds_full)} samples\")\n",
    "\n",
    "# Create splits\n",
    "total = len(ds_full)\n",
    "test_size = min(cfg.test_samples, total // 10)\n",
    "val_size = min(cfg.val_samples, total // 20)\n",
    "\n",
    "# Shuffle and split\n",
    "ds_full = ds_full.shuffle(seed=SEED)\n",
    "ds_test = ds_full.select(range(test_size))\n",
    "ds_val = ds_full.select(range(test_size, test_size + val_size))\n",
    "\n",
    "if cfg.train_samples is not None:\n",
    "    train_end = min(test_size + val_size + cfg.train_samples, total)\n",
    "else:\n",
    "    train_end = total\n",
    "ds_train = ds_full.select(range(test_size + val_size, train_end))\n",
    "\n",
    "print(f\"\\nDataset splits:\")\n",
    "print(f\"  Train: {len(ds_train):,}\")\n",
    "print(f\"  Val:   {len(ds_val):,}\")\n",
    "print(f\"  Test:  {len(ds_test):,}\")\n",
    "\n",
    "# Show example\n",
    "ex = ds_train[0]\n",
    "print(f\"\\nExample problem:\")\n",
    "print(f\"  Topic:      {ex['topic']}\")\n",
    "print(f\"  Difficulty: {ex['difficulty']}\")\n",
    "print(f\"  Question:   {ex['question'][:200]}...\")\n",
    "print(f\"  Answer:     {ex['final_answer']}\")\n",
    "\n",
    "# Show difficulty distribution\n",
    "import matplotlib.pyplot as plt\n",
    "difficulties = [ex[\"difficulty\"] for ex in ds_train.select(range(min(1000, len(ds_train))))]\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(difficulties, bins=20, edgecolor='black')\n",
    "plt.xlabel(\"Difficulty\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"DeepMath-103K Difficulty Distribution (Train Set Sample)\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "del temp_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Load Qwen3-8B with LoRA\n",
    "\n",
    "We use:\n",
    "- **Flash Attention 2**: For memory-efficient long context\n",
    "- **Gradient checkpointing**: Reduce memory during backprop\n",
    "- **BFloat16**: Stable training with less memory\n",
    "- **Higher LoRA rank**: More capacity for 8B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "def load_model_with_lora(model_id: str, lora_config: LoraConfig):\n",
    "    \"\"\"Load 8B model with LoRA and memory optimizations\"\"\"\n",
    "    print(f\"Loading base model: {model_id}...\")\n",
    "    \n",
    "    model_kwargs = {\n",
    "        \"torch_dtype\": torch.bfloat16 if cfg.bf16 else torch.float16,\n",
    "        \"device_map\": \"auto\",\n",
    "        \"trust_remote_code\": True,\n",
    "    }\n",
    "    \n",
    "    # Add Flash Attention if available\n",
    "    if cfg.use_flash_attention:\n",
    "        try:\n",
    "            model_kwargs[\"attn_implementation\"] = \"flash_attention_2\"\n",
    "            print(\"  Using Flash Attention 2\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Flash Attention not available: {e}\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
    "    model.config.use_cache = False  # Disable for training\n",
    "    \n",
    "    # Enable gradient checkpointing\n",
    "    if cfg.use_gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "        print(\"  Gradient checkpointing enabled\")\n",
    "    \n",
    "    print(\"Applying LoRA adapters...\")\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Print trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\nParameter summary:\")\n",
    "    print(f\"  Trainable: {trainable_params:,} ({100 * trainable_params / total_params:.3f}%)\")\n",
    "    print(f\"  Total:     {total_params:,}\")\n",
    "    print(f\"  Frozen:    {total_params - trainable_params:,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    cfg.model_id,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,\n",
    "    model_max_length=cfg.max_seq_length\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
    "print(f\"Max sequence length: {cfg.max_seq_length}\")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=cfg.lora_r,\n",
    "    lora_alpha=cfg.lora_alpha,\n",
    "    lora_dropout=cfg.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=cfg.lora_target_modules,\n",
    ")\n",
    "\n",
    "print(f\"\\nLoRA configuration:\")\n",
    "print(f\"  Rank: {cfg.lora_r}\")\n",
    "print(f\"  Alpha: {cfg.lora_alpha}\")\n",
    "print(f\"  Dropout: {cfg.lora_dropout}\")\n",
    "print(f\"  Target modules: {cfg.lora_target_modules}\")\n",
    "\n",
    "# Load model\n",
    "model = load_model_with_lora(cfg.model_id, lora_config)\n",
    "\n",
    "print(\"\\n‚úì Model ready for large-scale RL training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Baseline Evaluation\n",
    "\n",
    "Measure baseline performance on challenging DeepMath-103K problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_accuracy(model, tokenizer, dataset, num_samples: int = 100, temperature: float = 0.0, batch_size: int = 4):\n",
    "    \"\"\"Evaluate with 8192 token context\"\"\"\n",
    "    model.eval()\n",
    "    was_cache = model.config.use_cache\n",
    "    model.config.use_cache = True\n",
    "    \n",
    "    n = min(num_samples, len(dataset))\n",
    "    total_reward = 0.0\n",
    "    exact_matches = 0\n",
    "    \n",
    "    for i in tqdm(range(0, n, batch_size), desc=\"Evaluating\"):\n",
    "        batch = dataset.select(range(i, min(i + batch_size, n)))\n",
    "        prompts = [render_prompt(ex[\"question\"]) for ex in batch]\n",
    "        \n",
    "        # Tokenize with 8192 limit\n",
    "        inputs = tokenizer(\n",
    "            prompts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=cfg.max_seq_length - cfg.max_new_tokens,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        \n",
    "        # Generate\n",
    "        gen_kwargs = dict(\n",
    "            max_new_tokens=cfg.max_new_tokens,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True\n",
    "        )\n",
    "        if temperature > 0.0:\n",
    "            gen_kwargs.update(do_sample=True, temperature=temperature, top_p=cfg.top_p)\n",
    "        else:\n",
    "            gen_kwargs.update(do_sample=False)\n",
    "        \n",
    "        outputs = model.generate(**inputs, **gen_kwargs)\n",
    "        texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        \n",
    "        # Compute rewards\n",
    "        for ex, text in zip(batch, texts):\n",
    "            reward = compute_reward(text, ex[\"final_answer\"])\n",
    "            total_reward += reward\n",
    "            if reward >= 0.99:\n",
    "                exact_matches += 1\n",
    "    \n",
    "    model.config.use_cache = was_cache\n",
    "    avg_reward = total_reward / n\n",
    "    accuracy = exact_matches / n\n",
    "    \n",
    "    return {\"avg_reward\": avg_reward, \"accuracy\": accuracy}\n",
    "\n",
    "print(\"Computing baseline metrics on test set...\")\n",
    "print(\"(This may take a while due to 8192 token context)\\n\")\n",
    "baseline_metrics = evaluate_accuracy(\n",
    "    model, tokenizer, ds_test,\n",
    "    num_samples=min(100, len(ds_test)),\n",
    "    temperature=0.0,\n",
    "    batch_size=2  # Small batch for long context\n",
    ")\n",
    "\n",
    "print(f\"\\nBaseline Results (DeepMath-103K):\")\n",
    "print(f\"  Average Reward: {baseline_metrics['avg_reward']:.3f}\")\n",
    "print(f\"  Exact Match:    {baseline_metrics['accuracy']:.1%}\")\n",
    "\n",
    "# Save baseline\n",
    "with open(os.path.join(cfg.run_dir, \"baseline.json\"), \"w\") as f:\n",
    "    json.dump(baseline_metrics, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ GRPO Training Loop (8192 Context)\n",
    "\n",
    "Key adaptations for large-scale:\n",
    "- **8192 token sequences**: Allows complex reasoning chains\n",
    "- **Micro-batching**: Process 1 example at a time, accumulate gradients\n",
    "- **Memory efficient**: Gradient checkpointing + flash attention\n",
    "- **Harder problems**: DeepMath-103K difficulty 5-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "def create_response_mask(input_ids: torch.Tensor, prompt_lengths: List[int], eos_token_id: int) -> torch.Tensor:\n",
    "    \"\"\"Mask: 1 for generated tokens, 0 for prompt/padding/post-EOS\"\"\"\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    mask = torch.zeros_like(input_ids, dtype=torch.float32)\n",
    "    \n",
    "    for i, prompt_len in enumerate(prompt_lengths):\n",
    "        # Find first EOS in generated part\n",
    "        generated_ids = input_ids[i, prompt_len:]\n",
    "        eos_positions = (generated_ids == eos_token_id).nonzero(as_tuple=True)[0]\n",
    "        \n",
    "        if len(eos_positions) > 0:\n",
    "            first_eos = eos_positions[0].item()\n",
    "            mask[i, prompt_len:prompt_len + first_eos] = 1.0\n",
    "        else:\n",
    "            mask[i, prompt_len:] = 1.0\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def compute_policy_logprobs_chunked(model, input_ids: torch.Tensor, response_mask: torch.Tensor, chunk_size: int = 1) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute log probs in chunks to handle long sequences.\n",
    "    Returns: [batch_size, seq_len] log probs\n",
    "    \"\"\"\n",
    "    batch_size = input_ids.size(0)\n",
    "    seq_len = input_ids.size(1)\n",
    "    all_logprobs = []\n",
    "    \n",
    "    for i in range(0, batch_size, chunk_size):\n",
    "        chunk_ids = input_ids[i:i+chunk_size]\n",
    "        chunk_mask = response_mask[i:i+chunk_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=chunk_ids[:, :-1],\n",
    "            attention_mask=(chunk_ids[:, :-1] != tokenizer.pad_token_id)\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Get log probs for next tokens\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        next_tokens = chunk_ids[:, 1:].unsqueeze(-1)\n",
    "        token_logprobs = log_probs.gather(-1, next_tokens).squeeze(-1)\n",
    "        \n",
    "        # Pad and apply mask\n",
    "        token_logprobs = F.pad(token_logprobs, (1, 0), value=0.0)\n",
    "        token_logprobs = token_logprobs * chunk_mask\n",
    "        \n",
    "        all_logprobs.append(token_logprobs)\n",
    "    \n",
    "    return torch.cat(all_logprobs, dim=0)\n",
    "\n",
    "class MetricsTable:\n",
    "    def __init__(self):\n",
    "        self.rows = []\n",
    "        self.df = pd.DataFrame(columns=[\"step\", \"loss\", \"avg_reward\", \"accuracy\", \"kl_div\", \"val_reward\", \"val_acc\"])\n",
    "        self.handle = display(self.df, display_id=True)\n",
    "    \n",
    "    def update(self, step, loss=None, avg_reward=None, accuracy=None, kl_div=None, val_reward=None, val_acc=None):\n",
    "        row = {\"step\": step}\n",
    "        if loss is not None:\n",
    "            row[\"loss\"] = f\"{loss:.4f}\"\n",
    "        if avg_reward is not None:\n",
    "            row[\"avg_reward\"] = f\"{avg_reward:.3f}\"\n",
    "        if accuracy is not None:\n",
    "            row[\"accuracy\"] = f\"{accuracy:.1%}\"\n",
    "        if kl_div is not None:\n",
    "            row[\"kl_div\"] = f\"{kl_div:.4f}\"\n",
    "        if val_reward is not None:\n",
    "            row[\"val_reward\"] = f\"{val_reward:.3f}\"\n",
    "        if val_acc is not None:\n",
    "            row[\"val_acc\"] = f\"{val_acc:.1%}\"\n",
    "        \n",
    "        self.rows.append(row)\n",
    "        self.df = pd.DataFrame(self.rows)\n",
    "        self.handle.update(self.df)\n",
    "\n",
    "def save_checkpoint(model, step: int, metrics: dict):\n",
    "    \"\"\"Save LoRA checkpoint with metadata\"\"\"\n",
    "    save_path = os.path.join(cfg.run_dir, f\"checkpoint_step_{step}\")\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    model.save_pretrained(save_path)\n",
    "    \n",
    "    # Save metrics\n",
    "    with open(os.path.join(save_path, \"metrics.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úì Saved checkpoint: {save_path}\")\n",
    "    return save_path\n",
    "\n",
    "def train_grpo_large_scale():\n",
    "    \"\"\"GRPO training for 8B model on DeepMath-103K with 8192 context\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Starting Large-Scale GRPO Training\")\n",
    "    print(\"Model: Qwen3-8B | Dataset: DeepMath-103K | Context: 8192 tokens\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Setup\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=cfg.learning_rate,\n",
    "        weight_decay=cfg.weight_decay\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler (cosine decay)\n",
    "    from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=cfg.n_grpo_steps, eta_min=cfg.learning_rate * 0.1)\n",
    "    \n",
    "    metrics_table = MetricsTable()\n",
    "    logs = []\n",
    "    \n",
    "    for step in tqdm(range(cfg.n_grpo_steps), desc=\"GRPO Steps\"):\n",
    "        # ========== 1. Sample prompts ==========\n",
    "        rng = np.random.default_rng(SEED + step)\n",
    "        indices = rng.choice(len(ds_train), size=cfg.prompts_per_step, replace=False)\n",
    "        batch_examples = [ds_train[int(i)] for i in indices]\n",
    "        \n",
    "        # Repeat for K rollouts\n",
    "        prompts_repeated = sum([[render_prompt(ex[\"question\"])] * cfg.rollouts_per_prompt for ex in batch_examples], [])\n",
    "        answers_repeated = sum([[ex[\"final_answer\"]] * cfg.rollouts_per_prompt for ex in batch_examples], [])\n",
    "        \n",
    "        # ========== 2. Generate rollouts ==========\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(\n",
    "                prompts_repeated,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=cfg.max_seq_length - cfg.max_new_tokens,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(model.device)\n",
    "            \n",
    "            prompt_lengths = (inputs.input_ids != tokenizer.pad_token_id).sum(dim=1).tolist()\n",
    "            \n",
    "            gen_outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=cfg.max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=cfg.temperature,\n",
    "                top_p=cfg.top_p,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                return_dict_in_generate=True\n",
    "            )\n",
    "            \n",
    "            full_sequences = gen_outputs.sequences\n",
    "            generated_texts = tokenizer.batch_decode(full_sequences, skip_special_tokens=True)\n",
    "            \n",
    "            # ========== 3. Compute rewards ==========\n",
    "            rewards = torch.tensor(\n",
    "                [compute_reward(text, ans) for text, ans in zip(generated_texts, answers_repeated)],\n",
    "                dtype=torch.float32,\n",
    "                device=model.device\n",
    "            )\n",
    "            \n",
    "            # Group normalization\n",
    "            rewards_grouped = rewards.view(cfg.prompts_per_step, cfg.rollouts_per_prompt)\n",
    "            group_means = rewards_grouped.mean(dim=1, keepdim=True)\n",
    "            advantages = (rewards_grouped - group_means).view(-1)\n",
    "            \n",
    "            # Store old log probs\n",
    "            response_mask = create_response_mask(full_sequences, prompt_lengths, tokenizer.eos_token_id)\n",
    "            old_logprobs = compute_policy_logprobs_chunked(\n",
    "                model, full_sequences, response_mask, chunk_size=cfg.micro_batch_size\n",
    "            )\n",
    "        \n",
    "        # ========== 4. Policy update ==========\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for epoch in range(cfg.epochs_per_step):\n",
    "            perm = torch.randperm(full_sequences.size(0))\n",
    "            \n",
    "            for i in range(0, full_sequences.size(0), cfg.micro_batch_size):\n",
    "                indices = perm[i:i + cfg.micro_batch_size]\n",
    "                \n",
    "                mb_sequences = full_sequences[indices]\n",
    "                mb_mask = response_mask[indices]\n",
    "                mb_advantages = advantages[indices]\n",
    "                \n",
    "                # Current policy log probs\n",
    "                curr_logprobs = compute_policy_logprobs_chunked(\n",
    "                    model, mb_sequences, mb_mask, chunk_size=cfg.micro_batch_size\n",
    "                )\n",
    "                \n",
    "                # Policy gradient loss\n",
    "                sequence_logprobs = curr_logprobs.sum(dim=1)\n",
    "                loss = -(mb_advantages * sequence_logprobs).mean()\n",
    "                loss = loss / cfg.gradient_accumulation_steps\n",
    "                \n",
    "                loss.backward()\n",
    "                total_loss += loss.item() * cfg.gradient_accumulation_steps\n",
    "                num_batches += 1\n",
    "                \n",
    "                # Optimizer step\n",
    "                if (num_batches % cfg.gradient_accumulation_steps) == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "        \n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "        \n",
    "        # ========== 5. Metrics ==========\n",
    "        with torch.no_grad():\n",
    "            new_logprobs = compute_policy_logprobs_chunked(\n",
    "                model, full_sequences, response_mask, chunk_size=cfg.micro_batch_size\n",
    "            )\n",
    "            kl_div = ((old_logprobs - new_logprobs) * response_mask).sum() / response_mask.sum()\n",
    "            \n",
    "            avg_reward = rewards.mean().item()\n",
    "            accuracy = (rewards >= 0.99).float().mean().item()\n",
    "        \n",
    "        log_entry = {\n",
    "            \"step\": step,\n",
    "            \"loss\": avg_loss,\n",
    "            \"avg_reward\": avg_reward,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"kl_div\": kl_div.item(),\n",
    "            \"lr\": scheduler.get_last_lr()[0]\n",
    "        }\n",
    "        \n",
    "        # ========== 6. Evaluation ==========\n",
    "        if (step % cfg.eval_every == 0 and step > 0) or (step == cfg.n_grpo_steps - 1):\n",
    "            print(f\"\\nEvaluating at step {step}...\")\n",
    "            val_metrics = evaluate_accuracy(\n",
    "                model, tokenizer, ds_val,\n",
    "                num_samples=cfg.eval_samples,\n",
    "                temperature=0.0,\n",
    "                batch_size=2\n",
    "            )\n",
    "            log_entry[\"val_reward\"] = val_metrics[\"avg_reward\"]\n",
    "            log_entry[\"val_acc\"] = val_metrics[\"accuracy\"]\n",
    "            print(f\"Val Reward: {val_metrics['avg_reward']:.3f}, Val Acc: {val_metrics['accuracy']:.1%}\")\n",
    "        \n",
    "        # ========== 7. Logging ==========\n",
    "        if (step % cfg.log_every == 0) or (\"val_reward\" in log_entry):\n",
    "            metrics_table.update(\n",
    "                step=step,\n",
    "                loss=avg_loss,\n",
    "                avg_reward=avg_reward,\n",
    "                accuracy=accuracy,\n",
    "                kl_div=kl_div.item(),\n",
    "                val_reward=log_entry.get(\"val_reward\"),\n",
    "                val_acc=log_entry.get(\"val_acc\")\n",
    "            )\n",
    "        \n",
    "        logs.append(log_entry)\n",
    "        \n",
    "        # ========== 8. Checkpointing ==========\n",
    "        if (step % cfg.save_every == 0 and step > 0) or (step == cfg.n_grpo_steps - 1):\n",
    "            save_checkpoint(model, step, log_entry)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Save logs\n",
    "    pd.DataFrame(logs).to_csv(os.path.join(cfg.run_dir, \"training_logs.csv\"), index=False)\n",
    "    print(\"\\n‚úì Training complete!\")\n",
    "    return logs\n",
    "\n",
    "# Run training\n",
    "training_logs = train_grpo_large_scale()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Final Evaluation & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running final test evaluation...\")\n",
    "final_metrics = evaluate_accuracy(\n",
    "    model, tokenizer, ds_test,\n",
    "    num_samples=len(ds_test),\n",
    "    temperature=0.0,\n",
    "    batch_size=2\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS: Large-Scale LoRA RL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model:    Qwen3-8B-Base\")\n",
    "print(f\"Dataset:  DeepMath-103K (103K problems, difficulty 5-10)\")\n",
    "print(f\"Context:  8192 tokens\")\n",
    "print(f\"Training: {cfg.n_grpo_steps} GRPO steps\")\n",
    "print(\"\\nBaseline (before RL):\")\n",
    "print(f\"  Avg Reward:  {baseline_metrics['avg_reward']:.3f}\")\n",
    "print(f\"  Accuracy:    {baseline_metrics['accuracy']:.1%}\")\n",
    "print(\"\\nFinal (after RL):\")\n",
    "print(f\"  Avg Reward:  {final_metrics['avg_reward']:.3f}\")\n",
    "print(f\"  Accuracy:    {final_metrics['accuracy']:.1%}\")\n",
    "print(\"\\nImprovement:\")\n",
    "print(f\"  Œî Reward:    {(final_metrics['avg_reward'] - baseline_metrics['avg_reward']):.3f}\")\n",
    "print(f\"  Œî Accuracy:  {(final_metrics['accuracy'] - baseline_metrics['accuracy']):.1%}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save final results\n",
    "results = {\n",
    "    \"config\": {\n",
    "        \"model\": cfg.model_id,\n",
    "        \"dataset\": cfg.dataset_id,\n",
    "        \"max_seq_length\": cfg.max_seq_length,\n",
    "        \"lora_r\": cfg.lora_r,\n",
    "        \"n_grpo_steps\": cfg.n_grpo_steps,\n",
    "    },\n",
    "    \"baseline\": baseline_metrics,\n",
    "    \"final\": final_metrics,\n",
    "    \"improvement\": {\n",
    "        \"avg_reward\": final_metrics[\"avg_reward\"] - baseline_metrics[\"avg_reward\"],\n",
    "        \"accuracy\": final_metrics[\"accuracy\"] - baseline_metrics[\"accuracy\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(cfg.run_dir, \"final_results.json\"), \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úì All artifacts saved to: {cfg.run_dir}\")\n",
    "\n",
    "# Plot training curves\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.DataFrame(training_logs)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(df[\"step\"], df[\"loss\"])\n",
    "axes[0, 0].set_xlabel(\"Step\")\n",
    "axes[0, 0].set_ylabel(\"Loss\")\n",
    "axes[0, 0].set_title(\"Training Loss\")\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Reward\n",
    "axes[0, 1].plot(df[\"step\"], df[\"avg_reward\"], label=\"Train\")\n",
    "if \"val_reward\" in df.columns:\n",
    "    val_df = df.dropna(subset=[\"val_reward\"])\n",
    "    axes[0, 1].plot(val_df[\"step\"], val_df[\"val_reward\"], label=\"Val\", marker=\"o\")\n",
    "axes[0, 1].set_xlabel(\"Step\")\n",
    "axes[0, 1].set_ylabel(\"Avg Reward\")\n",
    "axes[0, 1].set_title(\"Average Reward\")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1, 0].plot(df[\"step\"], df[\"accuracy\"], label=\"Train\")\n",
    "if \"val_acc\" in df.columns:\n",
    "    val_df = df.dropna(subset=[\"val_acc\"])\n",
    "    axes[1, 0].plot(val_df[\"step\"], val_df[\"val_acc\"], label=\"Val\", marker=\"o\")\n",
    "axes[1, 0].set_xlabel(\"Step\")\n",
    "axes[1, 0].set_ylabel(\"Accuracy\")\n",
    "axes[1, 0].set_title(\"Exact Match Accuracy\")\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# KL Divergence\n",
    "axes[1, 1].plot(df[\"step\"], df[\"kl_div\"])\n",
    "axes[1, 1].set_xlabel(\"Step\")\n",
    "axes[1, 1].set_ylabel(\"KL Divergence\")\n",
    "axes[1, 1].set_title(\"KL Divergence (Policy Drift)\")\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(cfg.run_dir, \"training_curves.png\"), dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Training curves saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Deployment with vLLM/sglang\n",
    "\n",
    "Deploy your trained 8B LoRA adapter with inference engines:\n",
    "\n",
    "```python\n",
    "# vLLM deployment\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"Qwen/Qwen3-8B-Base\",\n",
    "    enable_lora=True,\n",
    "    max_lora_rank=32,  # Match your LoRA rank\n",
    "    max_model_len=8192,  # Support 8192 context\n",
    "    tensor_parallel_size=1\n",
    ")\n",
    "\n",
    "lora_path = \"./lora_8b_runs/lora_8b_deepmath_XXX/checkpoint_step_500\"\n",
    "lora_request = LoRARequest(\"deepmath_lora\", 1, lora_path)\n",
    "\n",
    "# Generate\n",
    "sampling_params = SamplingParams(temperature=0.0, max_tokens=2048)\n",
    "outputs = llm.generate(prompts, sampling_params, lora_request=lora_request)\n",
    "```\n",
    "\n",
    "**Performance benefits:**\n",
    "- PagedAttention for 8192 token context\n",
    "- Continuous batching for throughput\n",
    "- Multi-LoRA serving (different fine-tuned versions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Summary\n",
    "\n",
    "This notebook validates **LoRA's effectiveness for reasoning RL at scale**:\n",
    "\n",
    "### Key Findings:\n",
    "1. **Scalability**: LoRA enables RL training of 8B models with <1% trainable parameters\n",
    "2. **Long context**: 8192 token sequences allow complex reasoning chains\n",
    "3. **Hard problems**: DeepMath-103K difficulty 5-10 tests advanced mathematical reasoning\n",
    "4. **Memory efficiency**: Gradient checkpointing + Flash Attention fit on A100 80GB\n",
    "\n",
    "### Compared to smaller experiments:\n",
    "- **13x larger model** (0.6B ‚Üí 8B parameters)\n",
    "- **4x longer context** (2048 ‚Üí 8192 tokens)\n",
    "- **13x more data** (7.5K ‚Üí 103K problems)\n",
    "- **Harder dataset** (GSM8K level 1-3 ‚Üí DeepMath level 5-10)\n",
    "\n",
    "### Implementation highlights:\n",
    "- GRPO with group advantage normalization\n",
    "- Micro-batching for memory efficiency\n",
    "- Cosine LR schedule for stable training\n",
    "- Rule-based rewards from verifiable answers\n",
    "\n",
    "**Next steps:**\n",
    "- Try curriculum learning (start with easier problems)\n",
    "- Experiment with different LoRA ranks\n",
    "- Combine with other techniques (PPO, rejection sampling)\n",
    "- Test on other reasoning benchmarks (MATH, Minerva, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}