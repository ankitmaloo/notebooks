{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RL Training Boilerplate\n\nThis notebook provides a modular boilerplate for Reinforcement Learning training of language models using PyTorch directly (no verl).\n\n**Features:**\n- All hyperparameters at the top for easy configuration\n- Modular sections: model loading, tools, chat templates, environment, reward model\n- WandB integration for logging and tracking\n- Easy to swap models and modify components\n- Pure PyTorch implementation\n\n**Requirements:**\n- GPU with sufficient VRAM (A100 recommended)\n- WandB account for logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udee0\ufe0f Setup & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Install Dependencies\n!nvidia-smi -L || true\n\nimport sys\nprint(\"Python:\", sys.version)\n\n# Install required packages\ntry:\n    %pip install -q transformers>=4.51.3 accelerate>=1.4.0 peft>=0.14.0 \\\n                     datasets>=3.3.2 torch wandb huggingface_hub \\\n                     sentencepiece protobuf tqdm matplotlib pandas\nexcept Exception:\n    !pip install -q transformers>=4.51.3 accelerate>=1.4.0 peft>=0.14.0 \\\n                     datasets>=3.3.2 torch wandb huggingface_hub \\\n                     sentencepiece protobuf tqdm matplotlib pandas\n\nimport os, random, time, json, platform\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom tqdm.auto import tqdm\nimport pandas as pd\nfrom IPython.display import display\n\nprint(\"\\n=== Environment ===\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n    print(f\"CUDA version: {torch.version.cuda}\")\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nassert DEVICE == \"cuda\", \"Please connect a GPU for RL training.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd11 WandB & HuggingFace Authentication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Set API Keys (Optional - fill in if needed)\nimport os\n\n# WandB API key - get from https://wandb.ai/authorize\nWANDB_API_KEY = \"\"  # Your WandB API key\nif WANDB_API_KEY:\n    os.environ['WANDB_API_KEY'] = WANDB_API_KEY\n\n# HuggingFace token - get from https://huggingface.co/settings/tokens\nHF_TOKEN = \"\"  # Your HuggingFace token\nif HF_TOKEN:\n    os.environ['HF_TOKEN'] = HF_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Login to WandB and HuggingFace\nimport wandb\nfrom huggingface_hub import login\n\n# WandB login\ntry:\n    wandb.login()\n    print(\"\u2713 WandB login successful\")\nexcept Exception as e:\n    print(f\"\u26a0 WandB login failed: {e}\")\n    print(\"Training will continue without WandB logging\")\n\n# HuggingFace login\ntry:\n    if os.environ.get('HF_TOKEN'):\n        login(token=os.environ['HF_TOKEN'])\n        print(\"\u2713 HuggingFace login successful\")\nexcept Exception as e:\n    print(f\"\u26a0 HuggingFace login failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": "## \u2699\ufe0f HYPERPARAMETERS\n\n**All configurable parameters are in this section for easy modification.**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Hyperparameters Configuration\nfrom dataclasses import dataclass\nfrom typing import Optional, List\n\n@dataclass\nclass RLConfig:\n    # ==================== MODEL CONFIGURATION ====================\n    # Policy model (the model being trained)\n    policy_model_id: str = \"Qwen/Qwen3-0.6B\"  # Use Qwen3-0.6B-Base for base model\n    policy_model_dtype: str = \"bfloat16\"  # \"bfloat16\", \"float16\", or \"float32\"\n    \n    # Reference model (for KL penalty, optional)\n    use_reference_model: bool = False\n    reference_model_id: Optional[str] = None  # If None, uses policy_model_id\n    \n    # Reward model (optional)\n    use_reward_model: bool = False\n    reward_model_id: Optional[str] = None\n    \n    # ==================== LORA CONFIGURATION ====================\n    # NOTE: RL typically uses full-weight updates. LoRA is available but not recommended.\n    use_lora: bool = False  # Set to True only for parameter-efficient training\n    lora_r: int = 16\n    lora_alpha: int = 32\n    lora_dropout: float = 0.05\n    lora_target_modules: Optional[List[str]] = None  # Auto-detect if None\n    \n    # ==================== TRAINING CONFIGURATION ====================\n    num_steps: int = 100\n    num_prompts_per_step: int = 4  # Number of unique prompts per step\n    samples_per_prompt: int = 4  # Rollouts per prompt (effective batch = num_prompts * samples_per_prompt)\n    \n    learning_rate: float = 1e-4\n    weight_decay: float = 0.0\n    grad_accumulation_steps: int = 1\n    max_grad_norm: float = 1.0\n    \n    max_new_tokens: int = 512  # Qwen3 recommends 32768 for complex reasoning, use smaller for testing\n    train_temperature: float = 0.7  # For non-thinking mode training\n    eval_temperature: float = 0.7  # For non-thinking mode eval\n    top_p: float = 0.8  # Qwen3 recommendation for non-thinking\n    top_k: int = 20  # Qwen3 recommendation\n    \n    # ==================== RL ALGORITHM ====================\n    algorithm: str = \"pg\"  # Currently only \"pg\" (policy gradient) implemented\n    use_kl_penalty: bool = False\n    kl_coef: float = 0.1\n    \n    # Advantage normalization\n    normalize_advantages: bool = True\n    normalize_rewards: bool = False  # Set to True for reward normalization\n    \n    # ==================== QWEN3-SPECIFIC CONFIGURATION ====================\n    # Chat template\n    use_chat_template: bool = False  # Set to True for instruct models\n    system_prompt: Optional[str] = None\n    \n    # Thinking mode (Qwen3 feature)\n    enable_thinking: bool = False  # Enable thinking mode (<think>...</think> blocks)\n    thinking_train_temperature: float = 0.6  # Qwen3 recommendation for thinking mode\n    thinking_train_top_p: float = 0.95  # Qwen3 recommendation for thinking mode\n    thinking_eval_temperature: float = 0.6  # For thinking mode eval\n    thinking_eval_top_p: float = 0.95\n    parse_thinking: bool = False  # Parse and separate thinking content from response\n    \n    # Tool use (requires Qwen-Agent)\n    use_qwen_agent: bool = False  # Enable Qwen-Agent for tool calling\n    qwen_agent_tools: Optional[List[str]] = None  # List of tools (e.g., ['code_interpreter', 'time'])\n    \n    # ==================== DATA CONFIGURATION ====================\n    dataset_name: str = \"openai/gsm8k\"\n    dataset_config: Optional[str] = \"main\"\n    dataset_split: str = \"train\"\n    prompt_field: str = \"question\"\n    answer_field: Optional[str] = \"answer\"\n    \n    val_size: int = 200\n    val_every: int = 10\n    val_batch_size: int = 32\n    \n    # ==================== PROMPT TEMPLATE ====================\n    prompt_template: str = (\n        \"Solve step by step.\\n\"\n        \"Problem: {prompt}\\n\\nSolution:\"\n    )\n    \n    # ==================== REWARD CONFIGURATION ====================\n    reward_type: str = \"rule\"  # \"model\", \"rule\", or \"custom\"\n    use_ground_truth: bool = True  # Whether to pass ground truth to reward function\n    \n    # ==================== TOOLS & ENVIRONMENT ====================\n    use_tools: bool = False\n    use_environment: bool = False\n    \n    # ==================== LOGGING & CHECKPOINTING ====================\n    wandb_project: str = \"rl-training\"\n    wandb_run_name: Optional[str] = None\n    log_every: int = 10\n    save_every: int = 50\n    ema_momentum: float = 0.9\n    \n    output_dir: str = f\"./run_rl_{int(time.time())}\"\n    push_to_hub: bool = False\n    hub_repo_id: Optional[str] = None\n    \n    seed: int = 42\n    \n    def __post_init__(self):\n        # Auto-generate run name\n        if self.wandb_run_name is None:\n            model_short = self.policy_model_id.split(\"/\")[-1]\n            mode = \"thinking\" if self.enable_thinking else \"normal\"\n            self.wandb_run_name = f\"rl_{self.algorithm}_{model_short}_{mode}_{int(time.time())}\"\n        \n        # Set reference model to policy model if not specified\n        if self.use_reference_model and self.reference_model_id is None:\n            self.reference_model_id = self.policy_model_id\n        \n        # Auto-detect LoRA target modules\n        if self.use_lora and self.lora_target_modules is None:\n            self.lora_target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n        \n        # Validate configuration\n        if self.use_kl_penalty and not self.use_reference_model:\n            raise ValueError(\"KL penalty requires reference model (set use_reference_model=True)\")\n        if self.use_reward_model and not self.reward_model_id:\n            raise ValueError(\"Reward model ID required when use_reward_model=True\")\n        if self.enable_thinking:\n            print(\"\u26a0 Thinking mode enabled. Using thinking-mode parameters (temp=0.6, top_p=0.95)\")\n            print(\"  Note: Thinking mode requires adequate max_new_tokens (32768 recommended)\")\n        \n        os.makedirs(self.output_dir, exist_ok=True)\n\n# Create config\nconfig = RLConfig()\n\nprint(\"\\n=== RL Training Configuration ===\")\nprint(f\"Policy Model: {config.policy_model_id}\")\nprint(f\"Use LoRA: {config.use_lora}\")\nprint(f\"Algorithm: {config.algorithm}\")\nprint(f\"Enable Thinking: {config.enable_thinking}\")\nprint(f\"Use Chat Template: {config.use_chat_template}\")\nprint(f\"Steps: {config.num_steps}\")\nprint(f\"Prompts per step: {config.num_prompts_per_step}\")\nprint(f\"Samples per prompt: {config.samples_per_prompt}\")\nprint(f\"Effective batch size: {config.num_prompts_per_step * config.samples_per_prompt}\")\nprint(f\"Learning Rate: {config.learning_rate}\")\nprint(f\"Max New Tokens: {config.max_new_tokens}\")\nprint(f\"Output Dir: {config.output_dir}\")\n\n# Save config\nwith open(os.path.join(config.output_dir, \"config.json\"), \"w\") as f:\n    config_dict = {k: v for k, v in config.__dict__.items() if not callable(v)}\n    json.dump(config_dict, f, indent=2)\nprint(f\"\u2713 Config saved\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83c\udfb2 Set Random Seed"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(config.seed)\nprint(f\"\u2713 Random seed set to {config.seed}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udce6 Model Loading\n\nThis section loads the policy model, reference model (if needed), and reward model (if needed)."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Model Loading Utilities\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\nfrom peft import LoraConfig, get_peft_model\n\ndef get_torch_dtype(dtype_str: str):\n    if dtype_str == \"bfloat16\":\n        return torch.bfloat16\n    elif dtype_str == \"float16\":\n        return torch.float16\n    else:\n        return torch.float32\n\ndef load_tokenizer(model_id: str):\n    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token if tokenizer.eos_token else \"[PAD]\"\n    tokenizer.padding_side = \"left\"\n    return tokenizer\n\ndef load_causal_lm(model_id: str, dtype_str: str, use_lora: bool = False, lora_config=None):\n    dtype = get_torch_dtype(dtype_str)\n    print(f\"Loading model: {model_id}...\")\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id, torch_dtype=dtype, device_map=\"auto\", trust_remote_code=True\n    )\n    if use_lora and lora_config:\n        print(f\"Applying LoRA...\")\n        model = get_peft_model(model, lora_config)\n        model.print_trainable_parameters()\n    return model\n\nprint(\"\u2713 Model loading utilities defined\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Load Tokenizer\nprint(\"=== Loading Tokenizer ===\")\ntokenizer = load_tokenizer(config.policy_model_id)\nprint(f\"\u2713 Tokenizer loaded (vocab: {len(tokenizer)})\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Load Policy Model\nprint(\"=== Loading Policy Model ===\")\n\nlora_config = None\nif config.use_lora:\n    lora_config = LoraConfig(\n        r=config.lora_r, lora_alpha=config.lora_alpha, lora_dropout=config.lora_dropout,\n        bias=\"none\", task_type=\"CAUSAL_LM\", target_modules=config.lora_target_modules\n    )\n\npolicy_model = load_causal_lm(\n    config.policy_model_id, config.policy_model_dtype,\n    use_lora=config.use_lora, lora_config=lora_config\n)\npolicy_model.config.use_cache = False\nprint(f\"\u2713 Policy model loaded\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Load Reference Model (optional)\nreference_model = None\nif config.use_reference_model:\n    print(\"=== Loading Reference Model ===\")\n    reference_model = load_causal_lm(config.reference_model_id, config.policy_model_dtype)\n    reference_model.eval()\n    for param in reference_model.parameters():\n        param.requires_grad_(False)\n    print(\"\u2713 Reference model loaded\")\nelse:\n    print(\"\u2297 Reference model not used\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Load Reward Model (optional)\nreward_model = None\nif config.use_reward_model and config.reward_model_id:\n    print(\"=== Loading Reward Model ===\")\n    reward_model = AutoModelForSequenceClassification.from_pretrained(\n        config.reward_model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n    )\n    reward_model.eval()\n    for param in reward_model.parameters():\n        param.requires_grad_(False)\n    print(\"\u2713 Reward model loaded\")\nelse:\n    print(\"\u2297 Reward model not used (will use rule-based rewards)\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udcac Chat Template & Thinking Mode\n\n**Qwen3-specific features for chat formatting and thinking mode.**"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Chat Template Configuration\n\ndef format_prompt(prompt: str, enable_thinking: Optional[bool] = None) -> str:\n    \"\"\"\n    Format prompt according to configuration.\n    \n    Args:\n        prompt: Raw prompt text\n        enable_thinking: Override config.enable_thinking (for testing)\n    \n    Returns:\n        Formatted prompt string\n    \"\"\"\n    if enable_thinking is None:\n        enable_thinking = config.enable_thinking\n    \n    if config.use_chat_template:\n        # Use Qwen3 chat template\n        messages = []\n        if config.system_prompt:\n            messages.append({\"role\": \"system\", \"content\": config.system_prompt})\n        messages.append({\"role\": \"user\", \"content\": prompt})\n        \n        if hasattr(tokenizer, 'apply_chat_template'):\n            # Qwen3 supports enable_thinking parameter\n            return tokenizer.apply_chat_template(\n                messages,\n                tokenize=False,\n                add_generation_prompt=True,\n                enable_thinking=enable_thinking  # Qwen3-specific\n            )\n        else:\n            # Fallback for other models\n            formatted = \"\"\n            if config.system_prompt:\n                formatted += f\"System: {config.system_prompt}\\n\\n\"\n            formatted += f\"User: {prompt}\\n\\nAssistant:\"\n            return formatted\n    else:\n        # Simple template (for base models)\n        return config.prompt_template.format(prompt=prompt)\n\n# Test both modes\nprint(\"=== Chat Template Examples ===\")\ntest_q = \"What is 2+2?\"\n\nif config.use_chat_template:\n    # Test non-thinking mode\n    prompt_no_think = format_prompt(test_q, enable_thinking=False)\n    print(f\"\\nNon-thinking mode:\")\n    print(prompt_no_think[:300])\n    \n    # Test thinking mode\n    prompt_think = format_prompt(test_q, enable_thinking=True)\n    print(f\"\\nThinking mode:\")\n    print(prompt_think[:300])\nelse:\n    prompt = format_prompt(test_q)\n    print(f\"\\nSimple template:\")\n    print(prompt[:300])\n\nprint(f\"\\n\u2713 Chat template configured (thinking={config.enable_thinking})\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### \ud83e\udde0 Thinking Mode Utilities\n\nQwen3 can generate <think>...</think> blocks for reasoning. These utilities parse and handle thinking content."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Thinking Mode Utilities\n\nQWEN3_THINK_END_TOKEN = 151668  # Token ID for </think>\n\ndef parse_thinking_response(output_ids: list, tokenizer) -> tuple:\n    \"\"\"\n    Parse Qwen3 response to separate thinking content from final response.\n    \n    Args:\n        output_ids: List of generated token IDs\n        tokenizer: Tokenizer instance\n    \n    Returns:\n        (thinking_content, response_content) tuple\n    \"\"\"\n    try:\n        # Find </think> token (151668)\n        index = len(output_ids) - output_ids[::-1].index(QWEN3_THINK_END_TOKEN)\n    except ValueError:\n        # No thinking block found\n        index = 0\n    \n    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n    response_content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n    \n    return thinking_content, response_content\n\ndef get_generation_params(enable_thinking: bool = None, mode: str = \"train\"):\n    \"\"\"\n    Get Qwen3-recommended generation parameters.\n    \n    Args:\n        enable_thinking: If None, uses config.enable_thinking\n        mode: \"train\" or \"eval\" - determines which temperature/top_p to use\n    \n    Returns:\n        Dictionary of generation parameters\n    \"\"\"\n    if enable_thinking is None:\n        enable_thinking = config.enable_thinking\n    \n    if enable_thinking:\n        # Qwen3 best practices for thinking mode\n        temp = config.thinking_train_temperature if mode == \"train\" else config.thinking_eval_temperature\n        top_p = config.thinking_train_top_p if mode == \"train\" else config.thinking_eval_top_p\n    else:\n        # Qwen3 best practices for non-thinking mode\n        temp = config.train_temperature if mode == \"train\" else config.eval_temperature\n        top_p = config.top_p  # Same for both train/eval in non-thinking\n    \n    return {\n        \"do_sample\": True,\n        \"temperature\": temp,\n        \"top_p\": top_p,\n        \"top_k\": config.top_k,\n    }\n\nprint(\"\u2713 Thinking mode utilities defined\")\nprint(f\"  Thinking token ID: {QWEN3_THINK_END_TOKEN}\")\nprint(f\"  Parse thinking: {config.parse_thinking}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udee0\ufe0f Tool Use (Qwen-Agent)\n\n**Qwen3 excels at tool calling. Use Qwen-Agent for best results.**"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Tool Use Configuration\n\n# Check if Qwen-Agent is available\ntry:\n    import qwen_agent\n    QWEN_AGENT_AVAILABLE = True\nexcept ImportError:\n    QWEN_AGENT_AVAILABLE = False\n\ntools = {}\nqwen_agent_bot = None\n\nif config.use_qwen_agent and QWEN_AGENT_AVAILABLE:\n    print(\"=== Qwen-Agent Tool Use ===\")\n    from qwen_agent.agents import Assistant\n    \n    # Configure LLM for Qwen-Agent\n    llm_cfg = {\n        'model': config.policy_model_id,\n        'model_server': 'http://localhost:8000/v1',  # Set your API endpoint\n        'api_key': 'EMPTY',\n        'generate_cfg': {\n            'thought_in_content': True,  # Include thinking in content\n        },\n    }\n    \n    # Configure tools\n    tool_list = config.qwen_agent_tools if config.qwen_agent_tools else []\n    \n    # Example tools:\n    # - 'code_interpreter': Execute Python code\n    # - MCP servers for time, fetch, etc.\n    \n    print(f\"Configured tools: {tool_list}\")\n    print(\"\u2713 Qwen-Agent configured\")\n    \n    # Note: Agent will be used during generation if needed\n    # qwen_agent_bot = Assistant(llm=llm_cfg, function_list=tool_list)\n\nelif config.use_qwen_agent and not QWEN_AGENT_AVAILABLE:\n    print(\"\u26a0 Qwen-Agent requested but not installed\")\n    print(\"  Install: pip install qwen-agent\")\n\nelif config.use_tools:\n    print(\"=== Custom Tools ===\")\n    # Define custom tools here\n    # Example: tools[\"calculator\"] = calculator_function\n    print(f\"\u2713 Custom tools loaded: {list(tools.keys())}\")\n\nelse:\n    print(\"\u2297 Tools not used\")\n\n# Environment\nenvironment = None\nif config.use_environment:\n    # TODO: Initialize environment here\n    print(\"\u2713 Environment initialized\")\nelse:\n    print(\"\u2297 Environment not used\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83c\udf81 Reward Function\n\n**IMPORTANT: Customize this for your task!**"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Reward Function\nimport re\n\ndef compute_reward(prompt: str, response: str, ground_truth: Optional[str] = None) -> float:\n    \"\"\"\n    Compute reward for a generated response.\n    \n    **TODO: CUSTOMIZE THIS FOR YOUR TASK!**\n    \n    Args:\n        prompt: The input prompt\n        response: Generated response\n        ground_truth: Ground truth answer (if available)\n    \n    Returns:\n        Reward score (float)\n    \"\"\"\n    if config.reward_type == \"model\" and reward_model is not None:\n        # Use learned reward model\n        with torch.no_grad():\n            inputs = tokenizer(\n                prompt + response, \n                return_tensors=\"pt\", \n                truncation=True, \n                max_length=1024\n            ).to(reward_model.device)\n            outputs = reward_model(**inputs)\n            # Most reward models output a single scalar\n            if hasattr(outputs, 'score'):\n                reward = outputs.score.item()\n            else:\n                reward = outputs.logits[0].item()\n        return reward\n    \n    elif config.reward_type == \"rule\":\n        # Rule-based reward (CUSTOMIZE THIS FOR YOUR TASK)\n        reward = 0.0\n        \n        # Example 1: Format checking (e.g., answer in brackets)\n        if re.search(r\"\\[.*?\\]\", response):\n            reward += 0.5\n        \n        # Example 2: Exact match with ground truth (if available)\n        if ground_truth is not None and config.use_ground_truth:\n            # Extract predicted answer from response\n            pred_match = re.search(r\"\\[\\s*([^\\]]+)\\s*\\]\", response)\n            # Extract ground truth answer (customize based on format)\n            if \"####\" in ground_truth:\n                # GSM8K format: answer after ####\n                true_match = re.search(r\"####\\s*([^\\s]+)\", ground_truth)\n            else:\n                true_match = re.search(r\"\\[\\s*([^\\]]+)\\s*\\]\", ground_truth)\n            \n            if pred_match and true_match:\n                pred = pred_match.group(1).strip().replace(\",\", \"\")\n                true = true_match.group(1).strip().replace(\",\", \"\")\n                if pred == true:\n                    reward += 1.0  # Correct answer bonus\n        \n        # Example 3: Length penalty (optional)\n        if len(response.split()) > 500:\n            reward -= 0.1\n        \n        return reward\n    \n    else:\n        # Custom reward (TODO: implement your own)\n        return 0.0\n\n# Test reward function\ntest_response = \"Let me solve step by step. 2 + 2 = 4. The answer is [4].\"\ntest_ground_truth = \"#### 4\"\ntest_reward = compute_reward(\"What is 2+2?\", test_response, test_ground_truth)\n\nprint(f\"=== Reward Function ===\")\nprint(f\"Example reward: {test_reward:.4f}\")\nif config.use_ground_truth:\n    print(f\"Ground truth used: Yes\")\nelse:\n    print(f\"Ground truth used: No\")\nprint(\"\\n\u26a0 WARNING: Placeholder reward function!\")\nprint(\"   Customize compute_reward() for your specific task.\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udcca Data Loading"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Load Dataset\nfrom datasets import load_dataset\n\nprint(\"=== Loading Dataset ===\")\nif config.dataset_config:\n    dataset = load_dataset(config.dataset_name, config.dataset_config, split=config.dataset_split)\nelse:\n    dataset = load_dataset(config.dataset_name, split=config.dataset_split)\n\n# Validate required fields exist\nif config.prompt_field not in dataset.column_names:\n    raise ValueError(f\"Prompt field '{config.prompt_field}' not found in dataset. Available: {dataset.column_names}\")\nif config.use_ground_truth and config.answer_field and config.answer_field not in dataset.column_names:\n    raise ValueError(f\"Answer field '{config.answer_field}' not found in dataset. Available: {dataset.column_names}\")\n\n# Split into train/val\nval_size = min(config.val_size, len(dataset))\nval_dataset = dataset.select(range(val_size))\ntrain_dataset = dataset.select(range(val_size, len(dataset)))\n\nprint(f\"Train: {len(train_dataset):,} | Val: {len(val_dataset):,}\")\nprint(f\"Using fields: prompt='{config.prompt_field}', answer='{config.answer_field}'\")\n\n# Prepare prompts\ntrain_prompts = [format_prompt(ex[config.prompt_field]) for ex in train_dataset]\nval_prompts = [format_prompt(ex[config.prompt_field]) for ex in val_dataset]\n\nprint(f\"\\n\u2713 Prompts prepared\")\nprint(f\"Example prompt:\\n{train_prompts[0][:200]}\")\nif config.answer_field and train_dataset[0].get(config.answer_field):\n    print(f\"\\nExample answer:\\n{str(train_dataset[0][config.answer_field])[:200]}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \u2705 Pre-Training Validation\n\n**Test model generation BEFORE training to ensure everything works correctly.**\n\nThis section validates:\n- Chat template formatting\n- Model generation (both training and inference modes)\n- Thinking mode (if enabled)\n- Generation parameters"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Pre-Training Validation\n\nprint(\"=\" * 80)\nprint(\"PRE-TRAINING VALIDATION\")\nprint(\"=\" * 80)\n\n# Test prompts\ntest_prompts = [\n    \"What is 15 + 27?\",\n    \"Solve: 3x + 5 = 20\",\n]\n\nprint(f\"\\nTesting with {len(test_prompts)} prompts\")\nprint(f\"Model: {config.policy_model_id}\")\nprint(f\"Thinking mode: {config.enable_thinking}\")\nprint(f\"Chat template: {config.use_chat_template}\")\n\n# Test generation function\ndef test_generation(prompt, mode=\"train\"):\n    \"\"\"Test generation with given prompt.\"\"\"\n    formatted = format_prompt(prompt, enable_thinking=config.enable_thinking)\n    \n    inputs = tokenizer(\n        [formatted],\n        padding=True,\n        truncation=True,\n        max_length=2048,\n        return_tensors=\"pt\"\n    ).to(DEVICE)\n    \n    # Get generation params (use mode parameter)\n    gen_params = get_generation_params(config.enable_thinking, mode=mode)\n    gen_params.update({\n        \"max_new_tokens\": min(config.max_new_tokens, 512),  # Use smaller for testing\n        \"pad_token_id\": tokenizer.pad_token_id,\n        \"eos_token_id\": tokenizer.eos_token_id,\n        \"use_cache\": True,\n    })\n    \n    policy_model.eval()\n    policy_model.config.use_cache = True\n    \n    with torch.no_grad():\n        output = policy_model.generate(**inputs, **gen_params)\n    \n    # Extract generated part\n    generated_ids = output[0, inputs.input_ids.size(1):].tolist()\n    \n    # Parse thinking if enabled\n    if config.enable_thinking and config.parse_thinking:\n        thinking, response = parse_thinking_response(generated_ids, tokenizer)\n        return thinking, response, generated_ids\n    else:\n        response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n        return None, response, generated_ids\n\n# Test each prompt\nfor i, prompt in enumerate(test_prompts):\n    print(f\"\\n{'=' * 80}\")\n    print(f\"Test {i+1}: {prompt}\")\n    print(\"-\" * 80)\n    \n    # Test training mode\n    print(\"\\n[Training Mode Generation]\")\n    thinking_train, response_train, ids_train = test_generation(prompt, mode=\"train\")\n    if thinking_train:\n        print(f\"Thinking: {thinking_train[:200]}\")\n        print(f\"Response: {response_train[:200]}\")\n    else:\n        print(f\"Response: {response_train[:200]}\")\n    print(f\"Tokens generated: {len(ids_train)}\")\n    \n    # Test eval mode\n    print(\"\\n[Eval Mode Generation]\")\n    thinking_eval, response_eval, ids_eval = test_generation(prompt, mode=\"eval\")\n    if thinking_eval:\n        print(f\"Thinking: {thinking_eval[:200]}\")\n        print(f\"Response: {response_eval[:200]}\")\n    else:\n        print(f\"Response: {response_eval[:200]}\")\n    print(f\"Tokens generated: {len(ids_eval)}\")\n\nprint(f\"\\n{'=' * 80}\")\nprint(\"PRE-TRAINING VALIDATION COMPLETE\")\nprint(\"\u2713 Model generation working correctly\")\nprint(\"\u2713 Chat template functioning\")\nif config.enable_thinking:\n    print(\"\u2713 Thinking mode active\")\nprint(\"=\" * 80)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udcca Baseline Evaluation\n\nEvaluate model performance before training to establish baseline metrics."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Baseline Evaluation\n\ndef evaluate_model(model, prompts, dataset, num_samples=None, desc=\"Eval\"):\n    \"\"\"\n    Evaluate model on a dataset.\n    \n    Args:\n        model: Model to evaluate\n        prompts: List of formatted prompts\n        dataset: Dataset with ground truth\n        num_samples: Number of samples to evaluate (None = all)\n        desc: Description for progress bar\n    \n    Returns:\n        Dictionary with evaluation metrics\n    \"\"\"\n    if num_samples is None:\n        num_samples = len(prompts)\n    else:\n        num_samples = min(num_samples, len(prompts))\n    \n    model.eval()\n    model.config.use_cache = True\n    \n    total_reward = 0.0\n    all_rewards = []\n    \n    with torch.no_grad():\n        for i in tqdm(range(0, num_samples, config.val_batch_size), desc=desc):\n            batch_end = min(i + config.val_batch_size, num_samples)\n            batch_prompts = prompts[i:batch_end]\n            \n            # Tokenize\n            encoding = tokenizer(\n                batch_prompts,\n                padding=True,\n                truncation=True,\n                max_length=2048,\n                return_tensors=\"pt\"\n            ).to(DEVICE)\n            \n            # Get Qwen3-appropriate generation parameters for eval\n            gen_params = get_generation_params(config.enable_thinking, mode=\"eval\")\n            gen_params.update({\n                \"max_new_tokens\": config.max_new_tokens,\n                \"pad_token_id\": tokenizer.pad_token_id,\n                \"eos_token_id\": tokenizer.eos_token_id,\n                \"use_cache\": True,\n            })\n            \n            outputs = model.generate(**encoding, **gen_params)\n            generated_ids = outputs[:, encoding.input_ids.size(1):]\n            \n            # Parse responses\n            generated_texts = []\n            for gen_ids in generated_ids:\n                if config.enable_thinking and config.parse_thinking:\n                    # Parse thinking content\n                    _, response = parse_thinking_response(gen_ids.tolist(), tokenizer)\n                    generated_texts.append(response)\n                else:\n                    response = tokenizer.decode(gen_ids, skip_special_tokens=True)\n                    generated_texts.append(response)\n            \n            # Compute rewards\n            for j, (prompt, response) in enumerate(zip(batch_prompts, generated_texts)):\n                idx = i + j\n                ground_truth = dataset[idx].get(config.answer_field) if config.answer_field else None\n                reward = compute_reward(prompt, response, ground_truth)\n                all_rewards.append(reward)\n                total_reward += reward\n    \n    mean_reward = total_reward / num_samples\n    std_reward = np.std(all_rewards) if len(all_rewards) > 1 else 0.0\n    \n    model.train()\n    return {\n        \"mean_reward\": mean_reward,\n        \"std_reward\": std_reward,\n        \"num_samples\": num_samples\n    }\n\n# Run baseline evaluation\nprint(\"\\n\" + \"=\"*80)\nprint(\"BASELINE EVALUATION\")\nprint(\"=\"*80)\n\nbaseline_metrics = evaluate_model(\n    policy_model, \n    val_prompts, \n    val_dataset, \n    num_samples=min(100, len(val_prompts)),\n    desc=\"Baseline\"\n)\n\nprint(f\"\\nBaseline Metrics:\")\nprint(f\"  Mean Reward: {baseline_metrics['mean_reward']:.4f} \u00b1 {baseline_metrics['std_reward']:.4f}\")\nprint(f\"  Evaluated on: {baseline_metrics['num_samples']} examples\")\nprint(\"=\"*80)\n\n# Save baseline\nbaseline_path = os.path.join(config.output_dir, \"baseline_metrics.json\")\nwith open(baseline_path, \"w\") as f:\n    json.dump(baseline_metrics, f, indent=2)\nprint(f\"\u2713 Baseline saved to {baseline_path}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udd27 Training Utilities"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Training Utilities\n\ndef mask_after_eos(token_ids: torch.Tensor, eos_id: int) -> torch.Tensor:\n    \"\"\"Create mask for tokens before first EOS.\"\"\"\n    is_eos = (token_ids == eos_id)\n    eos_positions = is_eos.cumsum(dim=1)\n    return (eos_positions == 0).float()\n\ndef compute_model_logprobs(model, input_ids, attention_mask, target_ids, micro_batch_size=8):\n    \"\"\"Compute log probabilities of target tokens.\"\"\"\n    batch_size = input_ids.size(0)\n    target_len = target_ids.size(1)\n    \n    full_ids = torch.cat([input_ids, target_ids], dim=1)\n    full_mask = torch.cat([attention_mask, torch.ones_like(target_ids)], dim=1)\n    \n    if micro_batch_size >= batch_size:\n        outputs = model(input_ids=full_ids[:, :-1], attention_mask=full_mask[:, :-1])\n        logits = outputs.logits[:, -target_len:, :]\n        logprobs = F.log_softmax(logits, dim=-1)\n        token_logprobs = logprobs.gather(-1, target_ids.unsqueeze(-1)).squeeze(-1)\n    else:\n        # Micro-batching for memory efficiency\n        token_logprobs_list = []\n        for i in range(0, batch_size, micro_batch_size):\n            micro_full_ids = full_ids[i:i+micro_batch_size]\n            micro_full_mask = full_mask[i:i+micro_batch_size]\n            micro_target_ids = target_ids[i:i+micro_batch_size]\n            \n            outputs = model(input_ids=micro_full_ids[:, :-1], attention_mask=micro_full_mask[:, :-1])\n            logits = outputs.logits[:, -target_len:, :]\n            logprobs = F.log_softmax(logits, dim=-1)\n            micro_token_logprobs = logprobs.gather(-1, micro_target_ids.unsqueeze(-1)).squeeze(-1)\n            token_logprobs_list.append(micro_token_logprobs)\n        token_logprobs = torch.cat(token_logprobs_list, dim=0)\n    \n    return token_logprobs\n\nclass MetricsTracker:\n    \"\"\"Track and display training metrics with EMA smoothing.\"\"\"\n    \n    def __init__(self, ema_momentum=0.9):\n        self.ema_momentum = ema_momentum\n        self.metrics = []\n        self.ema_values = {}\n        empty_df = pd.DataFrame(columns=[\"step\", \"loss\", \"loss_ema\", \"reward\", \"reward_ema\", \"kl\", \"tokens\", \"val_reward\"])\n        self.display_handle = display(empty_df, display_id=True)\n    \n    def update_ema(self, key, value):\n        if key not in self.ema_values:\n            self.ema_values[key] = value\n        else:\n            self.ema_values[key] = self.ema_momentum * self.ema_values[key] + (1 - self.ema_momentum) * value\n        return self.ema_values[key]\n    \n    def log(self, step, loss, reward, kl=0.0, tokens=0, val_reward=None):\n        loss_ema = self.update_ema(\"loss\", loss)\n        reward_ema = self.update_ema(\"reward\", reward)\n        \n        metric = {\n            \"step\": step, \"loss\": loss, \"loss_ema\": loss_ema,\n            \"reward\": reward, \"reward_ema\": reward_ema, \"kl\": kl, \"tokens\": tokens\n        }\n        \n        if val_reward is not None:\n            metric[\"val_reward\"] = val_reward\n        \n        self.metrics.append(metric)\n        \n        df = pd.DataFrame(self.metrics[-100:])\n        styled = df.style.format({\n            \"loss\": \"{:.4f}\", \"loss_ema\": \"{:.4f}\",\n            \"reward\": \"{:.4f}\", \"reward_ema\": \"{:.4f}\", \"kl\": \"{:.4f}\",\n            \"val_reward\": lambda x: \"\" if pd.isna(x) else f\"{x:.4f}\"\n        })\n        self.display_handle.update(styled)\n        \n        return metric\n\nprint(\"\u2713 Training utilities defined\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udcc8 Initialize WandB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Initialize WandB Run\nwandb_run = wandb.init(\n    project=config.wandb_project,\n    name=config.wandb_run_name,\n    config=config.__dict__,\n    job_type=\"training\"\n)\n\nwandb.watch(policy_model, log=\"all\", log_freq=100)\n\nprint(f\"\u2713 WandB initialized\")\nprint(f\"  Run: {config.wandb_run_name}\")\nprint(f\"  URL: {wandb_run.get_url()}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\ude80 Training Loop"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Setup Optimizer\noptimizer = torch.optim.AdamW(\n    policy_model.parameters(),\n    lr=config.learning_rate,\n    weight_decay=config.weight_decay\n)\nprint(f\"\u2713 Optimizer initialized (lr={config.learning_rate})\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Training Loop\nmetrics_tracker = MetricsTracker(ema_momentum=config.ema_momentum)\ntotal_tokens = 0\nbest_val_reward = -float('inf')\n\n# Reward normalization tracking (if enabled)\nif config.normalize_rewards:\n    reward_mean = 0.0\n    reward_std = 1.0\n    reward_count = 0\n\nprint(\"=\" * 80)\nprint(\"STARTING TRAINING\")\nprint(\"=\" * 80)\n\npbar = tqdm(range(config.num_steps), desc=\"Training\")\n\n# Initialize gradients\noptimizer.zero_grad(set_to_none=True)\n\nfor step in pbar:\n    # ========== SAMPLE PROMPTS ==========\n    rng = np.random.default_rng(config.seed + step)\n    \n    # Sample prompts (with replacement to avoid crashes on small datasets)\n    num_prompts = min(config.num_prompts_per_step, len(train_dataset))\n    prompt_indices = rng.choice(len(train_dataset), size=num_prompts, replace=True)\n    \n    # Build batch: repeat each prompt for multiple samples\n    batch_prompts = []\n    batch_ground_truths = []\n    batch_original_indices = []\n    for idx in prompt_indices:\n        for _ in range(config.samples_per_prompt):\n            batch_prompts.append(train_prompts[idx])\n            if config.use_ground_truth and config.answer_field:\n                batch_ground_truths.append(train_dataset[idx][config.answer_field])\n            else:\n                batch_ground_truths.append(None)\n            batch_original_indices.append(idx)\n    \n    # ========== TOKENIZE ==========\n    prompt_encoding = tokenizer(\n        batch_prompts, \n        padding=True, \n        truncation=True, \n        max_length=2048, \n        return_tensors=\"pt\"\n    ).to(DEVICE)\n    \n    # ========== GENERATE RESPONSES (ON-POLICY) ==========\n    policy_model.eval()\n    policy_model.config.use_cache = True\n    \n    with torch.no_grad():\n        # Get Qwen3-appropriate generation parameters\n        gen_params = get_generation_params(config.enable_thinking)\n        gen_params.update({\n            \"max_new_tokens\": config.max_new_tokens,\n            \"pad_token_id\": tokenizer.pad_token_id,\n            \"eos_token_id\": tokenizer.eos_token_id,\n        })\n        \n        generation_output = policy_model.generate(**prompt_encoding, **gen_params)\n        generated_ids = generation_output[:, prompt_encoding.input_ids.size(1):]\n    \n    # Create validity mask (tokens before first EOS)\n    valid_mask = mask_after_eos(generated_ids, tokenizer.eos_token_id)\n    \n    # Check for empty generations\n    if valid_mask.sum() == 0:\n        print(f\"\\nWarning: All sequences empty at step {step}, skipping batch\")\n        continue\n    \n    # ========== COMPUTE REWARDS ==========\n    with torch.no_grad():\n        # Parse responses (handle thinking if enabled)\n        generated_texts = []\n        for gen_ids in generated_ids:\n            if config.enable_thinking and config.parse_thinking:\n                # Extract only the response part (not thinking)\n                _, response = parse_thinking_response(gen_ids.tolist(), tokenizer)\n                generated_texts.append(response)\n            else:\n                response = tokenizer.decode(gen_ids, skip_special_tokens=True)\n                generated_texts.append(response)\n        \n        rewards = []\n        for prompt, response, ground_truth in zip(batch_prompts, generated_texts, batch_ground_truths):\n            reward = compute_reward(prompt, response, ground_truth)\n            rewards.append(reward)\n        \n        rewards_tensor = torch.tensor(rewards, device=DEVICE, dtype=torch.float32)\n        \n        # Reward normalization (if enabled)\n        if config.normalize_rewards:\n            reward_count += len(rewards)\n            reward_mean = (reward_mean * (reward_count - len(rewards)) + rewards_tensor.sum().item()) / reward_count\n            reward_var = ((rewards_tensor - reward_mean) ** 2).mean().item()\n            reward_std = max(np.sqrt(reward_var), 1e-8)\n            normalized_rewards = (rewards_tensor - reward_mean) / reward_std\n            rewards_tensor = normalized_rewards\n        \n        mean_reward = rewards_tensor.mean().item()\n    \n    # ========== COMPUTE LOG PROBABILITIES ==========\n    policy_model.train()\n    policy_model.config.use_cache = False\n    \n    # Policy log probs (with gradient)\n    policy_logprobs = compute_model_logprobs(\n        policy_model,\n        prompt_encoding.input_ids,\n        prompt_encoding.attention_mask,\n        generated_ids,\n        micro_batch_size=8\n    )\n    \n    # ========== COMPUTE KL PENALTY (if using reference model) ==========\n    kl_penalty_value = 0.0\n    if config.use_kl_penalty and reference_model is not None:\n        with torch.no_grad():\n            reference_logprobs = compute_model_logprobs(\n                reference_model,\n                prompt_encoding.input_ids,\n                prompt_encoding.attention_mask,\n                generated_ids,\n                micro_batch_size=8\n            )\n            # Compute KL per token\n            kl_per_token = policy_logprobs.detach() - reference_logprobs\n            # Sum per sequence\n            kl_per_sequence = (kl_per_token * valid_mask).sum(dim=1) / valid_mask.sum(dim=1).clamp(min=1.0)\n            # Mean for logging\n            kl_penalty_value = kl_per_sequence.mean().item()\n            # Subtract KL from rewards (before broadcasting)\n            rewards_tensor = rewards_tensor - config.kl_coef * kl_per_sequence\n    \n    # ========== COMPUTE ADVANTAGES ==========\n    # Broadcast rewards to token level\n    rewards_broadcast = rewards_tensor.unsqueeze(1).expand_as(policy_logprobs)\n    \n    # Use mean reward as baseline\n    baseline = rewards_tensor.mean()\n    advantages = rewards_broadcast - baseline\n    \n    # Normalize advantages (recommended for stability)\n    if config.normalize_advantages:\n        adv_mean = (advantages * valid_mask).sum() / valid_mask.sum().clamp(min=1.0)\n        adv_std = torch.sqrt(((advantages - adv_mean) ** 2 * valid_mask).sum() / valid_mask.sum().clamp(min=1.0))\n        advantages = (advantages - adv_mean) / (adv_std + 1e-8)\n    \n    # ========== COMPUTE LOSS ==========\n    loss_per_token = -advantages.detach() * policy_logprobs * valid_mask\n    loss = loss_per_token.sum() / valid_mask.sum().clamp(min=1.0)\n    \n    # Scale loss for gradient accumulation\n    if config.grad_accumulation_steps > 1:\n        loss = loss / config.grad_accumulation_steps\n    \n    # ========== BACKWARD PASS ==========\n    loss.backward()\n    \n    # Gradient clipping and optimizer step\n    if (step + 1) % config.grad_accumulation_steps == 0:\n        if config.max_grad_norm > 0:\n            torch.nn.utils.clip_grad_norm_(policy_model.parameters(), config.max_grad_norm)\n        optimizer.step()\n        optimizer.zero_grad(set_to_none=True)\n    \n    # ========== LOGGING ==========\n    total_tokens += int(valid_mask.sum().item())\n    \n    # Validation (if enabled)\n    val_reward = None\n    if (step % config.val_every == 0 and step > 0) or (step == config.num_steps - 1):\n        val_metrics = evaluate_model(\n            policy_model,\n            val_prompts,\n            val_dataset,\n            num_samples=min(50, len(val_prompts)),\n            desc=f\"Val@{step}\"\n        )\n        val_reward = val_metrics['mean_reward']\n        \n        # Track best model\n        if val_reward > best_val_reward:\n            best_val_reward = val_reward\n            best_dir = os.path.join(config.output_dir, \"best_model\")\n            os.makedirs(best_dir, exist_ok=True)\n            policy_model.save_pretrained(best_dir)\n            tokenizer.save_pretrained(best_dir)\n            print(f\"\\n\u2713 New best model (reward={val_reward:.4f}) saved to {best_dir}\")\n    \n    # Log metrics\n    metric = metrics_tracker.log(\n        step=step,\n        loss=loss.item() * (config.grad_accumulation_steps if config.grad_accumulation_steps > 1 else 1),\n        reward=mean_reward,\n        kl=kl_penalty_value,\n        tokens=total_tokens\n    )\n    \n    if val_reward is not None:\n        metric['val_reward'] = val_reward\n    \n    wandb.log(metric, step=step)\n    \n    # Update progress bar\n    pbar.set_postfix({\n        \"loss\": f\"{metric['loss']:.3f}\",\n        \"reward\": f\"{mean_reward:.3f}\",\n        \"kl\": f\"{kl_penalty_value:.3f}\",\n        **({\"val\": f\"{val_reward:.3f}\"} if val_reward is not None else {})\n    })\n    \n    # ========== CHECKPOINTING ==========\n    if (step % config.save_every == 0 and step > 0) or (step == config.num_steps - 1):\n        checkpoint_dir = os.path.join(config.output_dir, f\"checkpoint-{step}\")\n        os.makedirs(checkpoint_dir, exist_ok=True)\n        policy_model.save_pretrained(checkpoint_dir)\n        tokenizer.save_pretrained(checkpoint_dir)\n        print(f\"\\n\u2713 Checkpoint saved: {checkpoint_dir}\")\n        \n        artifact = wandb.Artifact(name=f\"model-checkpoint-{step}\", type=\"model\")\n        artifact.add_dir(checkpoint_dir)\n        wandb.log_artifact(artifact)\n    \n    # Clean up\n    del generated_ids, generated_texts, rewards_tensor, policy_logprobs\n    if config.use_kl_penalty and reference_model is not None:\n        del reference_logprobs\n    torch.cuda.empty_cache()\n\nprint(\"=\" * 80)\nprint(\"TRAINING COMPLETE\")\nprint(f\"Best validation reward: {best_val_reward:.4f}\")\nprint(\"=\" * 80)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udcbe Save Final Model"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Save Final Model\nfinal_dir = os.path.join(config.output_dir, \"final_model\")\nos.makedirs(final_dir, exist_ok=True)\n\npolicy_model.save_pretrained(final_dir)\ntokenizer.save_pretrained(final_dir)\nprint(f\"\u2713 Final model saved to: {final_dir}\")\n\n# Merge LoRA if used\nif config.use_lora:\n    print(\"Merging LoRA adapters...\")\n    merged_model = policy_model.merge_and_unload()\n    merged_dir = os.path.join(config.output_dir, \"final_model_merged\")\n    os.makedirs(merged_dir, exist_ok=True)\n    merged_model.save_pretrained(merged_dir)\n    tokenizer.save_pretrained(merged_dir)\n    print(f\"\u2713 Merged model saved to: {merged_dir}\")\n\n# Push to Hub if configured\nif config.push_to_hub and config.hub_repo_id:\n    print(f\"Pushing to Hub: {config.hub_repo_id}\")\n    model_to_push = merged_model if config.use_lora else policy_model\n    model_to_push.push_to_hub(repo_id=config.hub_repo_id, private=True)\n    tokenizer.push_to_hub(repo_id=config.hub_repo_id, private=True)\n    print(f\"\u2713 Pushed to Hub\")\n\n# Log to WandB\nfinal_artifact = wandb.Artifact(name=\"final-model\", type=\"model\")\nfinal_artifact.add_dir(final_dir)\nwandb.log_artifact(final_artifact)\nprint(f\"\u2713 Logged to WandB: {wandb_run.get_url()}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udcca Results Summary"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Training Summary\n# Save metrics\nmetrics_df = pd.DataFrame(metrics_tracker.metrics)\nmetrics_df.to_csv(os.path.join(config.output_dir, \"training_metrics.csv\"), index=False)\n\n# Summary\nsummary = {\n    \"baseline_reward\": baseline_metrics[\"mean_reward\"],\n    \"final_loss\": metrics_tracker.metrics[-1][\"loss\"],\n    \"final_loss_ema\": metrics_tracker.metrics[-1][\"loss_ema\"],\n    \"final_reward\": metrics_tracker.metrics[-1][\"reward\"],\n    \"final_reward_ema\": metrics_tracker.metrics[-1][\"reward_ema\"],\n    \"best_val_reward\": best_val_reward,\n    \"total_tokens\": total_tokens,\n    \"total_steps\": config.num_steps\n}\n\nwith open(os.path.join(config.output_dir, \"summary.json\"), \"w\") as f:\n    json.dump(summary, f, indent=2)\n\nwandb.summary.update(summary)\n\nprint(\"=\" * 80)\nprint(\"TRAINING SUMMARY\")\nprint(\"=\" * 80)\nprint(f\"Baseline Reward: {summary['baseline_reward']:.4f}\")\nprint(f\"Final Loss: {summary['final_loss']:.4f}\")\nprint(f\"Final Loss (EMA): {summary['final_loss_ema']:.4f}\")\nprint(f\"Final Reward: {summary['final_reward']:.4f}\")\nprint(f\"Final Reward (EMA): {summary['final_reward_ema']:.4f}\")\nprint(f\"Best Val Reward: {summary['best_val_reward']:.4f}\")\nprint(f\"Total Tokens: {summary['total_tokens']:,}\")\nprint(f\"\\nImprovement: {summary['best_val_reward'] - summary['baseline_reward']:.4f}\")\nprint(f\"Output: {config.output_dir}\")\nprint(f\"WandB: {wandb_run.get_url()}\")\nprint(\"=\" * 80)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Finish WandB Run\nwandb.finish()\nprint(\"\u2713 WandB run finished\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83c\udf89 Done!\n\nYour RL training is complete. You can now:\n\n1. **Evaluate your model** on test data\n2. **Inspect the metrics** in WandB dashboard\n3. **Load the trained model** from the checkpoints\n4. **Customize** any section for your specific use case\n\n### Next Steps:\n\n- **Customize the reward function** in the \"Reward Function\" section\n- **Add validation metrics** specific to your task\n- **Implement advanced RL algorithms** (PPO, DPO, etc.)\n- **Tune hyperparameters** at the top of the notebook\n\n### Common Modifications:\n\n1. **Change model**: Edit `config.policy_model_id`\n2. **Change dataset**: Edit `config.dataset_name` and fields\n3. **Change algorithm**: Modify loss computation in training loop\n4. **Use reward model**: Set `config.use_reward_model=True`"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}