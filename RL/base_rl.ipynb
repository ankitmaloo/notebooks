{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RL Training Boilerplate\n\nThis notebook provides a modular boilerplate for Reinforcement Learning training of language models using PyTorch directly (no verl).\n\n**Features:**\n- All hyperparameters at the top for easy configuration\n- Modular sections: model loading, tools, chat templates, environment, reward model\n- WandB integration for logging and tracking\n- Easy to swap models and modify components\n- Pure PyTorch implementation\n\n**Requirements:**\n- GPU with sufficient VRAM (A100 recommended)\n- WandB account for logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udee0\ufe0f Setup & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Install Dependencies\n!nvidia-smi -L || true\n\nimport sys\nprint(\"Python:\", sys.version)\n\n# Install required packages\ntry:\n    %pip install -q transformers>=4.51.3 accelerate>=1.4.0 peft>=0.14.0 \\\n                     datasets>=3.3.2 torch wandb huggingface_hub \\\n                     sentencepiece protobuf tqdm matplotlib pandas\nexcept Exception:\n    !pip install -q transformers>=4.51.3 accelerate>=1.4.0 peft>=0.14.0 \\\n                     datasets>=3.3.2 torch wandb huggingface_hub \\\n                     sentencepiece protobuf tqdm matplotlib pandas\n\nimport os, random, time, json, platform\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom tqdm.auto import tqdm\nimport pandas as pd\nfrom IPython.display import display\n\nprint(\"\\n=== Environment ===\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n    print(f\"CUDA version: {torch.version.cuda}\")\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nassert DEVICE == \"cuda\", \"Please connect a GPU for RL training.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd11 WandB & HuggingFace Authentication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Set API Keys (Optional - fill in if needed)\nimport os\n\n# WandB API key - get from https://wandb.ai/authorize\nWANDB_API_KEY = \"\"  # Your WandB API key\nif WANDB_API_KEY:\n    os.environ['WANDB_API_KEY'] = WANDB_API_KEY\n\n# HuggingFace token - get from https://huggingface.co/settings/tokens\nHF_TOKEN = \"\"  # Your HuggingFace token\nif HF_TOKEN:\n    os.environ['HF_TOKEN'] = HF_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Login to WandB and HuggingFace\nimport wandb\nfrom huggingface_hub import login\n\n# WandB login\ntry:\n    wandb.login()\n    print(\"\u2713 WandB login successful\")\nexcept Exception as e:\n    print(f\"\u26a0 WandB login failed: {e}\")\n    print(\"Training will continue without WandB logging\")\n\n# HuggingFace login\ntry:\n    if os.environ.get('HF_TOKEN'):\n        login(token=os.environ['HF_TOKEN'])\n        print(\"\u2713 HuggingFace login successful\")\nexcept Exception as e:\n    print(f\"\u26a0 HuggingFace login failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": "## \u2699\ufe0f HYPERPARAMETERS\n\n**All configurable parameters are in this section for easy modification.**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Hyperparameters Configuration\nfrom dataclasses import dataclass\nfrom typing import Optional, List\n\n@dataclass\nclass RLConfig:\n    # ==================== MODEL CONFIGURATION ====================\n    # Policy model (the model being trained)\n    policy_model_id: str = \"Qwen/Qwen3-0.6B-Base\"\n    policy_model_dtype: str = \"bfloat16\"  # \"bfloat16\", \"float16\", or \"float32\"\n    \n    # Reference model (for KL penalty, optional)\n    use_reference_model: bool = False\n    reference_model_id: Optional[str] = None  # If None, uses policy_model_id\n    \n    # Reward model (optional)\n    use_reward_model: bool = False\n    reward_model_id: Optional[str] = None\n    \n    # ==================== LORA CONFIGURATION ====================\n    use_lora: bool = True\n    lora_r: int = 16\n    lora_alpha: int = 32\n    lora_dropout: float = 0.05\n    lora_target_modules: Optional[List[str]] = None  # Auto-detect if None\n    \n    # ==================== TRAINING CONFIGURATION ====================\n    num_steps: int = 100\n    batch_size: int = 4\n    samples_per_prompt: int = 4\n    \n    learning_rate: float = 1e-4\n    weight_decay: float = 0.0\n    grad_accumulation_steps: int = 1\n    max_grad_norm: float = 1.0\n    \n    max_new_tokens: int = 256\n    train_temperature: float = 0.7\n    eval_temperature: float = 0.0\n    top_p: float = 0.9\n    \n    # ==================== RL ALGORITHM ====================\n    algorithm: str = \"pg\"  # \"pg\", \"ppo\", \"dpo\", \"custom\"\n    use_kl_penalty: bool = False\n    kl_coef: float = 0.1\n    \n    # ==================== DATA CONFIGURATION ====================\n    dataset_name: str = \"openai/gsm8k\"\n    dataset_config: Optional[str] = \"main\"\n    dataset_split: str = \"train\"\n    prompt_field: str = \"question\"\n    answer_field: Optional[str] = \"answer\"\n    \n    val_size: int = 200\n    val_every: int = 10\n    \n    # ==================== PROMPT TEMPLATE ====================\n    prompt_template: str = (\n        \"Solve step by step.\\n\"\n        \"Problem: {prompt}\\n\\nSolution:\"\n    )\n    use_chat_template: bool = False\n    system_prompt: Optional[str] = None\n    \n    # ==================== REWARD CONFIGURATION ====================\n    reward_type: str = \"rule\"  # \"model\", \"rule\", or \"custom\"\n    \n    # ==================== TOOLS & ENVIRONMENT ====================\n    use_tools: bool = False\n    use_environment: bool = False\n    \n    # ==================== LOGGING & CHECKPOINTING ====================\n    wandb_project: str = \"rl-training\"\n    wandb_run_name: Optional[str] = None\n    log_every: int = 10\n    save_every: int = 50\n    ema_momentum: float = 0.9\n    \n    output_dir: str = f\"./run_rl_{int(time.time())}\"\n    push_to_hub: bool = False\n    hub_repo_id: Optional[str] = None\n    \n    seed: int = 42\n    \n    def __post_init__(self):\n        if self.wandb_run_name is None:\n            model_short = self.policy_model_id.split(\"/\")[-1]\n            self.wandb_run_name = f\"rl_{self.algorithm}_{model_short}_{int(time.time())}\"\n        if self.use_reference_model and self.reference_model_id is None:\n            self.reference_model_id = self.policy_model_id\n        if self.use_lora and self.lora_target_modules is None:\n            self.lora_target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n        os.makedirs(self.output_dir, exist_ok=True)\n\n# Create config\nconfig = RLConfig()\n\nprint(\"\\n=== RL Training Configuration ===\")\nprint(f\"Policy Model: {config.policy_model_id}\")\nprint(f\"Algorithm: {config.algorithm}\")\nprint(f\"Steps: {config.num_steps}\")\nprint(f\"Batch Size: {config.batch_size}\")\nprint(f\"Learning Rate: {config.learning_rate}\")\nprint(f\"Output Dir: {config.output_dir}\")\n\n# Save config\nwith open(os.path.join(config.output_dir, \"config.json\"), \"w\") as f:\n    config_dict = {k: v for k, v in config.__dict__.items() if not callable(v)}\n    json.dump(config_dict, f, indent=2)\nprint(f\"\u2713 Config saved\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83c\udfb2 Set Random Seed"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(config.seed)\nprint(f\"\u2713 Random seed set to {config.seed}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udce6 Model Loading\n\nThis section loads the policy model, reference model (if needed), and reward model (if needed)."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Model Loading Utilities\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\nfrom peft import LoraConfig, get_peft_model\n\ndef get_torch_dtype(dtype_str: str):\n    if dtype_str == \"bfloat16\":\n        return torch.bfloat16\n    elif dtype_str == \"float16\":\n        return torch.float16\n    else:\n        return torch.float32\n\ndef load_tokenizer(model_id: str):\n    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token if tokenizer.eos_token else \"[PAD]\"\n    tokenizer.padding_side = \"left\"\n    return tokenizer\n\ndef load_causal_lm(model_id: str, dtype_str: str, use_lora: bool = False, lora_config=None):\n    dtype = get_torch_dtype(dtype_str)\n    print(f\"Loading model: {model_id}...\")\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id, torch_dtype=dtype, device_map=\"auto\", trust_remote_code=True\n    )\n    if use_lora and lora_config:\n        print(f\"Applying LoRA...\")\n        model = get_peft_model(model, lora_config)\n        model.print_trainable_parameters()\n    return model\n\nprint(\"\u2713 Model loading utilities defined\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Load Tokenizer\nprint(\"=== Loading Tokenizer ===\")\ntokenizer = load_tokenizer(config.policy_model_id)\nprint(f\"\u2713 Tokenizer loaded (vocab: {len(tokenizer)})\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Load Policy Model\nprint(\"=== Loading Policy Model ===\")\n\nlora_config = None\nif config.use_lora:\n    lora_config = LoraConfig(\n        r=config.lora_r, lora_alpha=config.lora_alpha, lora_dropout=config.lora_dropout,\n        bias=\"none\", task_type=\"CAUSAL_LM\", target_modules=config.lora_target_modules\n    )\n\npolicy_model = load_causal_lm(\n    config.policy_model_id, config.policy_model_dtype,\n    use_lora=config.use_lora, lora_config=lora_config\n)\npolicy_model.config.use_cache = False\nprint(f\"\u2713 Policy model loaded\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Load Reference Model (optional)\nreference_model = None\nif config.use_reference_model:\n    print(\"=== Loading Reference Model ===\")\n    reference_model = load_causal_lm(config.reference_model_id, config.policy_model_dtype)\n    reference_model.eval()\n    for param in reference_model.parameters():\n        param.requires_grad_(False)\n    print(\"\u2713 Reference model loaded\")\nelse:\n    print(\"\u2297 Reference model not used\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Load Reward Model (optional)\nreward_model = None\nif config.use_reward_model and config.reward_model_id:\n    print(\"=== Loading Reward Model ===\")\n    reward_model = AutoModelForSequenceClassification.from_pretrained(\n        config.reward_model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n    )\n    reward_model.eval()\n    for param in reward_model.parameters():\n        param.requires_grad_(False)\n    print(\"\u2713 Reward model loaded\")\nelse:\n    print(\"\u2297 Reward model not used (will use rule-based rewards)\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udcac Chat Template"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Prompt Formatting\ndef format_prompt(prompt: str) -> str:\n    if config.use_chat_template:\n        messages = []\n        if config.system_prompt:\n            messages.append({\"role\": \"system\", \"content\": config.system_prompt})\n        messages.append({\"role\": \"user\", \"content\": prompt})\n        if hasattr(tokenizer, 'apply_chat_template'):\n            return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n        else:\n            return f\"User: {prompt}\\n\\nAssistant:\"\n    else:\n        return config.prompt_template.format(prompt=prompt)\n\n# Test\ntest_prompt = format_prompt(\"What is 2+2?\")\nprint(\"=== Prompt Formatting ===\")\nprint(f\"Example:\\n{test_prompt[:200]}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udee0\ufe0f Tools & Environment"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Tools Configuration\ntools = {}\nif config.use_tools:\n    # TODO: Add your tools here\n    # Example: tools[\"calculator\"] = calculator_function\n    print(f\"\u2713 Tools loaded: {list(tools.keys())}\")\nelse:\n    print(\"\u2297 Tools not used\")\n\nenvironment = None\nif config.use_environment:\n    # TODO: Initialize environment here\n    print(\"\u2713 Environment initialized\")\nelse:\n    print(\"\u2297 Environment not used\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83c\udf81 Reward Function\n\n**IMPORTANT: Customize this for your task!**"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Reward Function\nimport re\n\ndef compute_reward(prompt: str, response: str, ground_truth: Optional[str] = None) -> float:\n    \"\"\"\n    Compute reward for a generated response.\n    \n    **TODO: CUSTOMIZE THIS FOR YOUR TASK!**\n    \n    This is a placeholder. Replace with your actual reward logic:\n    - Call a learned reward model\n    - Rule-based scoring (length, format, keywords)\n    - Verifiable correctness (math, code execution)\n    - Task-specific metrics (BLEU, ROUGE, exact match)\n    \"\"\"\n    if config.reward_type == \"model\" and reward_model is not None:\n        # Use reward model\n        with torch.no_grad():\n            inputs = tokenizer(prompt + response, return_tensors=\"pt\", truncation=True, max_length=1024).to(reward_model.device)\n            outputs = reward_model(**inputs)\n            reward = outputs.logits[0, -1].item()\n        return reward\n    \n    elif config.reward_type == \"rule\":\n        # Rule-based reward (CUSTOMIZE THIS)\n        reward = 0.0\n        # Example: check for answer format\n        if re.search(r\"\\[.*?\\]\", response):\n            reward += 0.5\n        # Example: length penalty\n        if len(response.split()) > 500:\n            reward -= 0.1\n        return reward\n    \n    else:\n        # Custom reward (TODO: implement)\n        return 0.0\n\n# Test\ntest_reward = compute_reward(\"What is 2+2?\", \"The answer is [4].\")\nprint(f\"=== Reward Function ===\")\nprint(f\"Example reward: {test_reward:.4f}\")\nprint(\"\\n\u26a0 WARNING: Placeholder reward function!\")\nprint(\"   Customize compute_reward() for your task.\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udcca Data Loading"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Load Dataset\nfrom datasets import load_dataset\n\nprint(\"=== Loading Dataset ===\")\nif config.dataset_config:\n    dataset = load_dataset(config.dataset_name, config.dataset_config, split=config.dataset_split)\nelse:\n    dataset = load_dataset(config.dataset_name, split=config.dataset_split)\n\n# Split into train/val\nval_size = min(config.val_size, len(dataset))\nval_dataset = dataset.select(range(val_size))\ntrain_dataset = dataset.select(range(val_size, len(dataset)))\n\nprint(f\"Train: {len(train_dataset):,} | Val: {len(val_dataset):,}\")\n\n# Prepare prompts\ntrain_prompts = [format_prompt(ex[config.prompt_field]) for ex in train_dataset]\nval_prompts = [format_prompt(ex[config.prompt_field]) for ex in val_dataset]\n\nprint(f\"\\n\u2713 Prompts prepared\")\nprint(f\"Example:\\n{train_prompts[0][:200]}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udd27 Training Utilities"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Training Utilities\n\ndef mask_after_eos(token_ids: torch.Tensor, eos_id: int) -> torch.Tensor:\n    \"\"\"Create mask for tokens before first EOS.\"\"\"\n    is_eos = (token_ids == eos_id)\n    eos_positions = is_eos.cumsum(dim=1)\n    return (eos_positions == 0).float()\n\ndef compute_model_logprobs(model, input_ids, attention_mask, target_ids, micro_batch_size=8):\n    \"\"\"Compute log probabilities of target tokens.\"\"\"\n    batch_size = input_ids.size(0)\n    target_len = target_ids.size(1)\n    \n    full_ids = torch.cat([input_ids, target_ids], dim=1)\n    full_mask = torch.cat([attention_mask, torch.ones_like(target_ids)], dim=1)\n    \n    if micro_batch_size >= batch_size:\n        outputs = model(input_ids=full_ids[:, :-1], attention_mask=full_mask[:, :-1])\n        logits = outputs.logits[:, -target_len:, :]\n        logprobs = F.log_softmax(logits, dim=-1)\n        token_logprobs = logprobs.gather(-1, target_ids.unsqueeze(-1)).squeeze(-1)\n    else:\n        # Micro-batching for memory efficiency\n        token_logprobs_list = []\n        for i in range(0, batch_size, micro_batch_size):\n            micro_full_ids = full_ids[i:i+micro_batch_size]\n            micro_full_mask = full_mask[i:i+micro_batch_size]\n            micro_target_ids = target_ids[i:i+micro_batch_size]\n            \n            outputs = model(input_ids=micro_full_ids[:, :-1], attention_mask=micro_full_mask[:, :-1])\n            logits = outputs.logits[:, -target_len:, :]\n            logprobs = F.log_softmax(logits, dim=-1)\n            micro_token_logprobs = logprobs.gather(-1, micro_target_ids.unsqueeze(-1)).squeeze(-1)\n            token_logprobs_list.append(micro_token_logprobs)\n        token_logprobs = torch.cat(token_logprobs_list, dim=0)\n    \n    return token_logprobs\n\nclass MetricsTracker:\n    \"\"\"Track and display training metrics with EMA smoothing.\"\"\"\n    \n    def __init__(self, ema_momentum=0.9):\n        self.ema_momentum = ema_momentum\n        self.metrics = []\n        self.ema_values = {}\n        empty_df = pd.DataFrame(columns=[\"step\", \"loss\", \"loss_ema\", \"reward\", \"reward_ema\", \"kl\", \"tokens\"])\n        self.display_handle = display(empty_df, display_id=True)\n    \n    def update_ema(self, key, value):\n        if key not in self.ema_values:\n            self.ema_values[key] = value\n        else:\n            self.ema_values[key] = self.ema_momentum * self.ema_values[key] + (1 - self.ema_momentum) * value\n        return self.ema_values[key]\n    \n    def log(self, step, loss, reward, kl=0.0, tokens=0):\n        loss_ema = self.update_ema(\"loss\", loss)\n        reward_ema = self.update_ema(\"reward\", reward)\n        \n        metric = {\n            \"step\": step, \"loss\": loss, \"loss_ema\": loss_ema,\n            \"reward\": reward, \"reward_ema\": reward_ema, \"kl\": kl, \"tokens\": tokens\n        }\n        self.metrics.append(metric)\n        \n        df = pd.DataFrame(self.metrics[-100:])\n        self.display_handle.update(df.style.format({\n            \"loss\": \"{:.4f}\", \"loss_ema\": \"{:.4f}\",\n            \"reward\": \"{:.4f}\", \"reward_ema\": \"{:.4f}\", \"kl\": \"{:.4f}\"\n        }))\n        \n        return metric\n\nprint(\"\u2713 Training utilities defined\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udcc8 Initialize WandB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Initialize WandB Run\nwandb_run = wandb.init(\n    project=config.wandb_project,\n    name=config.wandb_run_name,\n    config=config.__dict__,\n    job_type=\"training\"\n)\n\nwandb.watch(policy_model, log=\"all\", log_freq=100)\n\nprint(f\"\u2713 WandB initialized\")\nprint(f\"  Run: {config.wandb_run_name}\")\nprint(f\"  URL: {wandb_run.get_url()}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\ude80 Training Loop"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Setup Optimizer\noptimizer = torch.optim.AdamW(\n    policy_model.parameters(),\n    lr=config.learning_rate,\n    weight_decay=config.weight_decay\n)\nprint(f\"\u2713 Optimizer initialized (lr={config.learning_rate})\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Training Loop\nmetrics_tracker = MetricsTracker(ema_momentum=config.ema_momentum)\ntotal_tokens = 0\n\nprint(\"=\" * 80)\nprint(\"STARTING TRAINING\")\nprint(\"=\" * 80)\n\npbar = tqdm(range(config.num_steps), desc=\"Training\")\n\nfor step in pbar:\n    # Sample batch of prompts\n    rng = np.random.default_rng(config.seed + step)\n    prompt_indices = rng.choice(len(train_prompts), size=config.batch_size, replace=False)\n    \n    # Repeat each prompt for multiple samples\n    batch_prompts = []\n    for idx in prompt_indices:\n        batch_prompts.extend([train_prompts[idx]] * config.samples_per_prompt)\n    \n    # Tokenize\n    prompt_encoding = tokenizer(\n        batch_prompts, padding=True, truncation=True, max_length=2048, return_tensors=\"pt\"\n    ).to(DEVICE)\n    \n    # Generate responses (on-policy)\n    policy_model.eval()\n    policy_model.config.use_cache = True\n    with torch.no_grad():\n        generation_output = policy_model.generate(\n            **prompt_encoding, do_sample=True, temperature=config.train_temperature,\n            top_p=config.top_p, max_new_tokens=config.max_new_tokens,\n            pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id\n        )\n        full_sequences = generation_output\n        generated_ids = full_sequences[:, prompt_encoding.input_ids.size(1):]\n    \n    # Create validity mask\n    valid_mask = mask_after_eos(generated_ids, tokenizer.eos_token_id)\n    \n    # Compute rewards\n    with torch.no_grad():\n        generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n        rewards = [compute_reward(prompt, response) for prompt, response in zip(batch_prompts, generated_texts)]\n        rewards_tensor = torch.tensor(rewards, device=DEVICE).unsqueeze(1)\n        mean_reward = rewards_tensor.mean().item()\n    \n    # Compute policy log probs (with gradient)\n    policy_model.train()\n    policy_model.config.use_cache = False\n    policy_logprobs = compute_model_logprobs(\n        policy_model, prompt_encoding.input_ids, prompt_encoding.attention_mask, generated_ids\n    )\n    \n    # Compute KL penalty (if using reference model)\n    kl_penalty = 0.0\n    if config.use_kl_penalty and reference_model is not None:\n        with torch.no_grad():\n            reference_logprobs = compute_model_logprobs(\n                reference_model, prompt_encoding.input_ids, prompt_encoding.attention_mask, generated_ids\n            )\n            kl_per_token = policy_logprobs.detach() - reference_logprobs\n            kl_penalty = (kl_per_token * valid_mask).sum() / valid_mask.sum().clamp(min=1.0)\n    \n    # Policy gradient loss\n    rewards_broadcast = rewards_tensor.expand_as(policy_logprobs)\n    baseline = rewards_tensor.mean()\n    advantages = rewards_broadcast - baseline\n    \n    if config.use_kl_penalty:\n        advantages = advantages - config.kl_coef * (kl_penalty if isinstance(kl_penalty, float) else kl_penalty.item())\n    \n    loss_per_token = -advantages.detach() * policy_logprobs * valid_mask\n    loss = loss_per_token.sum() / valid_mask.sum().clamp(min=1.0)\n    \n    # Backward pass\n    loss.backward()\n    if config.max_grad_norm > 0:\n        torch.nn.utils.clip_grad_norm_(policy_model.parameters(), config.max_grad_norm)\n    \n    if (step + 1) % config.grad_accumulation_steps == 0:\n        optimizer.step()\n        optimizer.zero_grad(set_to_none=True)\n    \n    # Logging\n    total_tokens += int(valid_mask.sum().item())\n    metric = metrics_tracker.log(\n        step=step, loss=loss.item(), reward=mean_reward,\n        kl=kl_penalty if isinstance(kl_penalty, float) else kl_penalty.item(),\n        tokens=total_tokens\n    )\n    wandb.log(metric, step=step)\n    \n    pbar.set_postfix({\n        \"loss\": f\"{loss.item():.3f}\",\n        \"reward\": f\"{mean_reward:.3f}\",\n        \"kl\": f\"{kl_penalty if isinstance(kl_penalty, float) else kl_penalty.item():.3f}\"\n    })\n    \n    # Checkpointing\n    if (step % config.save_every == 0 and step > 0) or (step == config.num_steps - 1):\n        checkpoint_dir = os.path.join(config.output_dir, f\"checkpoint-{step}\")\n        os.makedirs(checkpoint_dir, exist_ok=True)\n        policy_model.save_pretrained(checkpoint_dir)\n        tokenizer.save_pretrained(checkpoint_dir)\n        print(f\"\\n\u2713 Checkpoint saved: {checkpoint_dir}\")\n        \n        artifact = wandb.Artifact(name=f\"model-checkpoint-{step}\", type=\"model\")\n        artifact.add_dir(checkpoint_dir)\n        wandb.log_artifact(artifact)\n    \n    torch.cuda.empty_cache()\n\nprint(\"=\" * 80)\nprint(\"TRAINING COMPLETE\")\nprint(\"=\" * 80)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udcbe Save Final Model"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Save Final Model\nfinal_dir = os.path.join(config.output_dir, \"final_model\")\nos.makedirs(final_dir, exist_ok=True)\n\npolicy_model.save_pretrained(final_dir)\ntokenizer.save_pretrained(final_dir)\nprint(f\"\u2713 Final model saved to: {final_dir}\")\n\n# Merge LoRA if used\nif config.use_lora:\n    print(\"Merging LoRA adapters...\")\n    merged_model = policy_model.merge_and_unload()\n    merged_dir = os.path.join(config.output_dir, \"final_model_merged\")\n    os.makedirs(merged_dir, exist_ok=True)\n    merged_model.save_pretrained(merged_dir)\n    tokenizer.save_pretrained(merged_dir)\n    print(f\"\u2713 Merged model saved to: {merged_dir}\")\n\n# Push to Hub if configured\nif config.push_to_hub and config.hub_repo_id:\n    print(f\"Pushing to Hub: {config.hub_repo_id}\")\n    model_to_push = merged_model if config.use_lora else policy_model\n    model_to_push.push_to_hub(repo_id=config.hub_repo_id, private=True)\n    tokenizer.push_to_hub(repo_id=config.hub_repo_id, private=True)\n    print(f\"\u2713 Pushed to Hub\")\n\n# Log to WandB\nfinal_artifact = wandb.Artifact(name=\"final-model\", type=\"model\")\nfinal_artifact.add_dir(final_dir)\nwandb.log_artifact(final_artifact)\nprint(f\"\u2713 Logged to WandB: {wandb_run.get_url()}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udcca Results Summary"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Training Summary\n# Save metrics\nmetrics_df = pd.DataFrame(metrics_tracker.metrics)\nmetrics_df.to_csv(os.path.join(config.output_dir, \"training_metrics.csv\"), index=False)\n\n# Summary\nsummary = {\n    \"final_loss\": metrics_tracker.metrics[-1][\"loss\"],\n    \"final_loss_ema\": metrics_tracker.metrics[-1][\"loss_ema\"],\n    \"final_reward\": metrics_tracker.metrics[-1][\"reward\"],\n    \"final_reward_ema\": metrics_tracker.metrics[-1][\"reward_ema\"],\n    \"total_tokens\": total_tokens,\n    \"total_steps\": config.num_steps\n}\n\nwith open(os.path.join(config.output_dir, \"summary.json\"), \"w\") as f:\n    json.dump(summary, f, indent=2)\n\nwandb.summary.update(summary)\n\nprint(\"=\" * 80)\nprint(\"TRAINING SUMMARY\")\nprint(\"=\" * 80)\nprint(f\"Final Loss: {summary['final_loss']:.4f}\")\nprint(f\"Final Loss (EMA): {summary['final_loss_ema']:.4f}\")\nprint(f\"Final Reward: {summary['final_reward']:.4f}\")\nprint(f\"Final Reward (EMA): {summary['final_reward_ema']:.4f}\")\nprint(f\"Total Tokens: {summary['total_tokens']:,}\")\nprint(f\"\\nOutput: {config.output_dir}\")\nprint(f\"WandB: {wandb_run.get_url()}\")\nprint(\"=\" * 80)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title Finish WandB Run\nwandb.finish()\nprint(\"\u2713 WandB run finished\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83c\udf89 Done!\n\nYour RL training is complete. You can now:\n\n1. **Evaluate your model** on test data\n2. **Inspect the metrics** in WandB dashboard\n3. **Load the trained model** from the checkpoints\n4. **Customize** any section for your specific use case\n\n### Next Steps:\n\n- **Customize the reward function** in the \"Reward Function\" section\n- **Add validation metrics** specific to your task\n- **Implement advanced RL algorithms** (PPO, DPO, etc.)\n- **Tune hyperparameters** at the top of the notebook\n\n### Common Modifications:\n\n1. **Change model**: Edit `config.policy_model_id`\n2. **Change dataset**: Edit `config.dataset_name` and fields\n3. **Change algorithm**: Modify loss computation in training loop\n4. **Use reward model**: Set `config.use_reward_model=True`"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}