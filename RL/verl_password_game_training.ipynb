{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Verl PPO Training - Password Game\n",
        "\n",
        "This notebook implements PPO training for the **multi-turn password game** where:\n",
        "- Rules accumulate (each turn adds a new constraint)\n",
        "- Model must satisfy ALL previous rules + current rule\n",
        "- Reward shaped to encourage progress while minimizing password length\n",
        "- Single H100, no DDP\n",
        "- Qwen3-0.6B with thinking mode support\n",
        "\n",
        "## Key Features\n",
        "\n",
        "1. **Multi-turn Rollouts**: Handles cumulative rule nature\n",
        "2. **Shaped Rewards**: Progress bonus + length penalty + success bonus\n",
        "3. **Episode-based**: Full game runs as episodes\n",
        "4. **Thinking Mode**: Qwen's thinking capabilities for complex reasoning\n",
        "5. **Adaptive KL**: Dynamic KL coefficient adjustment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ› ï¸ Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch==2.4.0 torchvision==0.19.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install -q flash-attn --no-build-isolation\n",
        "!pip install -q transformers>=4.45.0 accelerate datasets tokenizers\n",
        "!pip install -q wandb tensorboard pandas matplotlib tqdm requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('/home/user/notebooks/RL')\n",
        "\n",
        "# Verify password game is accessible\n",
        "sys.path.append('/home/user/notebooks/tasks/password-game')\n",
        "\n",
        "import torch\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”‘ API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import keys\n",
        "try:\n",
        "    from keys import WANDB_API_KEY, HF_TOKEN, OPENAI_API_KEY\n",
        "    if WANDB_API_KEY:\n",
        "        os.environ['WANDB_API_KEY'] = WANDB_API_KEY\n",
        "    if HF_TOKEN:\n",
        "        os.environ['HF_TOKEN'] = HF_TOKEN\n",
        "    if OPENAI_API_KEY:\n",
        "        os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
        "    print(\"âœ“ Keys imported\")\n",
        "except ImportError:\n",
        "    print(\"âš ï¸ keys.py not found - some features may not work\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Login to services\n",
        "try:\n",
        "    wandb.login()\n",
        "    print(\"âœ“ WandB login successful\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ WandB: {e}\")\n",
        "\n",
        "try:\n",
        "    if os.environ.get('HF_TOKEN'):\n",
        "        login(token=os.environ['HF_TOKEN'])\n",
        "        print(\"âœ“ HuggingFace login successful\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ HF: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§ª Test Password Game Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from game import PasswordGame\n",
        "\n",
        "# Test the password game\n",
        "game = PasswordGame()\n",
        "print(\"Password Game Test\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Current rule: {game.get_current_rule()}\")\n",
        "print(f\"\\nGame state: {game.game_active}\")\n",
        "print(f\"\\nDynamic values:\")\n",
        "print(f\"  Captcha: {game.captcha}\")\n",
        "print(f\"  Country: {game.country}\")\n",
        "print(f\"  Wordle: {game.wordle_answer}\")\n",
        "print(f\"  Moon: {game.moon_phase}\")\n",
        "\n",
        "# Test reward calculation\n",
        "test_password = \"Hello123!\"\n",
        "feedback = game.get_rule_feedback(test_password)\n",
        "print(f\"\\nTest password: {test_password}\")\n",
        "print(f\"Rules passing: {feedback['total_passing']} / {len(feedback['rules_checked'])}\")\n",
        "print(f\"Reward: {feedback['reward']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš™ï¸ Configuration\n",
        "\n",
        "Adjust these settings for your training run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from verl_password_game_ppo import PasswordGamePPOConfig\n",
        "import time\n",
        "\n",
        "# Create config\n",
        "config = PasswordGamePPOConfig(\n",
        "    # Model\n",
        "    model_name=\"Qwen/Qwen2.5-0.6B\",\n",
        "    precision=\"bfloat16\",\n",
        "    use_flash_attn=True,\n",
        "    \n",
        "    # Training\n",
        "    num_epochs=5,\n",
        "    num_episodes_per_epoch=100,\n",
        "    batch_size=4,\n",
        "    samples_per_state=2,\n",
        "    learning_rate=5e-7,\n",
        "    \n",
        "    # PPO\n",
        "    ppo_epochs=4,\n",
        "    clip_range=0.2,\n",
        "    value_loss_coef=0.5,\n",
        "    kl_coef=0.1,\n",
        "    gamma=0.99,\n",
        "    \n",
        "    # Thinking mode\n",
        "    enable_thinking=True,\n",
        "    thinking_temperature=0.7,\n",
        "    parse_thinking=True,\n",
        "    \n",
        "    # Password game\n",
        "    max_turns_per_episode=10,\n",
        "    reward_scale=1.0,\n",
        "    length_penalty_scale=0.1,\n",
        "    progress_bonus=2.0,\n",
        "    final_success_bonus=10.0,\n",
        "    \n",
        "    # Logging\n",
        "    wandb_project=\"verl-password-game\",\n",
        "    wandb_run_name=f\"ppo_qwen_thinking_{int(time.time())}\",\n",
        "    log_interval=5,\n",
        "    eval_interval=25,\n",
        "    save_interval=50,\n",
        "    output_dir=f\"./verl_password_game_{int(time.time())}\",\n",
        "    \n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Model: {config.model_name}\")\n",
        "print(f\"  Thinking mode: {config.enable_thinking}\")\n",
        "print(f\"  Episodes/epoch: {config.num_episodes_per_epoch}\")\n",
        "print(f\"  Batch size: {config.batch_size}\")\n",
        "print(f\"  Learning rate: {config.learning_rate}\")\n",
        "print(f\"  Output: {config.output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸš€ Run Training\n",
        "\n",
        "**Note**: This will take several hours depending on your configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from verl_password_game_ppo import train_ppo_password_game\n",
        "\n",
        "# Start training\n",
        "train_ppo_password_game(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š Analyze Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load summary\n",
        "summary_path = os.path.join(config.output_dir, \"summary.json\")\n",
        "if os.path.exists(summary_path):\n",
        "    with open(summary_path) as f:\n",
        "        summary = json.load(f)\n",
        "    \n",
        "    print(\"Training Summary\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Baseline reward: {summary['baseline']['mean_reward']:.4f}\")\n",
        "    print(f\"Final reward: {summary['final']['mean_reward']:.4f}\")\n",
        "    print(f\"Improvement: {summary['improvement']:.4f}\")\n",
        "    print(f\"\\nBaseline rules: {summary['baseline']['mean_rules']:.2f}\")\n",
        "    print(f\"Final rules: {summary['final']['mean_rules']:.2f}\")\n",
        "    print(f\"\\nFinal success rate: {summary['final']['success_rate']:.2%}\")\n",
        "else:\n",
        "    print(\"Summary not found. Training may not be complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§ª Test Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from verl_password_game_ppo import PasswordGameEnv, extract_password_from_response, parse_thinking_response\n",
        "\n",
        "# Load best model\n",
        "best_model_path = os.path.join(config.output_dir, \"best_model\")\n",
        "\n",
        "if os.path.exists(best_model_path):\n",
        "    print(f\"Loading model from {best_model_path}...\")\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(best_model_path, trust_remote_code=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        best_model_path,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        attn_implementation=\"flash_attention_2\"\n",
        "    )\n",
        "    model.eval()\n",
        "    model.config.use_cache = True\n",
        "    \n",
        "    print(\"âœ“ Model loaded\")\n",
        "else:\n",
        "    print(f\"Model not found at {best_model_path}\")\n",
        "    print(\"Using untrained model for demonstration...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config.model_name, trust_remote_code=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        config.model_name,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run interactive game\n",
        "def play_game_interactive(model, tokenizer, config, max_turns=10):\n",
        "    \"\"\"Play one game and show step-by-step results.\"\"\"\n",
        "    env = PasswordGameEnv(tokenizer, config)\n",
        "    prompt, info = env.reset()\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(\"PASSWORD GAME - INTERACTIVE PLAYTHROUGH\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    gen_config = {\n",
        "        \"max_new_tokens\": config.max_new_tokens,\n",
        "        \"do_sample\": True,\n",
        "        \"temperature\": 0.7,\n",
        "        \"top_p\": 0.9,\n",
        "        \"pad_token_id\": tokenizer.pad_token_id,\n",
        "        \"eos_token_id\": tokenizer.eos_token_id,\n",
        "    }\n",
        "    \n",
        "    total_reward = 0\n",
        "    \n",
        "    for turn in range(max_turns):\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"TURN {turn + 1}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        # Show current rules\n",
        "        rules = env.game.get_all_rules_up_to_current()\n",
        "        print(f\"\\nRules to satisfy ({len(rules)}):\")\n",
        "        for i, rule in enumerate(rules, 1):\n",
        "            print(f\"  {i}. {rule}\")\n",
        "        \n",
        "        # Generate response\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(model.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, **gen_config)\n",
        "        \n",
        "        generated_ids = outputs[:, inputs.input_ids.size(1):]\n",
        "        \n",
        "        if config.enable_thinking:\n",
        "            thinking, response = parse_thinking_response(generated_ids[0], tokenizer)\n",
        "            if thinking:\n",
        "                print(f\"\\nðŸ’­ Thinking: {thinking[:200]}...\" if len(thinking) > 200 else f\"\\nðŸ’­ Thinking: {thinking}\")\n",
        "        else:\n",
        "            response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "        \n",
        "        password = extract_password_from_response(response)\n",
        "        print(f\"\\nðŸ”‘ Password: {password}\")\n",
        "        print(f\"   Length: {len(password)}\")\n",
        "        \n",
        "        # Step environment\n",
        "        next_prompt, reward, done, step_info = env.step(response)\n",
        "        total_reward += reward\n",
        "        \n",
        "        print(f\"\\nðŸ“Š Feedback:\")\n",
        "        print(f\"   Rules passing: {step_info['rules_passing']} / {step_info['current_rule_index'] + 1}\")\n",
        "        print(f\"   Reward: {reward:.2f}\")\n",
        "        print(f\"   Total reward: {total_reward:.2f}\")\n",
        "        \n",
        "        # Show which rules pass/fail\n",
        "        feedback = step_info['feedback']\n",
        "        for rule_info in feedback['rules_checked']:\n",
        "            status = \"âœ“\" if rule_info['passes'] else \"âœ—\"\n",
        "            print(f\"   {status} Rule {rule_info['rule_index'] + 1}\")\n",
        "        \n",
        "        if done:\n",
        "            print(f\"\\n{'='*80}\")\n",
        "            print(\"GAME OVER\")\n",
        "            print(f\"{'='*80}\")\n",
        "            print(f\"Final turn: {turn + 1}\")\n",
        "            print(f\"Rules satisfied: {step_info['rules_passing']}\")\n",
        "            print(f\"Total reward: {total_reward:.2f}\")\n",
        "            print(f\"Final password: {password}\")\n",
        "            print(f\"Final length: {len(password)}\")\n",
        "            break\n",
        "        \n",
        "        prompt = next_prompt\n",
        "    \n",
        "    return total_reward, step_info['rules_passing']\n",
        "\n",
        "# Play one game\n",
        "reward, rules = play_game_interactive(model, tokenizer, config, max_turns=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¬ Batch Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from verl_password_game_ppo import evaluate_policy\n",
        "\n",
        "# Evaluate on multiple episodes\n",
        "print(\"Running batch evaluation...\")\n",
        "eval_results = evaluate_policy(\n",
        "    policy_model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    config=config,\n",
        "    num_episodes=20,\n",
        "    desc=\"Batch Eval\"\n",
        ")\n",
        "\n",
        "print(\"\\nBatch Evaluation Results:\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Mean reward: {eval_results['mean_reward']:.4f} Â± {eval_results['std_reward']:.4f}\")\n",
        "print(f\"Mean rules satisfied: {eval_results['mean_rules']:.2f}\")\n",
        "print(f\"Max rules satisfied: {eval_results['max_rules']}\")\n",
        "print(f\"Success rate (â‰¥5 rules): {eval_results['success_rate']:.2%}\")\n",
        "print(f\"Episodes evaluated: {eval_results['num_episodes']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“ˆ Visualize Training Progress (from WandB)\n",
        "\n",
        "If you have WandB data, you can analyze it here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell requires wandb API\n",
        "# Uncomment and modify if you want to pull data from WandB\n",
        "\n",
        "# import wandb\n",
        "# api = wandb.Api()\n",
        "# run = api.run(f\"{config.wandb_project}/{config.wandb_run_name}\")\n",
        "# history = run.history()\n",
        "# \n",
        "# # Plot training metrics\n",
        "# fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "# \n",
        "# axes[0, 0].plot(history['train/reward'])\n",
        "# axes[0, 0].set_title('Training Reward')\n",
        "# axes[0, 0].set_xlabel('Step')\n",
        "# axes[0, 0].set_ylabel('Reward')\n",
        "# \n",
        "# axes[0, 1].plot(history['train/rules_satisfied'])\n",
        "# axes[0, 1].set_title('Rules Satisfied')\n",
        "# axes[0, 1].set_xlabel('Step')\n",
        "# axes[0, 1].set_ylabel('Rules')\n",
        "# \n",
        "# axes[1, 0].plot(history['train/kl_div'])\n",
        "# axes[1, 0].set_title('KL Divergence')\n",
        "# axes[1, 0].set_xlabel('Step')\n",
        "# axes[1, 0].set_ylabel('KL')\n",
        "# \n",
        "# axes[1, 1].plot(history['train/loss'])\n",
        "# axes[1, 1].set_title('Total Loss')\n",
        "# axes[1, 1].set_xlabel('Step')\n",
        "# axes[1, 1].set_ylabel('Loss')\n",
        "# \n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
