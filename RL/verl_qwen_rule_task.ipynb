{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Custom PPO Training: Qwen 0.6B on Rule-Based Task\n\nThis notebook implements from-scratch PPO training for Qwen2.5-0.6B on a rule-based task.\n\n**Task**: Environment provides a rule â†’ model generates output â†’ environment provides output as context for next rule.\n\n**Features**:\n- Custom PPO implementation (verl-inspired patterns)\n- Qwen 0.6B with thinking mode enabled\n- Full parameter training (NO LoRA)\n- Rule-based task environment\n- Baseline and full evaluation\n- WandB integration\n\n**Note**: This is a custom PPO implementation, not using verl's trainer directly."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi -L || true\n",
    "\n",
    "import sys\n",
    "print(\"Python:\", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies (no verl - custom PPO implementation)\n!pip install -q torch==2.4.0 torchvision==0.19.0 --index-url https://download.pytorch.org/whl/cu121\n!pip install -q flash-attn --no-build-isolation\n!pip install -q transformers>=4.45.0 accelerate datasets tokenizers\n!pip install -q wandb tensorboard pandas matplotlib tqdm"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random, time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Any, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "assert DEVICE == \"cuda\", \"GPU required for training\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”‘ Import API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import keys from keys.py\n",
    "try:\n",
    "    from keys import WANDB_API_KEY, HF_TOKEN\n",
    "    print(\"âœ“ Keys imported\")\n",
    "    if WANDB_API_KEY:\n",
    "        os.environ['WANDB_API_KEY'] = WANDB_API_KEY\n",
    "    if HF_TOKEN:\n",
    "        os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "except ImportError:\n",
    "    print(\"âš  keys.py not found\")\n",
    "    WANDB_API_KEY = \"\"\n",
    "    HF_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "try:\n",
    "    wandb.login()\n",
    "    print(\"âœ“ WandB login successful\")\n",
    "except Exception as e:\n",
    "    print(f\"âš  WandB: {e}\")\n",
    "\n",
    "try:\n",
    "    if os.environ.get('HF_TOKEN'):\n",
    "        login(token=os.environ['HF_TOKEN'])\n",
    "        print(\"âœ“ HuggingFace login successful\")\n",
    "except Exception as e:\n",
    "    print(f\"âš  HF: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class VerlPPOConfig:\n",
    "    # Model\n",
    "    model_name: str = \"Qwen/Qwen2.5-0.6B\"\n",
    "    precision: str = \"bfloat16\"\n",
    "    use_flash_attn: bool = True\n",
    "    \n",
    "    # Training\n",
    "    num_epochs: int = 3\n",
    "    num_steps_per_epoch: int = 100\n",
    "    batch_size: int = 4\n",
    "    samples_per_prompt: int = 4\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    learning_rate: float = 1e-6\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "    warmup_steps: int = 50\n",
    "    \n",
    "    # PPO\n",
    "    ppo_epochs: int = 4\n",
    "    clip_range: float = 0.2\n",
    "    value_loss_coef: float = 0.1\n",
    "    entropy_coef: float = 0.01\n",
    "    kl_coef: float = 0.1\n",
    "    gamma: float = 1.0\n",
    "    gae_lambda: float = 0.95\n",
    "    normalize_advantages: bool = True\n",
    "    \n",
    "    # Generation\n",
    "    max_prompt_length: int = 512\n",
    "    max_new_tokens: int = 512\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.9\n",
    "    top_k: int = 50\n",
    "    \n",
    "    # Thinking mode (Qwen-specific)\n",
    "    enable_thinking: bool = True\n",
    "    thinking_temperature: float = 0.6\n",
    "    thinking_top_p: float = 0.95\n",
    "    parse_thinking: bool = True\n",
    "    \n",
    "    # Data\n",
    "    num_train_samples: int = 1000\n",
    "    num_val_samples: int = 200\n",
    "    \n",
    "    # Logging\n",
    "    wandb_project: str = \"verl-qwen-rule-task\"\n",
    "    wandb_run_name: Optional[str] = None\n",
    "    log_interval: int = 10\n",
    "    eval_interval: int = 50\n",
    "    save_interval: int = 100\n",
    "    output_dir: str = f\"./verl_qwen_run_{int(time.time())}\"\n",
    "    \n",
    "    seed: int = 42\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.wandb_run_name is None:\n",
    "            mode = \"thinking\" if self.enable_thinking else \"normal\"\n",
    "            self.wandb_run_name = f\"verl_ppo_qwen06b_{mode}_{int(time.time())}\"\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "config = VerlPPOConfig()\n",
    "\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Thinking mode: {config.enable_thinking}\")\n",
    "print(f\"Batch size: {config.batch_size} x {config.samples_per_prompt} = {config.batch_size * config.samples_per_prompt}\")\n",
    "print(f\"Output: {config.output_dir}\")\n",
    "\n",
    "with open(os.path.join(config.output_dir, \"config.json\"), \"w\") as f:\n",
    "    json.dump(asdict(config), f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(config.seed)\n",
    "print(f\"âœ“ Seed: {config.seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.bfloat16 if config.precision == \"bfloat16\" else (torch.float16 if config.precision == \"float16\" else torch.float32)\n",
    "print(f\"Using dtype: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "print(f\"âœ“ Tokenizer loaded (vocab: {len(tokenizer)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\" if config.use_flash_attn else \"eager\"\n",
    ")\n",
    "policy_model.config.use_cache = False\n",
    "print(f\"âœ“ Policy model: {sum(p.numel() for p in policy_model.parameters())/1e9:.2f}B params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\" if config.use_flash_attn else \"eager\"\n",
    ")\n",
    "reference_model.eval()\n",
    "for param in reference_model.parameters():\n",
    "    param.requires_grad = False\n",
    "print(\"âœ“ Reference model loaded and frozen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueHead(nn.Module):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "        nn.init.orthogonal_(self.linear.weight, gain=0.01)\n",
    "        nn.init.constant_(self.linear.bias, 0.0)\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        return self.linear(hidden_states)\n",
    "\n",
    "value_head = ValueHead(policy_model.config.hidden_size).to(DEVICE).to(dtype)\n",
    "print(f\"âœ“ Value head (hidden_size={policy_model.config.hidden_size})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒ Rule-Based Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = [\"fruits\", \"vegetables\", \"colors\", \"animals\", \"countries\", \"cities\", \"numbers\", \"programming languages\"]\n",
    "\n",
    "class RuleBasedEnvironment:\n",
    "    def __init__(self, seed: int = 42):\n",
    "        self.rng = random.Random(seed)\n",
    "    \n",
    "    def generate_rule(self, context: Optional[str] = None) -> str:\n",
    "        if context is None:\n",
    "            template = self.rng.choice([\"List {n} {category}\", \"Generate {n} random {category}\"])\n",
    "            n = self.rng.randint(3, 7)\n",
    "            category = self.rng.choice(CATEGORIES)\n",
    "            return template.format(n=n, category=category)\n",
    "        else:\n",
    "            template = self.rng.choice([\n",
    "                \"Convert to uppercase: {text}\",\n",
    "                \"Convert to lowercase: {text}\",\n",
    "                \"Reverse order: {text}\",\n",
    "                \"Sort alphabetically: {text}\",\n",
    "                \"Count items: {text}\"\n",
    "            ])\n",
    "            return template.format(text=context)\n",
    "    \n",
    "    def format_prompt(self, rule: str, enable_thinking: bool = False) -> str:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that follows rules precisely.\"},\n",
    "            {\"role\": \"user\", \"content\": rule}\n",
    "        ]\n",
    "        if hasattr(tokenizer, 'apply_chat_template'):\n",
    "            return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=enable_thinking)\n",
    "        return f\"System: You are a helpful assistant.\\n\\nUser: {rule}\\n\\nAssistant:\"\n",
    "\n",
    "env = RuleBasedEnvironment(seed=config.seed)\n",
    "print(\"âœ“ Environment created\")\n",
    "print(f\"Example: {env.generate_rule()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleDataset(Dataset):\n",
    "    def __init__(self, num_samples: int, environment, enable_thinking: bool = False, seed: int = 42):\n",
    "        self.environment = environment\n",
    "        self.enable_thinking = enable_thinking\n",
    "        rng = random.Random(seed)\n",
    "        self.prompts = []\n",
    "        self.rules = []\n",
    "        for _ in range(num_samples):\n",
    "            context = None\n",
    "            if rng.random() > 0.5:\n",
    "                context = \", \".join([f\"item{i}\" for i in range(rng.randint(3, 6))])\n",
    "            rule = environment.generate_rule(context=context)\n",
    "            prompt = environment.format_prompt(rule, enable_thinking=enable_thinking)\n",
    "            self.rules.append(rule)\n",
    "            self.prompts.append(prompt)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {'prompt': self.prompts[idx], 'rule': self.rules[idx]}\n",
    "\n",
    "train_dataset = RuleDataset(config.num_train_samples, env, config.enable_thinking, config.seed)\n",
    "val_dataset = RuleDataset(config.num_val_samples, env, config.enable_thinking, config.seed + 1000)\n",
    "print(f\"âœ“ Train: {len(train_dataset)}, Val: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ Reward Function\n",
    "\n",
    "**TODO: IMPLEMENT YOUR ACTUAL REWARD LOGIC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward(prompt: str, response: str, rule: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate reward - PLACEHOLDER IMPLEMENTATION\n",
    "    TODO: Implement your actual reward function here!\n",
    "    \"\"\"\n",
    "    reward = 0.0\n",
    "    words = response.split()\n",
    "    \n",
    "    if 5 <= len(words) <= 100:\n",
    "        reward += 0.3\n",
    "    if response.strip():\n",
    "        reward += 0.2\n",
    "    if \"uppercase\" in rule.lower() and any(c.isupper() for c in response):\n",
    "        reward += 0.5\n",
    "    elif \"lowercase\" in rule.lower() and any(c.islower() for c in response):\n",
    "        reward += 0.5\n",
    "    elif \"list\" in rule.lower() and \",\" in response:\n",
    "        reward += 0.5\n",
    "    elif \"count\" in rule.lower() and any(c.isdigit() for c in response):\n",
    "        reward += 0.5\n",
    "    \n",
    "    return reward\n",
    "\n",
    "print(\"âš  WARNING: Placeholder reward function!\")\n",
    "print(\"  MUST implement calculate_reward() for your task\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Thinking Mode Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QWEN_THINK_END_TOKEN = 151668\n",
    "\n",
    "def parse_thinking_response(output_ids, tokenizer) -> Tuple[str, str]:\n",
    "    if isinstance(output_ids, torch.Tensor):\n",
    "        output_ids = output_ids.tolist()\n",
    "    try:\n",
    "        index = len(output_ids) - output_ids[::-1].index(QWEN_THINK_END_TOKEN)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    thinking = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip()\n",
    "    response = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip()\n",
    "    return thinking, response\n",
    "\n",
    "def get_generation_config(config, mode=\"train\") -> Dict:\n",
    "    temp = config.thinking_temperature if config.enable_thinking else config.temperature\n",
    "    top_p = config.thinking_top_p if config.enable_thinking else config.top_p\n",
    "    return {\n",
    "        \"max_new_tokens\": config.max_new_tokens,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": temp,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": config.top_k,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    }\n",
    "\n",
    "print(\"âœ“ Thinking utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, num_samples=None, batch_size=8, desc=\"Eval\"):\n",
    "    model.eval()\n",
    "    model.config.use_cache = True\n",
    "    if num_samples is None:\n",
    "        num_samples = len(dataset)\n",
    "    else:\n",
    "        num_samples = min(num_samples, len(dataset))\n",
    "    \n",
    "    gen_config = get_generation_config(config, mode=\"eval\")\n",
    "    gen_config[\"use_cache\"] = True\n",
    "    total_reward = 0.0\n",
    "    all_rewards = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, num_samples, batch_size), desc=desc):\n",
    "            batch_end = min(i + batch_size, num_samples)\n",
    "            batch = [dataset[j] for j in range(i, batch_end)]\n",
    "            prompts = [item['prompt'] for item in batch]\n",
    "            rules = [item['rule'] for item in batch]\n",
    "            inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=config.max_prompt_length).to(DEVICE)\n",
    "            outputs = model.generate(**inputs, **gen_config)\n",
    "            generated_ids = outputs[:, inputs.input_ids.size(1):]\n",
    "            for gen_ids, prompt, rule in zip(generated_ids, prompts, rules):\n",
    "                if config.enable_thinking and config.parse_thinking:\n",
    "                    _, response = parse_thinking_response(gen_ids, tokenizer)\n",
    "                else:\n",
    "                    response = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "                reward = calculate_reward(prompt, response, rule)\n",
    "                all_rewards.append(reward)\n",
    "                total_reward += reward\n",
    "    \n",
    "    model.train()\n",
    "    return {\"mean_reward\": total_reward / num_samples, \"std_reward\": np.std(all_rewards), \"num_samples\": num_samples}\n",
    "\n",
    "print(\"âœ“ Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"BASELINE EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "baseline_metrics = evaluate_model(policy_model, val_dataset, num_samples=min(100, len(val_dataset)), desc=\"Baseline\")\n",
    "print(f\"Baseline: {baseline_metrics['mean_reward']:.4f} Â± {baseline_metrics['std_reward']:.4f}\")\n",
    "with open(os.path.join(config.output_dir, \"baseline.json\"), \"w\") as f:\n",
    "    json.dump(baseline_metrics, f, indent=2)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ PPO Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_probs(model, input_ids, attention_mask, return_values=False):\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=return_values)\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits[:, :-1, :], dim=-1)\n",
    "    token_log_probs = torch.gather(log_probs, dim=2, index=input_ids[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "    mask = attention_mask[:, 1:].bool()\n",
    "    token_log_probs = token_log_probs * mask\n",
    "    if return_values:\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "        values = value_head(hidden_states).squeeze(-1)\n",
    "        return token_log_probs, values\n",
    "    return token_log_probs\n",
    "\n",
    "def compute_advantages(rewards, values, masks, gamma=1.0, gae_lambda=0.95):\n",
    "    batch_size, seq_len = rewards.shape\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "    gae = 0\n",
    "    for t in reversed(range(seq_len)):\n",
    "        next_value = 0 if t == seq_len - 1 else values[:, t + 1]\n",
    "        delta = rewards[:, t] + gamma * next_value * masks[:, t] - values[:, t]\n",
    "        gae = delta + gamma * gae_lambda * masks[:, t] * gae\n",
    "        advantages[:, t] = gae\n",
    "    returns = advantages + values\n",
    "    return advantages, returns\n",
    "\n",
    "def whiten(values, mask, shift_mean=True):\n",
    "    mean = (values * mask).sum() / mask.sum()\n",
    "    var = ((values - mean) ** 2 * mask).sum() / mask.sum()\n",
    "    std = torch.sqrt(var + 1e-8)\n",
    "    return (values - mean) / std if shift_mean else values / std\n",
    "\n",
    "print(\"âœ“ PPO utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    list(policy_model.parameters()) + list(value_head.parameters()),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "total_steps = config.num_epochs * config.num_steps_per_epoch\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup_steps, num_training_steps=total_steps)\n",
    "print(f\"âœ“ Optimizer: LR={config.learning_rate}, Steps={total_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_run = wandb.init(project=config.wandb_project, name=config.wandb_run_name, config=asdict(config))\n",
    "print(f\"âœ“ WandB: {wandb_run.get_url()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ PPO Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*80)\nprint(\"STARTING CUSTOM PPO TRAINING\")\nprint(\"=\"*80)\n\npolicy_model.train()\nvalue_head.train()\ngen_config = get_generation_config(config, mode=\"train\")\nglobal_step = 0\nbest_val_reward = -float('inf')\ntrain_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n\nfor epoch in range(config.num_epochs):\n    print(f\"\\nEpoch {epoch+1}/{config.num_epochs}\")\n    epoch_iter = iter(train_dataloader)\n    \n    for step in tqdm(range(config.num_steps_per_epoch), desc=f\"Epoch {epoch+1}\"):\n        try:\n            batch = next(epoch_iter)\n        except StopIteration:\n            epoch_iter = iter(train_dataloader)\n            batch = next(epoch_iter)\n        \n        prompts = batch['prompt']\n        rules = batch['rule']\n        \n        # ROLLOUT\n        policy_model.eval()\n        policy_model.config.use_cache = True\n        with torch.no_grad():\n            prompt_inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=config.max_prompt_length).to(DEVICE)\n            all_responses = []\n            all_full_ids = []\n            all_masks = []\n            for _ in range(config.samples_per_prompt):\n                outputs = policy_model.generate(**prompt_inputs, **gen_config)\n                generated_ids = outputs[:, prompt_inputs.input_ids.size(1):]\n                responses = []\n                for gen_ids in generated_ids:\n                    if config.enable_thinking and config.parse_thinking:\n                        _, resp = parse_thinking_response(gen_ids, tokenizer)\n                    else:\n                        resp = tokenizer.decode(gen_ids, skip_special_tokens=True)\n                    responses.append(resp)\n                all_responses.extend(responses)\n                all_full_ids.append(outputs)\n                mask = torch.ones_like(outputs)\n                mask[outputs == tokenizer.pad_token_id] = 0\n                all_masks.append(mask)\n            all_full_ids = torch.cat(all_full_ids, dim=0)\n            all_masks = torch.cat(all_masks, dim=0)\n            expanded_prompts = prompts * config.samples_per_prompt\n            expanded_rules = rules * config.samples_per_prompt\n        \n        # REWARDS\n        rewards = torch.tensor([calculate_reward(p, r, rule) for p, r, rule in zip(expanded_prompts, all_responses, expanded_rules)], device=DEVICE, dtype=dtype)\n        mean_reward = rewards.mean().item()\n        \n        # OLD PROBS & VALUES\n        with torch.no_grad():\n            old_log_probs, old_values = compute_log_probs(policy_model, all_full_ids, all_masks, return_values=True)\n            ref_log_probs = compute_log_probs(reference_model, all_full_ids, all_masks)\n            prompt_len = prompt_inputs.input_ids.size(1)\n            old_values_gen = old_values[:, prompt_len:]\n            generated_ids_all = all_full_ids[:, prompt_len:]\n        \n        # ADVANTAGES\n        response_mask = (generated_ids_all != tokenizer.pad_token_id).float()\n        reward_per_token = torch.zeros_like(generated_ids_all, dtype=dtype)\n        for i, reward in enumerate(rewards):\n            valid = generated_ids_all[i] != tokenizer.pad_token_id\n            reward_per_token[i][valid] = reward / valid.sum().clamp(min=1)\n        advantages, returns = compute_advantages(reward_per_token, old_values_gen, response_mask, config.gamma, config.gae_lambda)\n        if config.normalize_advantages:\n            advantages = whiten(advantages, response_mask)\n        \n        # PPO UPDATES (Fixed: update after each epoch, not accumulated)\n        policy_model.train()\n        policy_model.config.use_cache = False\n        for ppo_epoch in range(config.ppo_epochs):\n            curr_log_probs, curr_values = compute_log_probs(policy_model, all_full_ids, all_masks, return_values=True)\n            curr_values_gen = curr_values[:, prompt_len:]\n            curr_lp_gen = curr_log_probs[:, prompt_len-1:]\n            old_lp_gen = old_log_probs[:, prompt_len-1:]\n            ref_lp_gen = ref_log_probs[:, prompt_len-1:]\n            \n            ratio = torch.exp(curr_lp_gen - old_lp_gen.detach())\n            policy_loss = torch.max(\n                -advantages.detach() * ratio,\n                -advantages.detach() * torch.clamp(ratio, 1-config.clip_range, 1+config.clip_range)\n            )\n            policy_loss = (policy_loss * response_mask).sum() / response_mask.sum()\n            value_loss = ((curr_values_gen - returns.detach())**2 * response_mask).sum() / response_mask.sum()\n            kl_penalty = ((curr_lp_gen - ref_lp_gen.detach()) * response_mask).sum() / response_mask.sum()\n            \n            # Total loss (removed entropy term - was incorrectly calculated)\n            loss = policy_loss + config.value_loss_coef * value_loss + config.kl_coef * kl_penalty\n            \n            # Backward and update after EACH PPO epoch\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(list(policy_model.parameters()) + list(value_head.parameters()), config.max_grad_norm)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n        \n        # LOGGING\n        if global_step % config.log_interval == 0:\n            wandb.log({\"step\": global_step, \"loss\": loss.item(), \"reward\": mean_reward, \"kl\": kl_penalty.item()}, step=global_step)\n        \n        # EVAL\n        if global_step % config.eval_interval == 0 and global_step > 0:\n            val_metrics = evaluate_model(policy_model, val_dataset, num_samples=50, desc=f\"Eval@{global_step}\")\n            wandb.log({\"val_reward\": val_metrics['mean_reward']}, step=global_step)\n            if val_metrics['mean_reward'] > best_val_reward:\n                best_val_reward = val_metrics['mean_reward']\n                best_dir = os.path.join(config.output_dir, \"best_model\")\n                os.makedirs(best_dir, exist_ok=True)\n                policy_model.save_pretrained(best_dir)\n                tokenizer.save_pretrained(best_dir)\n                print(f\"\\nâœ“ Best: {best_val_reward:.4f}\")\n        \n        # CHECKPOINT\n        if global_step % config.save_interval == 0 and global_step > 0:\n            ckpt_dir = os.path.join(config.output_dir, f\"checkpoint-{global_step}\")\n            os.makedirs(ckpt_dir, exist_ok=True)\n            policy_model.save_pretrained(ckpt_dir)\n            tokenizer.save_pretrained(ckpt_dir)\n        \n        global_step += 1\n        torch.cuda.empty_cache()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"TRAINING COMPLETE\")\nprint(f\"Best val reward: {best_val_reward:.4f}\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "final_metrics = evaluate_model(policy_model, val_dataset, num_samples=len(val_dataset), desc=\"Final\")\n",
    "print(f\"Final: {final_metrics['mean_reward']:.4f} Â± {final_metrics['std_reward']:.4f}\")\n",
    "print(f\"Improvement: {final_metrics['mean_reward'] - baseline_metrics['mean_reward']:.4f}\")\n",
    "with open(os.path.join(config.output_dir, \"final.json\"), \"w\") as f:\n",
    "    json.dump(final_metrics, f, indent=2)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dir = os.path.join(config.output_dir, \"final_model\")\n",
    "os.makedirs(final_dir, exist_ok=True)\n",
    "policy_model.save_pretrained(final_dir)\n",
    "tokenizer.save_pretrained(final_dir)\n",
    "print(f\"âœ“ Saved to {final_dir}\")\n",
    "\n",
    "summary = {\n",
    "    \"baseline\": baseline_metrics['mean_reward'],\n",
    "    \"final\": final_metrics['mean_reward'],\n",
    "    \"best_val\": best_val_reward,\n",
    "    \"improvement\": final_metrics['mean_reward'] - baseline_metrics['mean_reward']\n",
    "}\n",
    "with open(os.path.join(config.output_dir, \"summary.json\"), \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"\\nSummary: {summary}\")\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}