{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Password Game Baseline Evaluation: Qwen3-0.6B\n",
    "\n",
    "This notebook evaluates the **untrained** Qwen3-0.6B model on the Password Game task to establish a baseline.\n",
    "\n",
    "## Metrics Collected:\n",
    "- **Rules Satisfied**: Number of rules the model successfully satisfies\n",
    "- **Final Reward**: Reward score at end of game (+1 per rule, -0.1 per character)\n",
    "- **Success Rate**: % of games where all rules are satisfied\n",
    "- **Average Password Length**: Mean length of final passwords\n",
    "- **Rule-by-Rule Performance**: Success rate for each individual rule\n",
    "- **Episode Statistics**: Per-episode detailed breakdown\n",
    "\n",
    "## Evaluation Setup:\n",
    "- **Model**: Qwen/Qwen2.5-0.6B (pretrained, no fine-tuning)\n",
    "- **Episodes**: 20 independent game sessions\n",
    "- **Prompting**: Qwen chat template with system/user roles\n",
    "- **Max Steps**: 30 password attempts per episode\n",
    "- **Temperature**: 0.7 (balanced exploration/exploitation)\n",
    "\n",
    "Results are saved for post-training comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check_gpu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi -L || true\n",
    "\n",
    "import sys\n",
    "print(f\"Python: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q transformers>=4.45.0 accelerate tokenizers\n",
    "!pip install -q matplotlib seaborn pandas numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Add tasks directory to path\n",
    "sys.path.insert(0, '/home/user/notebooks/tasks/password-game')\n",
    "from game import PasswordGame, rules\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_section",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvalConfig:\n",
    "    \"\"\"Baseline evaluation configuration.\"\"\"\n",
    "    \n",
    "    # Model\n",
    "    model_name: str = \"Qwen/Qwen2.5-0.6B\"\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    precision: str = \"bfloat16\"  # bfloat16, float16, float32\n",
    "    \n",
    "    # Evaluation\n",
    "    num_episodes: int = 20  # Number of independent game sessions\n",
    "    max_steps_per_episode: int = 30  # Max password attempts per game\n",
    "    \n",
    "    # Generation\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.9\n",
    "    top_k: int = 50\n",
    "    max_new_tokens: int = 256\n",
    "    \n",
    "    # Prompting\n",
    "    use_chat_template: bool = True\n",
    "    system_prompt: str = \"\"\"You are an AI assistant playing the Password Game. You will receive rules one at a time, and must create a password that satisfies ALL current and previous rules. \n",
    "\n",
    "Instructions:\n",
    "1. Read the current rule carefully\n",
    "2. Review all previous rules\n",
    "3. Generate a password satisfying ALL rules\n",
    "4. ONLY output the password string, nothing else\n",
    "5. Keep password as short as possible while satisfying rules\n",
    "\n",
    "Important: Your response should ONLY contain the password - no explanations, no thinking, just the password string.\"\"\"\n",
    "    \n",
    "    # Output\n",
    "    output_dir: str = f\"./baseline_eval_{int(time.time())}\"\n",
    "    save_episodes: bool = True  # Save individual episode data\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed: int = 42\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "config = EvalConfig()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BASELINE EVALUATION CONFIGURATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Device: {config.device}\")\n",
    "print(f\"Episodes: {config.num_episodes}\")\n",
    "print(f\"Max Steps/Episode: {config.max_steps_per_episode}\")\n",
    "print(f\"Temperature: {config.temperature}\")\n",
    "print(f\"Output: {config.output_dir}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save config\n",
    "with open(os.path.join(config.output_dir, \"config.json\"), \"w\") as f:\n",
    "    json.dump(asdict(config), f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    \"\"\"Set random seed for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(config.seed)\n",
    "print(f\"✓ Random seed set to {config.seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_model",
   "metadata": {},
   "source": [
    "## 3. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_tokenizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"✓ Tokenizer loaded (vocab size: {len(tokenizer)})\")\n",
    "print(f\"  - Pad token: {tokenizer.pad_token}\")\n",
    "print(f\"  - EOS token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_model_weights",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model...\")\n",
    "dtype_map = {\n",
    "    \"bfloat16\": torch.bfloat16,\n",
    "    \"float16\": torch.float16,\n",
    "    \"float32\": torch.float32\n",
    "}\n",
    "dtype = dtype_map[config.precision]\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"✓ Model loaded ({num_params/1e9:.2f}B parameters)\")\n",
    "print(f\"  - Device: {next(model.parameters()).device}\")\n",
    "print(f\"  - Dtype: {next(model.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt_engineering",
   "metadata": {},
   "source": [
    "## 4. Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt_builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(game: PasswordGame, step: int) -> str:\n",
    "    \"\"\"\n",
    "    Build prompt for current game state using Qwen chat template.\n",
    "    \n",
    "    Args:\n",
    "        game: PasswordGame instance\n",
    "        step: Current step number\n",
    "    \n",
    "    Returns:\n",
    "        Formatted prompt string\n",
    "    \"\"\"\n",
    "    state = game.get_minimal_game_state()\n",
    "    \n",
    "    # Build user message with all rules\n",
    "    user_msg = f\"\"\"Step {step + 1}: Create a password satisfying these rules:\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    for i, rule in enumerate(state['all_rules']):\n",
    "        user_msg += f\"{i + 1}. {rule}\\n\"\n",
    "    \n",
    "    user_msg += \"\\nYour password:\"\n",
    "    \n",
    "    # Use Qwen chat template\n",
    "    if config.use_chat_template and hasattr(tokenizer, 'apply_chat_template'):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": config.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_msg}\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "    else:\n",
    "        # Fallback format\n",
    "        prompt = f\"System: {config.system_prompt}\\n\\nUser: {user_msg}\\n\\nAssistant:\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Test prompt generation\n",
    "test_game = PasswordGame()\n",
    "test_prompt = build_prompt(test_game, 0)\n",
    "print(\"Sample prompt (first 500 chars):\")\n",
    "print(\"=\" * 80)\n",
    "print(test_prompt[:500] + \"...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate_password",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_password(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate password from prompt using model.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Formatted prompt string\n",
    "    \n",
    "    Returns:\n",
    "        Generated password string (cleaned)\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=2048\n",
    "    ).to(config.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=config.max_new_tokens,\n",
    "            temperature=config.temperature,\n",
    "            top_p=config.top_p,\n",
    "            top_k=config.top_k,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated part\n",
    "    generated_ids = outputs[0][inputs.input_ids.shape[1]:]\n",
    "    password = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Clean up the password - extract just the password string\n",
    "    password = password.strip()\n",
    "    \n",
    "    # If model added explanation, try to extract just password\n",
    "    # Look for common patterns\n",
    "    if '\\n' in password:\n",
    "        password = password.split('\\n')[0].strip()\n",
    "    \n",
    "    # Remove common prefixes\n",
    "    for prefix in ['Password:', 'password:', 'Answer:', 'answer:']:\n",
    "        if password.startswith(prefix):\n",
    "            password = password[len(prefix):].strip()\n",
    "    \n",
    "    return password\n",
    "\n",
    "# Test generation\n",
    "test_password = generate_password(test_prompt)\n",
    "print(f\"Test password: '{test_password}'\")\n",
    "print(f\"Length: {len(test_password)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval_episode",
   "metadata": {},
   "source": [
    "## 5. Episode Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "episode_runner",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EpisodeResult:\n",
    "    \"\"\"Results from a single episode.\"\"\"\n",
    "    episode_id: int\n",
    "    success: bool\n",
    "    final_reward: float\n",
    "    rules_satisfied: int\n",
    "    total_rules: int\n",
    "    final_password: str\n",
    "    password_length: int\n",
    "    steps_taken: int\n",
    "    rule_progression: List[int]  # Rules satisfied at each step\n",
    "    passwords: List[str]  # All attempted passwords\n",
    "    rule_feedback: Dict  # Detailed rule feedback\n",
    "    game_state: Dict  # Final game state\n",
    "\n",
    "def run_episode(episode_id: int, verbose: bool = False) -> EpisodeResult:\n",
    "    \"\"\"\n",
    "    Run a single evaluation episode.\n",
    "    \n",
    "    Args:\n",
    "        episode_id: Episode number\n",
    "        verbose: Print detailed progress\n",
    "    \n",
    "    Returns:\n",
    "        EpisodeResult with metrics and data\n",
    "    \"\"\"\n",
    "    game = PasswordGame()\n",
    "    \n",
    "    passwords = []\n",
    "    rule_progression = []\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Episode {episode_id}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Captcha: {game.captcha}\")\n",
    "        print(f\"Country: {game.country}\")\n",
    "        print(f\"Wordle: {game.wordle_answer}\")\n",
    "        print(f\"Moon: {game.moon_phase}\")\n",
    "    \n",
    "    current_password = \"\"\n",
    "    \n",
    "    for step in range(config.max_steps_per_episode):\n",
    "        if not game.game_active:\n",
    "            break\n",
    "        \n",
    "        # Generate password\n",
    "        prompt = build_prompt(game, step)\n",
    "        current_password = generate_password(prompt)\n",
    "        passwords.append(current_password)\n",
    "        \n",
    "        # Get feedback\n",
    "        feedback = game.get_rule_feedback(current_password)\n",
    "        rule_progression.append(feedback['total_passing'])\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nStep {step + 1}:\")\n",
    "            print(f\"  Password: {current_password[:50]}{'...' if len(current_password) > 50 else ''}\")\n",
    "            print(f\"  Length: {len(current_password)}\")\n",
    "            print(f\"  Rules satisfied: {feedback['total_passing']}/{len(feedback['rules_checked'])}\")\n",
    "            print(f\"  Reward: {feedback['reward']:.2f}\")\n",
    "        \n",
    "        # Check if all current rules are satisfied\n",
    "        current_rule = game.get_current_rule()\n",
    "        all_satisfied = all(r['passes'] for r in feedback['rules_checked'])\n",
    "        \n",
    "        if all_satisfied:\n",
    "            if verbose:\n",
    "                print(f\"  ✓ All rules satisfied! Advancing...\")\n",
    "            game.advance_rule()\n",
    "        else:\n",
    "            if verbose:\n",
    "                failed_rules = [r for r in feedback['rules_checked'] if not r['passes']]\n",
    "                print(f\"  ✗ Failed {len(failed_rules)} rules:\")\n",
    "                for r in failed_rules[:3]:  # Show first 3 failures\n",
    "                    print(f\"    - Rule {r['rule_index']}: {r['rule_text'][:60]}...\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    final_feedback = game.get_rule_feedback(current_password)\n",
    "    final_reward = game.calculate_reward(current_password)\n",
    "    \n",
    "    success = not game.game_active  # Game ends when all rules satisfied\n",
    "    \n",
    "    result = EpisodeResult(\n",
    "        episode_id=episode_id,\n",
    "        success=success,\n",
    "        final_reward=final_reward,\n",
    "        rules_satisfied=final_feedback['total_passing'],\n",
    "        total_rules=len(rules),\n",
    "        final_password=current_password,\n",
    "        password_length=len(current_password),\n",
    "        steps_taken=len(passwords),\n",
    "        rule_progression=rule_progression,\n",
    "        passwords=passwords,\n",
    "        rule_feedback=final_feedback,\n",
    "        game_state=game.get_game_state()\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Episode {episode_id} Complete\")\n",
    "        print(f\"  Success: {success}\")\n",
    "        print(f\"  Rules: {result.rules_satisfied}/{result.total_rules}\")\n",
    "        print(f\"  Reward: {result.final_reward:.2f}\")\n",
    "        print(f\"  Password length: {result.password_length}\")\n",
    "        print(f\"  Steps: {result.steps_taken}\")\n",
    "        print(f\"{'='*80}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test single episode\n",
    "print(\"Running test episode...\")\n",
    "test_result = run_episode(0, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_eval",
   "metadata": {},
   "source": [
    "## 6. Run Full Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "full_eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(f\"RUNNING {config.num_episodes} EVALUATION EPISODES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = []\n",
    "\n",
    "for episode_id in tqdm(range(config.num_episodes), desc=\"Episodes\"):\n",
    "    result = run_episode(episode_id, verbose=False)\n",
    "    results.append(result)\n",
    "    \n",
    "    # Save individual episode if configured\n",
    "    if config.save_episodes:\n",
    "        episode_dir = os.path.join(config.output_dir, \"episodes\")\n",
    "        os.makedirs(episode_dir, exist_ok=True)\n",
    "        \n",
    "        episode_data = {\n",
    "            \"episode_id\": result.episode_id,\n",
    "            \"success\": result.success,\n",
    "            \"final_reward\": result.final_reward,\n",
    "            \"rules_satisfied\": result.rules_satisfied,\n",
    "            \"total_rules\": result.total_rules,\n",
    "            \"final_password\": result.final_password,\n",
    "            \"password_length\": result.password_length,\n",
    "            \"steps_taken\": result.steps_taken,\n",
    "            \"rule_progression\": result.rule_progression,\n",
    "            \"passwords\": result.passwords,\n",
    "            \"rule_feedback\": result.rule_feedback\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(episode_dir, f\"episode_{episode_id:03d}.json\"), \"w\") as f:\n",
    "            json.dump(episode_data, f, indent=2)\n",
    "\n",
    "print(\"\\n✓ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics",
   "metadata": {},
   "source": [
    "## 7. Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate metrics\n",
    "metrics = {\n",
    "    \"num_episodes\": len(results),\n",
    "    \"success_rate\": sum(r.success for r in results) / len(results),\n",
    "    \"avg_rules_satisfied\": np.mean([r.rules_satisfied for r in results]),\n",
    "    \"std_rules_satisfied\": np.std([r.rules_satisfied for r in results]),\n",
    "    \"max_rules_satisfied\": max(r.rules_satisfied for r in results),\n",
    "    \"min_rules_satisfied\": min(r.rules_satisfied for r in results),\n",
    "    \"avg_final_reward\": np.mean([r.final_reward for r in results]),\n",
    "    \"std_final_reward\": np.std([r.final_reward for r in results]),\n",
    "    \"avg_password_length\": np.mean([r.password_length for r in results]),\n",
    "    \"std_password_length\": np.std([r.password_length for r in results]),\n",
    "    \"avg_steps_taken\": np.mean([r.steps_taken for r in results]),\n",
    "    \"std_steps_taken\": np.std([r.steps_taken for r in results]),\n",
    "}\n",
    "\n",
    "# Rule-by-rule analysis\n",
    "rule_success_rates = defaultdict(int)\n",
    "for result in results:\n",
    "    for rule_check in result.rule_feedback['rules_checked']:\n",
    "        if rule_check['passes']:\n",
    "            rule_success_rates[rule_check['rule_index']] += 1\n",
    "\n",
    "rule_success_rates = {\n",
    "    rule_idx: count / len(results)\n",
    "    for rule_idx, count in rule_success_rates.items()\n",
    "}\n",
    "\n",
    "metrics['rule_success_rates'] = rule_success_rates\n",
    "\n",
    "# Print summary\n",
    "print(\"=\" * 80)\n",
    "print(\"BASELINE EVALUATION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Episodes: {metrics['num_episodes']}\")\n",
    "print(f\"\\nSuccess Rate: {metrics['success_rate']*100:.1f}%\")\n",
    "print(f\"\\nRules Satisfied: {metrics['avg_rules_satisfied']:.2f} ± {metrics['std_rules_satisfied']:.2f}\")\n",
    "print(f\"  - Min: {metrics['min_rules_satisfied']}\")\n",
    "print(f\"  - Max: {metrics['max_rules_satisfied']}\")\n",
    "print(f\"  - Total Rules: {results[0].total_rules}\")\n",
    "print(f\"\\nFinal Reward: {metrics['avg_final_reward']:.2f} ± {metrics['std_final_reward']:.2f}\")\n",
    "print(f\"\\nPassword Length: {metrics['avg_password_length']:.1f} ± {metrics['std_password_length']:.1f}\")\n",
    "print(f\"\\nSteps Taken: {metrics['avg_steps_taken']:.1f} ± {metrics['std_steps_taken']:.1f}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save metrics\n",
    "with open(os.path.join(config.output_dir, \"metrics.json\"), \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Metrics saved to {config.output_dir}/metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz",
   "metadata": {},
   "source": [
    "## 8. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Password Game Baseline Evaluation - Qwen3-0.6B', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Rules Satisfied Distribution\n",
    "ax = axes[0, 0]\n",
    "rules_satisfied = [r.rules_satisfied for r in results]\n",
    "ax.hist(rules_satisfied, bins=20, edgecolor='black', alpha=0.7)\n",
    "ax.axvline(metrics['avg_rules_satisfied'], color='red', linestyle='--', linewidth=2, label=f\"Mean: {metrics['avg_rules_satisfied']:.1f}\")\n",
    "ax.set_xlabel('Rules Satisfied')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Rules Satisfied Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Final Reward Distribution\n",
    "ax = axes[0, 1]\n",
    "final_rewards = [r.final_reward for r in results]\n",
    "ax.hist(final_rewards, bins=20, edgecolor='black', alpha=0.7, color='green')\n",
    "ax.axvline(metrics['avg_final_reward'], color='red', linestyle='--', linewidth=2, label=f\"Mean: {metrics['avg_final_reward']:.2f}\")\n",
    "ax.set_xlabel('Final Reward')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Final Reward Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Password Length Distribution\n",
    "ax = axes[0, 2]\n",
    "password_lengths = [r.password_length for r in results]\n",
    "ax.hist(password_lengths, bins=20, edgecolor='black', alpha=0.7, color='orange')\n",
    "ax.axvline(metrics['avg_password_length'], color='red', linestyle='--', linewidth=2, label=f\"Mean: {metrics['avg_password_length']:.1f}\")\n",
    "ax.set_xlabel('Password Length')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Password Length Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Success Rate\n",
    "ax = axes[1, 0]\n",
    "success_count = sum(r.success for r in results)\n",
    "fail_count = len(results) - success_count\n",
    "ax.bar(['Success', 'Failure'], [success_count, fail_count], color=['green', 'red'], alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title(f'Success Rate: {metrics[\"success_rate\"]*100:.1f}%')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 5. Steps Taken Distribution\n",
    "ax = axes[1, 1]\n",
    "steps_taken = [r.steps_taken for r in results]\n",
    "ax.hist(steps_taken, bins=15, edgecolor='black', alpha=0.7, color='purple')\n",
    "ax.axvline(metrics['avg_steps_taken'], color='red', linestyle='--', linewidth=2, label=f\"Mean: {metrics['avg_steps_taken']:.1f}\")\n",
    "ax.set_xlabel('Steps Taken')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Steps Taken Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Rule Progression (average across episodes)\n",
    "ax = axes[1, 2]\n",
    "max_steps = max(len(r.rule_progression) for r in results)\n",
    "avg_progression = []\n",
    "for step in range(max_steps):\n",
    "    step_values = [r.rule_progression[step] for r in results if step < len(r.rule_progression)]\n",
    "    if step_values:\n",
    "        avg_progression.append(np.mean(step_values))\n",
    "\n",
    "ax.plot(range(len(avg_progression)), avg_progression, marker='o', linewidth=2)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Avg Rules Satisfied')\n",
    "ax.set_title('Average Rule Progression')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.output_dir, 'summary_plots.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Summary plots saved to {config.output_dir}/summary_plots.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_rule_performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule-by-rule performance\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "rule_indices = sorted(rule_success_rates.keys())\n",
    "success_rates = [rule_success_rates[i] * 100 for i in rule_indices]\n",
    "\n",
    "colors = ['green' if rate > 50 else 'orange' if rate > 25 else 'red' for rate in success_rates]\n",
    "\n",
    "bars = ax.bar(rule_indices, success_rates, color=colors, alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax.axhline(50, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='50% threshold')\n",
    "ax.set_xlabel('Rule Index', fontsize=12)\n",
    "ax.set_ylabel('Success Rate (%)', fontsize=12)\n",
    "ax.set_title('Success Rate by Rule (Baseline)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(rule_indices)\n",
    "ax.set_xticklabels(rule_indices, rotation=45)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.legend()\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (idx, rate) in enumerate(zip(rule_indices, success_rates)):\n",
    "    ax.text(idx, rate + 1, f'{rate:.0f}%', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.output_dir, 'rule_performance.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Rule performance plot saved to {config.output_dir}/rule_performance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Rules vs Reward\n",
    "ax = axes[0]\n",
    "rules_satisfied = [r.rules_satisfied for r in results]\n",
    "final_rewards = [r.final_reward for r in results]\n",
    "ax.scatter(rules_satisfied, final_rewards, alpha=0.6, s=100, edgecolor='black')\n",
    "ax.set_xlabel('Rules Satisfied')\n",
    "ax.set_ylabel('Final Reward')\n",
    "ax.set_title('Rules Satisfied vs Final Reward')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Correlation coefficient\n",
    "corr = np.corrcoef(rules_satisfied, final_rewards)[0, 1]\n",
    "ax.text(0.05, 0.95, f'Correlation: {corr:.3f}', transform=ax.transAxes, \n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "        verticalalignment='top')\n",
    "\n",
    "# Password Length vs Rules\n",
    "ax = axes[1]\n",
    "password_lengths = [r.password_length for r in results]\n",
    "ax.scatter(password_lengths, rules_satisfied, alpha=0.6, s=100, edgecolor='black', color='orange')\n",
    "ax.set_xlabel('Password Length')\n",
    "ax.set_ylabel('Rules Satisfied')\n",
    "ax.set_title('Password Length vs Rules Satisfied')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Correlation coefficient\n",
    "corr = np.corrcoef(password_lengths, rules_satisfied)[0, 1]\n",
    "ax.text(0.05, 0.95, f'Correlation: {corr:.3f}', transform=ax.transAxes, \n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "        verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.output_dir, 'correlations.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Correlation plots saved to {config.output_dir}/correlations.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_table",
   "metadata": {},
   "source": [
    "## 9. Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed results table\n",
    "results_data = []\n",
    "for r in results:\n",
    "    results_data.append({\n",
    "        'Episode': r.episode_id,\n",
    "        'Success': '✓' if r.success else '✗',\n",
    "        'Rules': f\"{r.rules_satisfied}/{r.total_rules}\",\n",
    "        'Reward': f\"{r.final_reward:.2f}\",\n",
    "        'Password Length': r.password_length,\n",
    "        'Steps': r.steps_taken,\n",
    "        'Final Password (preview)': r.final_password[:50] + '...' if len(r.final_password) > 50 else r.final_password\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results_data)\n",
    "\n",
    "# Display first 10 episodes\n",
    "print(\"\\nResults Summary (first 10 episodes):\")\n",
    "print(df.head(10).to_string(index=False))\n",
    "\n",
    "# Save full table\n",
    "df.to_csv(os.path.join(config.output_dir, 'results_table.csv'), index=False)\n",
    "print(f\"\\n✓ Full results table saved to {config.output_dir}/results_table.csv\")\n",
    "\n",
    "# Summary statistics table\n",
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'Success Rate (%)',\n",
    "        'Avg Rules Satisfied',\n",
    "        'Avg Final Reward',\n",
    "        'Avg Password Length',\n",
    "        'Avg Steps Taken'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{metrics['success_rate']*100:.1f}%\",\n",
    "        f\"{metrics['avg_rules_satisfied']:.2f} ± {metrics['std_rules_satisfied']:.2f}\",\n",
    "        f\"{metrics['avg_final_reward']:.2f} ± {metrics['std_final_reward']:.2f}\",\n",
    "        f\"{metrics['avg_password_length']:.1f} ± {metrics['std_password_length']:.1f}\",\n",
    "        f\"{metrics['avg_steps_taken']:.1f} ± {metrics['std_steps_taken']:.1f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_df.to_csv(os.path.join(config.output_dir, 'summary_statistics.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best_worst",
   "metadata": {},
   "source": [
    "## 10. Best & Worst Episodes Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "best_worst_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best and worst episodes\n",
    "best_episode = max(results, key=lambda r: r.rules_satisfied)\n",
    "worst_episode = min(results, key=lambda r: r.rules_satisfied)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BEST EPISODE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Episode ID: {best_episode.episode_id}\")\n",
    "print(f\"Success: {best_episode.success}\")\n",
    "print(f\"Rules Satisfied: {best_episode.rules_satisfied}/{best_episode.total_rules}\")\n",
    "print(f\"Final Reward: {best_episode.final_reward:.2f}\")\n",
    "print(f\"Password Length: {best_episode.password_length}\")\n",
    "print(f\"Steps Taken: {best_episode.steps_taken}\")\n",
    "print(f\"\\nFinal Password: {best_episode.final_password}\")\n",
    "print(f\"\\nRule Progression: {best_episode.rule_progression}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"WORST EPISODE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Episode ID: {worst_episode.episode_id}\")\n",
    "print(f\"Success: {worst_episode.success}\")\n",
    "print(f\"Rules Satisfied: {worst_episode.rules_satisfied}/{worst_episode.total_rules}\")\n",
    "print(f\"Final Reward: {worst_episode.final_reward:.2f}\")\n",
    "print(f\"Password Length: {worst_episode.password_length}\")\n",
    "print(f\"Steps Taken: {worst_episode.steps_taken}\")\n",
    "print(f\"\\nFinal Password: {worst_episode.final_password}\")\n",
    "print(f\"\\nRule Progression: {worst_episode.rule_progression}\")\n",
    "\n",
    "# Failed rules in worst episode\n",
    "print(\"\\nFailed Rules in Worst Episode:\")\n",
    "for rule_check in worst_episode.rule_feedback['rules_checked']:\n",
    "    if not rule_check['passes']:\n",
    "        print(f\"  ✗ Rule {rule_check['rule_index']}: {rule_check['rule_text']}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export",
   "metadata": {},
   "source": [
    "## 11. Export for Post-Training Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive export for post-training comparison\n",
    "baseline_export = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"config\": asdict(config),\n",
    "    \"model\": {\n",
    "        \"name\": config.model_name,\n",
    "        \"num_parameters\": num_params,\n",
    "        \"dtype\": str(dtype)\n",
    "    },\n",
    "    \"metrics\": {\n",
    "        \"num_episodes\": metrics['num_episodes'],\n",
    "        \"success_rate\": metrics['success_rate'],\n",
    "        \"avg_rules_satisfied\": metrics['avg_rules_satisfied'],\n",
    "        \"std_rules_satisfied\": metrics['std_rules_satisfied'],\n",
    "        \"max_rules_satisfied\": metrics['max_rules_satisfied'],\n",
    "        \"min_rules_satisfied\": metrics['min_rules_satisfied'],\n",
    "        \"avg_final_reward\": metrics['avg_final_reward'],\n",
    "        \"std_final_reward\": metrics['std_final_reward'],\n",
    "        \"avg_password_length\": metrics['avg_password_length'],\n",
    "        \"std_password_length\": metrics['std_password_length'],\n",
    "        \"avg_steps_taken\": metrics['avg_steps_taken'],\n",
    "        \"std_steps_taken\": metrics['std_steps_taken']\n",
    "    },\n",
    "    \"rule_performance\": rule_success_rates,\n",
    "    \"episode_results\": [\n",
    "        {\n",
    "            \"episode_id\": r.episode_id,\n",
    "            \"success\": r.success,\n",
    "            \"rules_satisfied\": r.rules_satisfied,\n",
    "            \"final_reward\": r.final_reward,\n",
    "            \"password_length\": r.password_length,\n",
    "            \"steps_taken\": r.steps_taken\n",
    "        }\n",
    "        for r in results\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save baseline export\n",
    "export_path = os.path.join(config.output_dir, \"baseline_export.json\")\n",
    "with open(export_path, \"w\") as f:\n",
    "    json.dump(baseline_export, f, indent=2)\n",
    "\n",
    "print(f\"✓ Baseline export saved to {export_path}\")\n",
    "print(f\"\\nUse this file to compare with post-training results!\")\n",
    "print(f\"\\nAll results saved to: {config.output_dir}\")\n",
    "print(\"\\nFiles generated:\")\n",
    "print(\"  - config.json: Evaluation configuration\")\n",
    "print(\"  - metrics.json: Aggregated metrics\")\n",
    "print(\"  - baseline_export.json: Full baseline data for comparison\")\n",
    "print(\"  - summary_plots.png: Visual summary\")\n",
    "print(\"  - rule_performance.png: Per-rule success rates\")\n",
    "print(\"  - correlations.png: Correlation analysis\")\n",
    "print(\"  - results_table.csv: Detailed episode results\")\n",
    "print(\"  - summary_statistics.csv: Summary statistics\")\n",
    "print(\"  - episodes/: Individual episode data (JSON)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## 12. Conclusion\n",
    "\n",
    "The baseline evaluation is complete! Key findings:\n",
    "\n",
    "1. **Model Capability**: The untrained Qwen3-0.6B shows limited ability to solve the Password Game\n",
    "2. **Rule Difficulty**: Some rules are harder than others (see rule performance plot)\n",
    "3. **Password Complexity**: Trade-off between satisfying rules and password length\n",
    "4. **Training Opportunity**: Clear room for improvement through RL training\n",
    "\n",
    "### Next Steps:\n",
    "1. Use this baseline to train a policy with PPO/GRPO\n",
    "2. Compare post-training results using `baseline_export.json`\n",
    "3. Analyze which rules improved most after training\n",
    "4. Iterate on reward function if needed\n",
    "\n",
    "### Using the Baseline:\n",
    "```python\n",
    "# Load baseline for comparison\n",
    "with open('baseline_export.json', 'r') as f:\n",
    "    baseline = json.load(f)\n",
    "\n",
    "baseline_reward = baseline['metrics']['avg_final_reward']\n",
    "baseline_success = baseline['metrics']['success_rate']\n",
    "\n",
    "# Compare with post-training\n",
    "improvement_reward = trained_reward - baseline_reward\n",
    "improvement_success = trained_success - baseline_success\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
