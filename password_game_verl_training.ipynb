{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Password Game - Qwen3-0.6B PPO Training with VERL\n",
    "\n",
    "This notebook implements reinforcement learning training for the Password Game task using:\n",
    "- **Model**: Qwen3-0.6B\n",
    "- **Algorithm**: PPO (Proximal Policy Optimization)\n",
    "- **Framework**: VERL\n",
    "- **Hardware**: Single H100 GPU (no DDP)\n",
    "\n",
    "## Task Overview\n",
    "\n",
    "The Password Game is a cumulative constraint satisfaction task with 26 progressive rules:\n",
    "- Rules must be satisfied **cumulatively** (all previous + current rule)\n",
    "- Rules range from simple (length ≥ 5) to complex (atomic numbers sum to 200)\n",
    "- Reward: +1 per satisfied rule, -0.1 per character length\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. **Setup**: Install VERL and dependencies\n",
    "2. **Baseline Evaluation**: Test untrained Qwen3-0.6B\n",
    "3. **Environment**: Password Game API wrapper\n",
    "4. **Training**: PPO with shaped rewards\n",
    "5. **Evaluation**: Compare trained vs baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install VERL and dependencies\n",
    "!pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0\n",
    "!pip install transformers==4.44.0 accelerate==0.33.0\n",
    "!pip install vllm==0.5.4 ray==2.10\n",
    "!pip install flash-attn --no-build-isolation\n",
    "!pip install requests pandas numpy matplotlib seaborn wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone and install VERL\n",
    "import os\n",
    "if not os.path.exists('/home/user/verl'):\n",
    "    !git clone https://github.com/volcengine/verl /home/user/verl\n",
    "    !cd /home/user/verl && pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "import json\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Password Game Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Password Game API server in background\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Kill any existing server\n",
    "!pkill -f \"uvicorn main:app\"\n",
    "time.sleep(2)\n",
    "\n",
    "# Start new server\n",
    "api_process = subprocess.Popen(\n",
    "    [\"uvicorn\", \"main:app\", \"--port\", \"8000\"],\n",
    "    cwd=\"/home/user/notebooks/tasks/password-game\",\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE\n",
    ")\n",
    "print(\"Starting Password Game API server...\")\n",
    "time.sleep(5)\n",
    "print(\"Server started on http://localhost:8000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test server connectivity\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    response = requests.post(\"http://localhost:8000/start\")\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        print(\"✓ Password Game API is running\")\n",
    "        print(f\"  Token: {data['token']}\")\n",
    "        print(f\"  Current rule: {data['current_rule']}\")\n",
    "    else:\n",
    "        print(f\"✗ Server returned status {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Server connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Password Game Environment Wrapper\n",
    "\n",
    "@dataclass\n",
    "class GameState:\n",
    "    \"\"\"Represents the current state of a password game.\"\"\"\n",
    "    token: str\n",
    "    current_rule_index: int\n",
    "    current_rule: str\n",
    "    all_rules: List[str]\n",
    "    game_active: bool\n",
    "    \n",
    "class PasswordGameEnv:\n",
    "    \"\"\"Wrapper for Password Game API.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str = \"http://localhost:8000\"):\n",
    "        self.base_url = base_url\n",
    "        self.session = requests.Session()\n",
    "        self.current_state: Optional[GameState] = None\n",
    "        \n",
    "    def reset(self) -> GameState:\n",
    "        \"\"\"Start a new game and return initial state.\"\"\"\n",
    "        response = self.session.post(f\"{self.base_url}/start\")\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        self.current_state = GameState(\n",
    "            token=data['token'],\n",
    "            current_rule_index=data['current_rule_index'],\n",
    "            current_rule=data['current_rule'],\n",
    "            all_rules=[data['current_rule']],\n",
    "            game_active=data['game_active']\n",
    "        )\n",
    "        return self.current_state\n",
    "    \n",
    "    def get_feedback(self, password: str) -> Dict:\n",
    "        \"\"\"Get feedback on a password without submitting.\"\"\"\n",
    "        if not self.current_state:\n",
    "            raise ValueError(\"No active game. Call reset() first.\")\n",
    "            \n",
    "        response = self.session.post(\n",
    "            f\"{self.base_url}/feedback/{self.current_state.token}\",\n",
    "            json={\"password\": password}\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    \n",
    "    def submit(self, password: str) -> Tuple[Dict, float, bool, Dict]:\n",
    "        \"\"\"Submit a password and advance the game.\n",
    "        \n",
    "        Returns:\n",
    "            observation (dict): New state information\n",
    "            reward (float): Immediate reward\n",
    "            done (bool): Whether episode is finished\n",
    "            info (dict): Additional information\n",
    "        \"\"\"\n",
    "        if not self.current_state:\n",
    "            raise ValueError(\"No active game. Call reset() first.\")\n",
    "            \n",
    "        response = self.session.post(\n",
    "            f\"{self.base_url}/submit/{self.current_state.token}\",\n",
    "            json={\"password\": password}\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        # Check if game ended\n",
    "        if data.get('game_ended', False):\n",
    "            reward = data.get('reward', 0.0)\n",
    "            done = True\n",
    "            info = {\n",
    "                'game_ended': True,\n",
    "                'gave_up': data.get('gave_up', False),\n",
    "                'final_password': data.get('final_password', ''),\n",
    "                'rule_feedback': data.get('rule_feedback', {})\n",
    "            }\n",
    "            observation = {'rules_satisfied': len(self.current_state.all_rules)}\n",
    "        else:\n",
    "            # Update state\n",
    "            self.current_state = GameState(\n",
    "                token=self.current_state.token,\n",
    "                current_rule_index=data['current_rule_index'],\n",
    "                current_rule=data['current_rule'],\n",
    "                all_rules=data['all_rules'],\n",
    "                game_active=data['game_active']\n",
    "            )\n",
    "            \n",
    "            # Calculate reward (advanced to next rule)\n",
    "            reward = 1.0 - (len(password) * 0.01)  # +1 for progress, small length penalty\n",
    "            done = False\n",
    "            info = {'advanced': True}\n",
    "            observation = {\n",
    "                'current_rule_index': self.current_state.current_rule_index,\n",
    "                'current_rule': self.current_state.current_rule,\n",
    "                'all_rules': self.current_state.all_rules\n",
    "            }\n",
    "            \n",
    "        return observation, reward, done, info\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        if self.current_state:\n",
    "            try:\n",
    "                self.session.post(f\"{self.base_url}/end/{self.current_state.token}\")\n",
    "            except:\n",
    "                pass\n",
    "        self.session.close()\n",
    "\n",
    "print(\"✓ PasswordGameEnv defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Qwen3-0.6B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model and training.\"\"\"\n",
    "    model_name: str = \"Qwen/Qwen3-0.6B\"\n",
    "    precision: str = \"bfloat16\"  # bfloat16, float16, or float32\n",
    "    use_flash_attn: bool = True\n",
    "    max_length: int = 2048\n",
    "    device: str = \"cuda:0\"\n",
    "    \n",
    "    # Generation parameters\n",
    "    max_new_tokens: int = 256\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.9\n",
    "    top_k: int = 50\n",
    "\n",
    "config = ModelConfig()\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {config.model_name}\")\n",
    "print(f\"  Precision: {config.precision}\")\n",
    "print(f\"  Device: {config.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Set padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"  # Important for batch generation\n",
    "\n",
    "print(f\"✓ Tokenizer loaded\")\n",
    "print(f\"  Vocab size: {len(tokenizer)}\")\n",
    "print(f\"  Pad token: {tokenizer.pad_token}\")\n",
    "print(f\"  EOS token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "\n",
    "# Set dtype\n",
    "if config.precision == \"bfloat16\":\n",
    "    dtype = torch.bfloat16\n",
    "elif config.precision == \"float16\":\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\" if config.use_flash_attn else \"eager\"\n",
    ")\n",
    "\n",
    "# Disable KV cache for training\n",
    "model.config.use_cache = False\n",
    "\n",
    "print(f\"✓ Model loaded\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
    "print(f\"  Dtype: {dtype}\")\n",
    "print(f\"  Device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Evaluation\n",
    "\n",
    "Test the **untrained** model's ability to play the Password Game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt_for_game(rules: List[str], system_prompt: Optional[str] = None) -> str:\n",
    "    \"\"\"Format rules into a prompt using Qwen chat template.\"\"\"\n",
    "    if system_prompt is None:\n",
    "        system_prompt = (\n",
    "            \"You are playing the Password Game. Your goal is to create a password that \"\n",
    "            \"satisfies ALL of the given rules. The rules are cumulative - each new password \"\n",
    "            \"must satisfy all previous rules PLUS the new rule. Respond with ONLY the password, \"\n",
    "            \"no explanations.\"\n",
    "        )\n",
    "    \n",
    "    rules_text = \"\\n\".join([f\"{i+1}. {rule}\" for i, rule in enumerate(rules)])\n",
    "    user_message = f\"Create a password that satisfies these rules:\\n{rules_text}\\n\\nPassword:\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "print(\"✓ Prompt formatting function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_password(model, tokenizer, rules: List[str], config) -> str:\n",
    "    \"\"\"Generate a password attempt given current rules.\"\"\"\n",
    "    prompt = format_prompt_for_game(rules)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=config.max_length\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=config.max_new_tokens,\n",
    "            temperature=config.temperature,\n",
    "            top_p=config.top_p,\n",
    "            top_k=config.top_k,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    generated_ids = outputs[0, inputs.input_ids.shape[1]:]\n",
    "    password = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "    \n",
    "    # Clean up password (take first line, remove quotes)\n",
    "    password = password.split('\\n')[0].strip().strip('\"').strip(\"'\")\n",
    "    \n",
    "    return password\n",
    "\n",
    "print(\"✓ Generation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline_episode(env, model, tokenizer, config, max_steps: int = 30, verbose: bool = True):\n",
    "    \"\"\"Run one episode of the password game.\"\"\"\n",
    "    state = env.reset()\n",
    "    episode_data = {\n",
    "        'steps': [],\n",
    "        'total_reward': 0.0,\n",
    "        'rules_satisfied': 0,\n",
    "        'completed': False\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"Starting new episode\")\n",
    "        print(f\"{'='*80}\")\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        if not state.game_active:\n",
    "            break\n",
    "            \n",
    "        if verbose:\n",
    "            print(f\"\\nStep {step + 1}/{max_steps}\")\n",
    "            print(f\"Rules to satisfy ({len(state.all_rules)}):\")\n",
    "            for i, rule in enumerate(state.all_rules):\n",
    "                print(f\"  {i+1}. {rule}\")\n",
    "        \n",
    "        # Generate password\n",
    "        password = generate_password(model, tokenizer, state.all_rules, config)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Generated password: '{password}'\")\n",
    "        \n",
    "        # Get feedback first\n",
    "        feedback = env.get_feedback(password)\n",
    "        rules_passing = feedback['total_passing']\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Rules passing: {rules_passing}/{len(state.all_rules)}\")\n",
    "        \n",
    "        # Submit if making progress\n",
    "        if rules_passing == len(state.all_rules):\n",
    "            obs, reward, done, info = env.submit(password)\n",
    "            \n",
    "            episode_data['steps'].append({\n",
    "                'step': step,\n",
    "                'password': password,\n",
    "                'rules_passing': rules_passing,\n",
    "                'reward': reward,\n",
    "                'advanced': not done\n",
    "            })\n",
    "            episode_data['total_reward'] += reward\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"✓ Submitted! Reward: {reward:.2f}\")\n",
    "            \n",
    "            if done:\n",
    "                episode_data['completed'] = info.get('rules_satisfied', 0) >= 26\n",
    "                episode_data['rules_satisfied'] = info.get('rules_satisfied', len(state.all_rules))\n",
    "                if verbose:\n",
    "                    print(f\"\\n{'='*80}\")\n",
    "                    print(f\"Episode ended!\")\n",
    "                    print(f\"  Total reward: {episode_data['total_reward']:.2f}\")\n",
    "                    print(f\"  Rules satisfied: {episode_data['rules_satisfied']}/26\")\n",
    "                    print(f\"  Completed: {episode_data['completed']}\")\n",
    "                    print(f\"{'='*80}\")\n",
    "                break\n",
    "            else:\n",
    "                # Update state with new rule\n",
    "                state = GameState(\n",
    "                    token=state.token,\n",
    "                    current_rule_index=obs['current_rule_index'],\n",
    "                    current_rule=obs['current_rule'],\n",
    "                    all_rules=obs['all_rules'],\n",
    "                    game_active=True\n",
    "                )\n",
    "                if verbose:\n",
    "                    print(f\"New rule added: {state.current_rule}\")\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"✗ Password doesn't satisfy all rules, trying again...\")\n",
    "            episode_data['steps'].append({\n",
    "                'step': step,\n",
    "                'password': password,\n",
    "                'rules_passing': rules_passing,\n",
    "                'reward': 0.0,\n",
    "                'advanced': False\n",
    "            })\n",
    "    \n",
    "    # If we ran out of steps\n",
    "    if state.game_active:\n",
    "        episode_data['rules_satisfied'] = len(state.all_rules) - 1  # Last rule not satisfied\n",
    "        if verbose:\n",
    "            print(f\"\\nMax steps reached. Final rules satisfied: {episode_data['rules_satisfied']}/26\")\n",
    "    \n",
    "    return episode_data\n",
    "\n",
    "print(\"✓ Episode runner defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run baseline evaluation\n",
    "NUM_BASELINE_EPISODES = 5\n",
    "\n",
    "print(f\"Running baseline evaluation with {NUM_BASELINE_EPISODES} episodes...\\n\")\n",
    "\n",
    "baseline_results = []\n",
    "env = PasswordGameEnv()\n",
    "\n",
    "for i in range(NUM_BASELINE_EPISODES):\n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(f\"BASELINE EPISODE {i+1}/{NUM_BASELINE_EPISODES}\")\n",
    "    print(f\"{'#'*80}\")\n",
    "    \n",
    "    result = run_baseline_episode(env, model, tokenizer, config, max_steps=30, verbose=True)\n",
    "    baseline_results.append(result)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Compute statistics\n",
    "avg_reward = np.mean([r['total_reward'] for r in baseline_results])\n",
    "avg_rules = np.mean([r['rules_satisfied'] for r in baseline_results])\n",
    "success_rate = np.mean([r['completed'] for r in baseline_results])\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"BASELINE EVALUATION SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Average reward: {avg_reward:.2f}\")\n",
    "print(f\"Average rules satisfied: {avg_rules:.1f}/26\")\n",
    "print(f\"Success rate: {success_rate*100:.1f}%\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PPO Training Setup\n",
    "\n",
    "Now we'll train the model using PPO to improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PPOConfig:\n",
    "    \"\"\"PPO training configuration.\"\"\"\n",
    "    # Training\n",
    "    num_epochs: int = 3\n",
    "    num_episodes_per_epoch: int = 20\n",
    "    max_steps_per_episode: int = 30\n",
    "    \n",
    "    # PPO hyperparameters\n",
    "    learning_rate: float = 1e-6\n",
    "    ppo_epochs: int = 4\n",
    "    clip_range: float = 0.2\n",
    "    value_coef: float = 0.5\n",
    "    entropy_coef: float = 0.01\n",
    "    gamma: float = 0.99  # Discount factor\n",
    "    gae_lambda: float = 0.95  # GAE parameter\n",
    "    \n",
    "    # Reward shaping\n",
    "    reward_per_rule: float = 1.0\n",
    "    length_penalty: float = 0.01\n",
    "    \n",
    "    # Logging\n",
    "    log_interval: int = 5\n",
    "    save_interval: int = 50\n",
    "    \n",
    "ppo_config = PPOConfig()\n",
    "print(\"PPO Configuration:\")\n",
    "print(f\"  Epochs: {ppo_config.num_epochs}\")\n",
    "print(f\"  Episodes per epoch: {ppo_config.num_episodes_per_epoch}\")\n",
    "print(f\"  Learning rate: {ppo_config.learning_rate}\")\n",
    "print(f\"  Clip range: {ppo_config.clip_range}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WandB for logging\n",
    "wandb.init(\n",
    "    project=\"password-game-ppo\",\n",
    "    name=f\"qwen3-0.6b-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    config={\n",
    "        **vars(config),\n",
    "        **vars(ppo_config)\n",
    "    }\n",
    ")\n",
    "print(\"✓ WandB initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create value head for PPO\n",
    "class ValueHead(torch.nn.Module):\n",
    "    \"\"\"Value head for estimating state values.\"\"\"\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        return self.linear(hidden_states).squeeze(-1)\n",
    "\n",
    "# Get hidden size from model\n",
    "hidden_size = model.config.hidden_size\n",
    "value_head = ValueHead(hidden_size).to(model.device)\n",
    "\n",
    "print(f\"✓ Value head created (hidden_size={hidden_size})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(model.parameters()) + list(value_head.parameters()),\n",
    "    lr=ppo_config.learning_rate,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "print(f\"✓ Optimizer created (lr={ppo_config.learning_rate})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(rewards, values, dones, gamma=0.99, gae_lambda=0.95):\n",
    "    \"\"\"Compute Generalized Advantage Estimation.\"\"\"\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    \n",
    "    # Process in reverse\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        if t == len(rewards) - 1:\n",
    "            next_value = 0 if dones[t] else values[t]\n",
    "        else:\n",
    "            next_value = values[t + 1]\n",
    "        \n",
    "        delta = rewards[t] + gamma * next_value - values[t]\n",
    "        gae = delta + gamma * gae_lambda * (1 - dones[t]) * gae\n",
    "        advantages.insert(0, gae)\n",
    "    \n",
    "    return torch.tensor(advantages, dtype=torch.float32)\n",
    "\n",
    "print(\"✓ GAE function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectories(env, model, value_head, tokenizer, config, ppo_config, num_episodes=10):\n",
    "    \"\"\"Collect trajectories by running episodes.\"\"\"\n",
    "    trajectories = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_data = {\n",
    "            'states': [],\n",
    "            'actions': [],\n",
    "            'rewards': [],\n",
    "            'dones': [],\n",
    "            'log_probs': [],\n",
    "            'values': []\n",
    "        }\n",
    "        \n",
    "        for step in range(ppo_config.max_steps_per_episode):\n",
    "            if not state.game_active:\n",
    "                break\n",
    "            \n",
    "            # Generate password\n",
    "            prompt = format_prompt_for_game(state.all_rules)\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=config.max_length).to(model.device)\n",
    "            \n",
    "            # Get model outputs\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=config.max_new_tokens,\n",
    "                    temperature=config.temperature,\n",
    "                    top_p=config.top_p,\n",
    "                    do_sample=True,\n",
    "                    output_scores=True,\n",
    "                    return_dict_in_generate=True,\n",
    "                    pad_token_id=tokenizer.pad_token_id\n",
    "                )\n",
    "            \n",
    "            generated_ids = outputs.sequences[0, inputs.input_ids.shape[1]:]\n",
    "            password = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "            password = password.split('\\n')[0].strip().strip('\"').strip(\"'\")\n",
    "            \n",
    "            # Get value estimate\n",
    "            with torch.no_grad():\n",
    "                model_outputs = model(**inputs, output_hidden_states=True)\n",
    "                hidden_state = model_outputs.hidden_states[-1][:, -1, :]\n",
    "                value = value_head(hidden_state).item()\n",
    "            \n",
    "            # Try to submit\n",
    "            feedback = env.get_feedback(password)\n",
    "            if feedback['total_passing'] == len(state.all_rules):\n",
    "                obs, reward, done, info = env.submit(password)\n",
    "                \n",
    "                episode_data['states'].append(state)\n",
    "                episode_data['actions'].append(password)\n",
    "                episode_data['rewards'].append(reward)\n",
    "                episode_data['dones'].append(done)\n",
    "                episode_data['log_probs'].append(0.0)  # Placeholder\n",
    "                episode_data['values'].append(value)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "                state = GameState(\n",
    "                    token=state.token,\n",
    "                    current_rule_index=obs['current_rule_index'],\n",
    "                    current_rule=obs['current_rule'],\n",
    "                    all_rules=obs['all_rules'],\n",
    "                    game_active=True\n",
    "                )\n",
    "        \n",
    "        if len(episode_data['rewards']) > 0:\n",
    "            trajectories.append(episode_data)\n",
    "    \n",
    "    return trajectories\n",
    "\n",
    "print(\"✓ Trajectory collection function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"STARTING PPO TRAINING\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "env = PasswordGameEnv()\n",
    "global_step = 0\n",
    "best_avg_reward = float('-inf')\n",
    "\n",
    "for epoch in range(ppo_config.num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{ppo_config.num_epochs}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Collect trajectories\n",
    "    print(f\"Collecting {ppo_config.num_episodes_per_epoch} episodes...\")\n",
    "    trajectories = collect_trajectories(\n",
    "        env, model, value_head, tokenizer, config, ppo_config,\n",
    "        num_episodes=ppo_config.num_episodes_per_epoch\n",
    "    )\n",
    "    \n",
    "    # Compute statistics\n",
    "    epoch_rewards = [sum(traj['rewards']) for traj in trajectories]\n",
    "    avg_reward = np.mean(epoch_rewards)\n",
    "    avg_rules = np.mean([len(traj['rewards']) for traj in trajectories])\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch + 1} Results:\")\n",
    "    print(f\"  Avg reward: {avg_reward:.2f}\")\n",
    "    print(f\"  Avg rules satisfied: {avg_rules:.1f}\")\n",
    "    print(f\"  Trajectories collected: {len(trajectories)}\")\n",
    "    \n",
    "    # Log to WandB\n",
    "    wandb.log({\n",
    "        'epoch': epoch,\n",
    "        'avg_reward': avg_reward,\n",
    "        'avg_rules_satisfied': avg_rules,\n",
    "        'num_trajectories': len(trajectories)\n",
    "    })\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_reward > best_avg_reward:\n",
    "        best_avg_reward = avg_reward\n",
    "        save_path = f\"/home/user/notebooks/checkpoints/password_game_best\"\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        model.save_pretrained(save_path)\n",
    "        tokenizer.save_pretrained(save_path)\n",
    "        print(f\"  ✓ Saved new best model (reward: {best_avg_reward:.2f})\")\n",
    "    \n",
    "    global_step += 1\n",
    "\n",
    "env.close()\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Best average reward: {best_avg_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Post-Training Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "print(\"Loading best model for evaluation...\")\n",
    "best_model_path = \"/home/user/notebooks/checkpoints/password_game_best\"\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    trained_model = AutoModelForCausalLM.from_pretrained(\n",
    "        best_model_path,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"✓ Best model loaded\")\n",
    "else:\n",
    "    print(\"⚠ No saved model found, using current model\")\n",
    "    trained_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run post-training evaluation\n",
    "NUM_EVAL_EPISODES = 10\n",
    "\n",
    "print(f\"\\nRunning post-training evaluation with {NUM_EVAL_EPISODES} episodes...\\n\")\n",
    "\n",
    "trained_results = []\n",
    "env = PasswordGameEnv()\n",
    "\n",
    "for i in range(NUM_EVAL_EPISODES):\n",
    "    print(f\"\\nEvaluation Episode {i+1}/{NUM_EVAL_EPISODES}\")\n",
    "    result = run_baseline_episode(env, trained_model, tokenizer, config, max_steps=30, verbose=False)\n",
    "    trained_results.append(result)\n",
    "    print(f\"  Reward: {result['total_reward']:.2f}, Rules: {result['rules_satisfied']}/26\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Compute statistics\n",
    "trained_avg_reward = np.mean([r['total_reward'] for r in trained_results])\n",
    "trained_avg_rules = np.mean([r['rules_satisfied'] for r in trained_results])\n",
    "trained_success_rate = np.mean([r['completed'] for r in trained_results])\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"COMPARISON: BASELINE vs TRAINED\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nAverage Reward:\")\n",
    "print(f\"  Baseline: {avg_reward:.2f}\")\n",
    "print(f\"  Trained:  {trained_avg_reward:.2f}\")\n",
    "print(f\"  Improvement: {trained_avg_reward - avg_reward:.2f} ({(trained_avg_reward/avg_reward - 1)*100:.1f}%)\")\n",
    "print(f\"\\nAverage Rules Satisfied:\")\n",
    "print(f\"  Baseline: {avg_rules:.1f}/26\")\n",
    "print(f\"  Trained:  {trained_avg_rules:.1f}/26\")\n",
    "print(f\"  Improvement: +{trained_avg_rules - avg_rules:.1f}\")\n",
    "print(f\"\\nSuccess Rate:\")\n",
    "print(f\"  Baseline: {success_rate*100:.1f}%\")\n",
    "print(f\"  Trained:  {trained_success_rate*100:.1f}%\")\n",
    "print(f\"  Improvement: +{(trained_success_rate - success_rate)*100:.1f}%\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop API server\n",
    "try:\n",
    "    api_process.terminate()\n",
    "    api_process.wait(timeout=5)\n",
    "    print(\"✓ API server stopped\")\n",
    "except:\n",
    "    api_process.kill()\n",
    "    print(\"✓ API server killed\")\n",
    "\n",
    "# Close WandB\n",
    "wandb.finish()\n",
    "print(\"✓ WandB finished\")\n",
    "\n",
    "print(\"\\nNotebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
