{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dce099",
   "metadata": {},
   "outputs": [],
   "source": "!nvidia-smi -L || true\n\nimport sys\nprint(\"Python:\", sys.version)\n\n# Install required packages\n# Core ML\n!pip install -q transformers>=4.51.3 accelerate>=1.4.0 peft>=0.14.0 \\\n                 datasets>=3.3.2 torch wandb huggingface_hub \\\n                 sentencepiece protobuf tqdm matplotlib pandas\n\n# Inference backends (install based on what you'll use)\n# Uncomment the backend you want:\n# !pip install -q vllm>=0.6.0  # For vLLM backend\n# !pip install -q \"sglang[all]>=0.4.0\"  # For SGLang backend\n\nprint(\"\\n=== Environment ===\")\nimport torch\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n    print(f\"CUDA version: {torch.version.cuda}\")\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nassert DEVICE == \"cuda\", \"Please connect a GPU for RL training.\""
  },
  {
   "cell_type": "code",
   "id": "zzj88o3ci6",
   "source": "# Core imports for RL harness\nimport os\nimport random\nimport time\nimport json\nimport asyncio\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom copy import deepcopy\nfrom dataclasses import dataclass, field, asdict\nfrom typing import List, Dict, Optional, Any, Tuple, Callable\nfrom collections import defaultdict, deque\nfrom queue import Queue\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom tqdm.auto import tqdm\nimport pandas as pd\n\n# HuggingFace\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\n# Optional: vLLM (comment out if not installed)\ntry:\n    from vllm import LLM, SamplingParams\n    VLLM_AVAILABLE = True\n    print(\"✓ vLLM available\")\nexcept ImportError:\n    VLLM_AVAILABLE = False\n    print(\"⚠ vLLM not installed\")\n\n# Optional: SGLang (comment out if not installed)\ntry:\n    import sglang as sgl\n    SGLANG_AVAILABLE = True\n    print(\"✓ SGLang available\")\nexcept ImportError:\n    SGLANG_AVAILABLE = False\n    print(\"⚠ SGLang not installed\")\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8vl1q5nqmla",
   "source": "# Core Data Structures\nImmutable data structures for tracking rollouts and generation results.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "603kjhznm4s",
   "source": "@dataclass\nclass GenerationResult:\n    \"\"\"Output from inference backend\"\"\"\n    text: str\n    token_ids: List[int]\n    logprobs: Optional[List[float]] = None  # Log probabilities per token\n    top_logprobs: Optional[List[Dict[int, float]]] = None  # Top-k logprobs\n    finish_reason: str = \"length\"  # \"stop\", \"length\", \"error\"\n    \n@dataclass\nclass StepData:\n    \"\"\"Single step in a trajectory\"\"\"\n    prompt: str\n    response: GenerationResult\n    action_logprobs: Optional[torch.Tensor] = None  # For gradient computation\n    reward: Optional[np.ndarray] = None  # Multi-dimensional reward vector\n    value_estimate: Optional[float] = None\n    timestamp: float = field(default_factory=time.time)\n    metadata: Dict[str, Any] = field(default_factory=dict)\n\n@dataclass\nclass RolloutState:\n    \"\"\"Complete state of a single rollout\"\"\"\n    rollout_id: str\n    initial_prompt: str\n    trajectory: List[StepData] = field(default_factory=list)\n    step_count: int = 0\n    terminated: bool = False\n    termination_reason: Optional[str] = None\n    cumulative_reward: np.ndarray = field(default_factory=lambda: np.zeros(1))\n    verification_budget_spent: float = 0.0\n    max_verification_budget: float = 10.0\n    ref_model_snapshot_id: Optional[str] = None  # For dynamic ref changes\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    \n    def checkpoint(self) -> 'RolloutState':\n        \"\"\"Create a deep copy for branching\"\"\"\n        return deepcopy(self)\n    \n    def can_verify(self, cost: float) -> bool:\n        \"\"\"Check if verification is within budget\"\"\"\n        return self.verification_budget_spent + cost <= self.max_verification_budget\n    \n    def add_step(self, step: StepData):\n        \"\"\"Add a step to trajectory\"\"\"\n        self.trajectory.append(step)\n        self.step_count += 1\n        if step.reward is not None:\n            self.cumulative_reward = self.cumulative_reward + step.reward\n    \n    def get_full_context(self) -> str:\n        \"\"\"Get the full conversation context\"\"\"\n        context = self.initial_prompt\n        for step in self.trajectory:\n            context += step.response.text\n        return context\n    \n    def to_dict(self) -> Dict:\n        \"\"\"Serialize for storage\"\"\"\n        return asdict(self)\n\n@dataclass\nclass TrainingBatch:\n    \"\"\"Batch of data for training update\"\"\"\n    prompts: List[str]\n    responses: List[str]\n    token_ids: List[List[int]]\n    rewards: np.ndarray  # (batch_size, num_objectives)\n    old_logprobs: Optional[torch.Tensor] = None  # For importance sampling\n    advantages: Optional[torch.Tensor] = None\n    returns: Optional[torch.Tensor] = None\n    \nprint(\"✓ Data structures defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ac055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# WandB API key - get from https://wandb.ai/authorize\n",
    "WANDB_API_KEY = \"\"  # Your WandB API key\n",
    "if WANDB_API_KEY:\n",
    "    os.environ['WANDB_API_KEY'] = WANDB_API_KEY\n",
    "\n",
    "# HuggingFace token - get from https://huggingface.co/settings/tokens\n",
    "HF_TOKEN = \"\"  # Your HuggingFace token\n",
    "if HF_TOKEN:\n",
    "    os.environ['HF_TOKEN'] = HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5610c0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from huggingface_hub import login\n",
    "\n",
    "# WandB login\n",
    "try:\n",
    "    wandb.login()\n",
    "    print(\"✓ WandB login successful\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ WandB login failed: {e}\")\n",
    "    print(\"Training will continue without WandB logging\")\n",
    "\n",
    "# HuggingFace login\n",
    "try:\n",
    "    if os.environ.get('HF_TOKEN'):\n",
    "        login(token=os.environ['HF_TOKEN'])\n",
    "        print(\"✓ HuggingFace login successful\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ HuggingFace login failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b3a9e8",
   "metadata": {},
   "source": [
    "# Async Rollout manager with dynamic batching\n",
    "Since rollouts vary dramatically in length (6-50 steps), use an async architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d59df5",
   "metadata": {},
   "outputs": [],
   "source": "class AsyncRolloutManager:\n    \"\"\"\n    Manages concurrent rollouts with dynamic batching.\n    Groups rollouts by step count for efficient batch inference.\n    \"\"\"\n    \n    def __init__(\n        self,\n        inference_backend: InferenceBackend,\n        max_concurrent_rollouts: int = 32,\n        max_steps: int = 50,\n        num_objectives: int = 1,\n    ):\n        self.backend = inference_backend\n        self.max_concurrent = max_concurrent_rollouts\n        self.max_steps = max_steps\n        self.num_objectives = num_objectives\n        \n        self.active_rollouts: Dict[str, RolloutState] = {}\n        self.completed_rollouts: List[RolloutState] = []\n        self.rollout_counter = 0\n        \n        # Customizable hooks - you override these!\n        self.should_terminate_fn: Optional[Callable[[RolloutState], bool]] = None\n        self.get_reward_fn: Optional[Callable[[RolloutState, StepData], np.ndarray]] = None\n        self.process_response_fn: Optional[Callable[[str], str]] = None\n    \n    def start_rollout(self, prompt: str, metadata: Optional[Dict] = None) -> str:\n        \"\"\"Start a new rollout from prompt\"\"\"\n        if len(self.active_rollouts) >= self.max_concurrent:\n            raise RuntimeError(f\"Max concurrent rollouts ({self.max_concurrent}) reached\")\n        \n        rollout_id = f\"rollout_{self.rollout_counter}\"\n        self.rollout_counter += 1\n        \n        state = RolloutState(\n            rollout_id=rollout_id,\n            initial_prompt=prompt,\n            cumulative_reward=np.zeros(self.num_objectives),\n            metadata=metadata or {},\n        )\n        \n        self.active_rollouts[rollout_id] = state\n        return rollout_id\n    \n    def step_all(\n        self,\n        max_new_tokens: int = 256,\n        temperature: float = 1.0,\n        top_p: float = 0.95,\n        **gen_kwargs\n    ) -> Dict[str, GenerationResult]:\n        \"\"\"\n        Step all active rollouts.\n        Groups by step count for efficient batching.\n        Returns: Dict mapping rollout_id to generation result\n        \"\"\"\n        if not self.active_rollouts:\n            return {}\n        \n        # Group active rollouts by step count for batching\n        step_groups: Dict[int, List[str]] = defaultdict(list)\n        for rid, state in self.active_rollouts.items():\n            if not state.terminated:\n                step_groups[state.step_count].append(rid)\n        \n        all_results = {}\n        \n        # Process each step group as a batch\n        for step_num, rollout_ids in step_groups.items():\n            # Gather prompts (full context so far)\n            prompts = [self.active_rollouts[rid].get_full_context() for rid in rollout_ids]\n            \n            # Batch generation\n            gen_results = self.backend.generate(\n                prompts,\n                max_new_tokens=max_new_tokens,\n                temperature=temperature,\n                top_p=top_p,\n                **gen_kwargs\n            )\n            \n            # Process results for each rollout\n            for rid, gen_result in zip(rollout_ids, gen_results):\n                state = self.active_rollouts[rid]\n                \n                # Optionally process response\n                if self.process_response_fn:\n                    gen_result.text = self.process_response_fn(gen_result.text)\n                \n                # Create step data\n                step_data = StepData(\n                    prompt=state.get_full_context(),\n                    response=gen_result,\n                )\n                \n                # Compute reward if function provided\n                if self.get_reward_fn:\n                    step_data.reward = self.get_reward_fn(state, step_data)\n                else:\n                    step_data.reward = np.zeros(self.num_objectives)\n                \n                # Add step to trajectory\n                state.add_step(step_data)\n                \n                # Check termination conditions\n                terminated = False\n                termination_reason = None\n                \n                # Max steps reached\n                if state.step_count >= self.max_steps:\n                    terminated = True\n                    termination_reason = \"max_steps\"\n                \n                # EOS token generated\n                if gen_result.finish_reason == \"stop\":\n                    terminated = True\n                    termination_reason = \"eos\"\n                \n                # Custom termination\n                if self.should_terminate_fn and self.should_terminate_fn(state):\n                    terminated = True\n                    termination_reason = \"custom\"\n                \n                if terminated:\n                    state.terminated = True\n                    state.termination_reason = termination_reason\n                \n                all_results[rid] = gen_result\n        \n        return all_results\n    \n    def complete_terminated_rollouts(self) -> List[RolloutState]:\n        \"\"\"Move terminated rollouts to completed list\"\"\"\n        newly_completed = []\n        to_remove = []\n        \n        for rid, state in self.active_rollouts.items():\n            if state.terminated:\n                self.completed_rollouts.append(state)\n                newly_completed.append(state)\n                to_remove.append(rid)\n        \n        for rid in to_remove:\n            del self.active_rollouts[rid]\n        \n        return newly_completed\n    \n    def has_active_rollouts(self) -> bool:\n        \"\"\"Check if there are active rollouts\"\"\"\n        return len(self.active_rollouts) > 0\n    \n    def get_statistics(self) -> Dict:\n        \"\"\"Get current rollout statistics\"\"\"\n        active_steps = [s.step_count for s in self.active_rollouts.values()]\n        completed_steps = [s.step_count for s in self.completed_rollouts]\n        \n        return {\n            \"active\": len(self.active_rollouts),\n            \"completed\": len(self.completed_rollouts),\n            \"active_avg_steps\": np.mean(active_steps) if active_steps else 0,\n            \"completed_avg_steps\": np.mean(completed_steps) if completed_steps else 0,\n            \"completed_rewards\": [s.cumulative_reward.tolist() for s in self.completed_rollouts[-10:]],\n        }\n    \n    def reset(self):\n        \"\"\"Reset manager for new training iteration\"\"\"\n        self.active_rollouts.clear()\n        self.completed_rollouts.clear()\n\nprint(\"✓ AsyncRolloutManager defined\")"
  },
  {
   "cell_type": "markdown",
   "id": "baedb9df",
   "metadata": {},
   "source": [
    "# Flexible Rollout State with Checkpointing\n",
    "Track everything per-rollout with ability to branch/restore:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21afee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RolloutState:\n",
    "    prompt: str\n",
    "    trajectory: List[StepData]\n",
    "    step_count: int = 0\n",
    "    terminated: bool = False\n",
    "    termination_reason: Optional[str] = None\n",
    "    cumulative_reward: np.ndarray  # Multi-dimensional for Pareto\n",
    "    verification_budget_spent: float = 0.0\n",
    "    ref_model_snapshot: Optional[str] = None  # For dynamic ref changes\n",
    "    \n",
    "    def checkpoint(self) -> 'RolloutState':\n",
    "        return deepcopy(self)\n",
    "    \n",
    "    def can_verify(self, cost: float) -> bool:\n",
    "        return self.verification_budget_spent + cost <= self.max_budget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814a9571",
   "metadata": {},
   "source": [
    "# Multi-Backend Inference with Unified Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d9add0",
   "metadata": {},
   "outputs": [],
   "source": "class InferenceBackend(ABC):\n    \"\"\"Abstract base class for inference backends\"\"\"\n    \n    @abstractmethod\n    def generate(self, prompts: List[str], **kwargs) -> List[GenerationResult]:\n        \"\"\"Generate responses for a batch of prompts\"\"\"\n        pass\n    \n    @abstractmethod\n    def shutdown(self):\n        \"\"\"Clean up resources\"\"\"\n        pass\n\n\nclass HuggingFaceBackend(InferenceBackend):\n    \"\"\"\n    HuggingFace backend - full gradient access.\n    Use this for:\n    - Gradient computation during training (FULL UPDATES)\n    - Reference model logit computation\n    - Small-scale experiments\n    \"\"\"\n    \n    def __init__(\n        self,\n        model_name: str,\n        device: str = \"cuda\",\n        dtype: str = \"bfloat16\",  # \"float32\", \"bfloat16\", \"float16\"\n        gradient_checkpointing: bool = False,  # Save memory during backward pass\n    ):\n        self.device = device\n        self.model_name = model_name\n        self.dtype_map = {\n            \"float32\": torch.float32,\n            \"bfloat16\": torch.bfloat16,\n            \"float16\": torch.float16,\n        }\n        self.torch_dtype = self.dtype_map.get(dtype, torch.bfloat16)\n        \n        # Tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n        # Load model - FULL WEIGHTS\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=self.torch_dtype,\n            device_map=None,  # We'll move to device manually for full control\n            trust_remote_code=True,\n        )\n        self.model = self.model.to(device)\n        \n        # Optional: Gradient checkpointing for memory efficiency\n        if gradient_checkpointing:\n            self.model.gradient_checkpointing_enable()\n            print(f\"✓ Gradient checkpointing enabled\")\n        \n        # Count trainable parameters\n        total_params = sum(p.numel() for p in self.model.parameters())\n        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n        print(f\"✓ HuggingFace backend loaded: {model_name}\")\n        print(f\"  Total params: {total_params:,}\")\n        print(f\"  Trainable params: {trainable_params:,}\")\n        print(f\"  Dtype: {dtype}\")\n    \n    def generate(\n        self,\n        prompts: List[str],\n        max_new_tokens: int = 256,\n        temperature: float = 1.0,\n        top_p: float = 0.95,\n        do_sample: bool = True,\n        return_logprobs: bool = False,\n        **kwargs\n    ) -> List[GenerationResult]:\n        \"\"\"Generate responses with optional logprobs\"\"\"\n        self.model.eval()  # Set to eval mode for generation\n        results = []\n        \n        for prompt in prompts:\n            inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True)\n            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n            \n            with torch.no_grad():\n                outputs = self.model.generate(\n                    **inputs,\n                    max_new_tokens=max_new_tokens,\n                    temperature=temperature if do_sample else 1.0,\n                    top_p=top_p if do_sample else 1.0,\n                    do_sample=do_sample,\n                    pad_token_id=self.tokenizer.pad_token_id,\n                    output_scores=return_logprobs,\n                    return_dict_in_generate=True,\n                )\n            \n            # Decode\n            generated_ids = outputs.sequences[0][inputs[\"input_ids\"].shape[1]:]\n            text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n            \n            # Extract logprobs if requested\n            logprobs = None\n            if return_logprobs and hasattr(outputs, \"scores\"):\n                logprobs = []\n                for i, score in enumerate(outputs.scores):\n                    probs = F.log_softmax(score[0], dim=-1)\n                    token_id = generated_ids[i].item()\n                    logprobs.append(probs[token_id].item())\n            \n            results.append(GenerationResult(\n                text=text,\n                token_ids=generated_ids.tolist(),\n                logprobs=logprobs,\n                finish_reason=\"stop\" if generated_ids[-1] == self.tokenizer.eos_token_id else \"length\"\n            ))\n        \n        return results\n    \n    def get_logits(\n        self,\n        prompts: List[str],\n        responses: List[str],\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Get logits for prompt+response pairs.\n        Returns: (logits, attention_mask)\n        Essential for KL divergence computation in RL.\n        \"\"\"\n        full_texts = [p + r for p, r in zip(prompts, responses)]\n        \n        inputs = self.tokenizer(\n            full_texts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n        )\n        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n        \n        with torch.no_grad():\n            outputs = self.model(**inputs)\n        \n        return outputs.logits, inputs[\"attention_mask\"]\n    \n    def compute_log_probs(\n        self,\n        prompts: List[str],\n        responses: List[str],\n    ) -> torch.Tensor:\n        \"\"\"\n        Compute log probabilities of responses given prompts.\n        Critical for RL training - stable numerical computation.\n        \"\"\"\n        full_texts = [p + r for p, r in zip(prompts, responses)]\n        \n        # Tokenize\n        prompt_encodings = self.tokenizer(prompts, padding=True, return_tensors=\"pt\")\n        full_encodings = self.tokenizer(full_texts, padding=True, return_tensors=\"pt\")\n        \n        prompt_lens = [len(self.tokenizer.encode(p)) for p in prompts]\n        \n        inputs = {k: v.to(self.device) for k, v in full_encodings.items()}\n        \n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            logits = outputs.logits  # (batch, seq_len, vocab)\n        \n        # Compute log probs for each response token\n        log_probs_list = []\n        for i in range(len(prompts)):\n            # Response starts after prompt\n            start_idx = prompt_lens[i] - 1  # -1 because we predict next token\n            response_logits = logits[i, start_idx:-1]  # (response_len, vocab)\n            response_ids = full_encodings[\"input_ids\"][i, prompt_lens[i]:]\n            \n            # Filter out padding\n            mask = full_encodings[\"attention_mask\"][i, prompt_lens[i]:]\n            \n            # Compute log softmax\n            log_probs = F.log_softmax(response_logits, dim=-1)\n            \n            # Gather log probs of actual tokens\n            token_log_probs = log_probs.gather(1, response_ids.unsqueeze(1).to(self.device)).squeeze()\n            \n            # Apply mask and sum\n            masked_log_probs = token_log_probs * mask.to(self.device).float()\n            log_probs_list.append(masked_log_probs.sum())\n        \n        return torch.stack(log_probs_list)\n    \n    def shutdown(self):\n        del self.model\n        torch.cuda.empty_cache()\n        print(\"✓ HuggingFace backend shutdown\")\n\n\nclass VLLMBackend(InferenceBackend):\n    \"\"\"\n    vLLM backend - fast inference with PagedAttention.\n    Use this for:\n    - Fast rollout generation\n    - Large batch inference\n    Note: vLLM doesn't support gradient computation!\n    \"\"\"\n    \n    def __init__(\n        self,\n        model_name: str,\n        tensor_parallel_size: int = 1,\n        gpu_memory_utilization: float = 0.9,\n        max_model_len: int = 4096,\n    ):\n        if not VLLM_AVAILABLE:\n            raise ImportError(\"vLLM not installed. Run: pip install vllm\")\n        \n        self.model_name = model_name\n        self.llm = LLM(\n            model=model_name,\n            tensor_parallel_size=tensor_parallel_size,\n            gpu_memory_utilization=gpu_memory_utilization,\n            max_model_len=max_model_len,\n            trust_remote_code=True,\n        )\n        print(f\"✓ vLLM backend loaded: {model_name}\")\n    \n    def generate(\n        self,\n        prompts: List[str],\n        max_new_tokens: int = 256,\n        temperature: float = 1.0,\n        top_p: float = 0.95,\n        return_logprobs: bool = False,\n        n_logprobs: int = 5,\n        **kwargs\n    ) -> List[GenerationResult]:\n        \"\"\"Batch generate with vLLM\"\"\"\n        \n        sampling_params = SamplingParams(\n            max_tokens=max_new_tokens,\n            temperature=temperature,\n            top_p=top_p,\n            logprobs=n_logprobs if return_logprobs else None,\n        )\n        \n        outputs = self.llm.generate(prompts, sampling_params)\n        \n        results = []\n        for output in outputs:\n            completion = output.outputs[0]\n            \n            logprobs = None\n            if return_logprobs and completion.logprobs:\n                logprobs = [lp[completion.token_ids[i]].logprob \n                           for i, lp in enumerate(completion.logprobs)]\n            \n            results.append(GenerationResult(\n                text=completion.text,\n                token_ids=list(completion.token_ids),\n                logprobs=logprobs,\n                finish_reason=completion.finish_reason or \"length\"\n            ))\n        \n        return results\n    \n    def shutdown(self):\n        del self.llm\n        torch.cuda.empty_cache()\n        print(\"✓ vLLM backend shutdown\")\n\n\nclass SGLangBackend(InferenceBackend):\n    \"\"\"\n    SGLang backend - RadixAttention for efficient KV caching.\n    Use this for:\n    - Fast rollout generation\n    - Prefix caching (good for multi-turn)\n    Note: SGLang doesn't support gradient computation!\n    \"\"\"\n    \n    def __init__(\n        self,\n        model_name: str,\n        mem_fraction_static: float = 0.8,\n        tp_size: int = 1,\n    ):\n        if not SGLANG_AVAILABLE:\n            raise ImportError(\"SGLang not installed. Run: pip install 'sglang[all]'\")\n        \n        self.model_name = model_name\n        self.engine = sgl.Engine(\n            model_path=model_name,\n            mem_fraction_static=mem_fraction_static,\n            tp_size=tp_size,\n        )\n        print(f\"✓ SGLang backend loaded: {model_name}\")\n    \n    def generate(\n        self,\n        prompts: List[str],\n        max_new_tokens: int = 256,\n        temperature: float = 1.0,\n        top_p: float = 0.95,\n        return_logprobs: bool = False,\n        n_logprobs: int = 5,\n        **kwargs\n    ) -> List[GenerationResult]:\n        \"\"\"Batch generate with SGLang\"\"\"\n        \n        sampling_params = {\n            \"max_new_tokens\": max_new_tokens,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n        }\n        \n        if return_logprobs:\n            outputs = self.engine.generate(\n                prompts,\n                sampling_params,\n                return_logprob=True,\n                top_logprobs_num=n_logprobs,\n            )\n        else:\n            outputs = self.engine.generate(prompts, sampling_params)\n        \n        results = []\n        for output in outputs:\n            logprobs = None\n            if return_logprobs and \"meta_info\" in output:\n                # SGLang returns: (logprob, token_id, text)\n                logprobs = [lp[0] for lp in output[\"meta_info\"][\"output_token_logprobs\"]]\n            \n            results.append(GenerationResult(\n                text=output[\"text\"],\n                token_ids=output.get(\"output_ids\", []),\n                logprobs=logprobs,\n                finish_reason=output.get(\"meta_info\", {}).get(\"finish_reason\", {}).get(\"type\", \"length\")\n            ))\n        \n        return results\n    \n    def shutdown(self):\n        self.engine.shutdown()\n        print(\"✓ SGLang backend shutdown\")\n\n\nprint(\"✓ Inference backends defined\")"
  },
  {
   "cell_type": "markdown",
   "id": "8699fc5d",
   "metadata": {},
   "source": [
    "# Dynamic Reference Model Manager\n",
    "Handle changing reference models mid-training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9a5cec",
   "metadata": {},
   "outputs": [],
   "source": "class ReferenceModelManager:\n    \"\"\"\n    Manages reference model for KL divergence computation.\n    Supports dynamic snapshots mid-training.\n    \"\"\"\n    \n    def __init__(self, model_name: str, device: str = \"cuda\"):\n        self.model_name = model_name\n        self.device = device\n        self.snapshots: Dict[str, Dict] = {}  # snapshot_id -> state_dict\n        \n        # Load reference model (frozen)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=torch.bfloat16,\n            device_map=\"auto\",\n            trust_remote_code=True,\n        )\n        self.model.eval()\n        \n        # Freeze all parameters\n        for param in self.model.parameters():\n            param.requires_grad = False\n        \n        print(f\"✓ Reference model loaded: {model_name}\")\n    \n    def snapshot_current_training_model(self, training_model: nn.Module) -> str:\n        \"\"\"\n        Create snapshot of current training model weights.\n        Useful for dynamic reference model updates.\n        \"\"\"\n        snapshot_id = f\"snapshot_{int(time.time() * 1000)}\"\n        # Store only trainable parameters (e.g., LoRA weights)\n        self.snapshots[snapshot_id] = {\n            k: v.cpu().clone() for k, v in training_model.state_dict().items()\n        }\n        print(f\"✓ Created snapshot: {snapshot_id}\")\n        return snapshot_id\n    \n    def load_snapshot(self, snapshot_id: str):\n        \"\"\"Load a previously saved snapshot into reference model\"\"\"\n        if snapshot_id not in self.snapshots:\n            raise ValueError(f\"Snapshot {snapshot_id} not found\")\n        \n        self.model.load_state_dict(self.snapshots[snapshot_id], strict=False)\n        print(f\"✓ Loaded snapshot: {snapshot_id}\")\n    \n    def compute_log_probs(\n        self,\n        prompts: List[str],\n        responses: List[str],\n        snapshot_id: Optional[str] = None,\n    ) -> torch.Tensor:\n        \"\"\"\n        Compute log probabilities from reference model.\n        Optionally uses a specific snapshot.\n        \"\"\"\n        # Load snapshot if specified\n        if snapshot_id and snapshot_id in self.snapshots:\n            original_state = {k: v.clone() for k, v in self.model.state_dict().items()}\n            self.model.load_state_dict(self.snapshots[snapshot_id], strict=False)\n        \n        # Compute log probs\n        full_texts = [p + r for p, r in zip(prompts, responses)]\n        prompt_lens = [len(self.tokenizer.encode(p)) for p in prompts]\n        \n        full_encodings = self.tokenizer(full_texts, padding=True, return_tensors=\"pt\")\n        inputs = {k: v.to(self.device) for k, v in full_encodings.items()}\n        \n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            logits = outputs.logits\n        \n        log_probs_list = []\n        for i in range(len(prompts)):\n            start_idx = prompt_lens[i] - 1\n            response_logits = logits[i, start_idx:-1]\n            response_ids = full_encodings[\"input_ids\"][i, prompt_lens[i]:]\n            mask = full_encodings[\"attention_mask\"][i, prompt_lens[i]:]\n            \n            log_probs = F.log_softmax(response_logits, dim=-1)\n            token_log_probs = log_probs.gather(1, response_ids.unsqueeze(1).to(self.device)).squeeze()\n            masked_log_probs = token_log_probs * mask.to(self.device).float()\n            log_probs_list.append(masked_log_probs.sum())\n        \n        # Restore original state if snapshot was used\n        if snapshot_id and snapshot_id in self.snapshots:\n            self.model.load_state_dict(original_state, strict=False)\n        \n        return torch.stack(log_probs_list)\n    \n    def compute_kl_divergence(\n        self,\n        prompts: List[str],\n        responses: List[str],\n        train_model: nn.Module,\n        train_tokenizer,\n        snapshot_id: Optional[str] = None,\n        per_token: bool = False,\n    ) -> torch.Tensor:\n        \"\"\"\n        Compute KL divergence: KL(π_train || π_ref)\n        \n        Args:\n            per_token: If True, return per-token KL. Otherwise, sum over sequence.\n        \"\"\"\n        # Get reference log probs\n        ref_log_probs = self.compute_log_probs(prompts, responses, snapshot_id)\n        \n        # Get training model log probs (with gradients if needed)\n        full_texts = [p + r for p, r in zip(prompts, responses)]\n        prompt_lens = [len(train_tokenizer.encode(p)) for p in prompts]\n        \n        full_encodings = train_tokenizer(full_texts, padding=True, return_tensors=\"pt\")\n        inputs = {k: v.to(self.device) for k, v in full_encodings.items()}\n        \n        train_outputs = train_model(**inputs)\n        train_logits = train_outputs.logits\n        \n        train_log_probs_list = []\n        for i in range(len(prompts)):\n            start_idx = prompt_lens[i] - 1\n            response_logits = train_logits[i, start_idx:-1]\n            response_ids = full_encodings[\"input_ids\"][i, prompt_lens[i]:]\n            mask = full_encodings[\"attention_mask\"][i, prompt_lens[i]:]\n            \n            log_probs = F.log_softmax(response_logits, dim=-1)\n            token_log_probs = log_probs.gather(1, response_ids.unsqueeze(1).to(self.device)).squeeze()\n            masked_log_probs = token_log_probs * mask.to(self.device).float()\n            train_log_probs_list.append(masked_log_probs.sum())\n        \n        train_log_probs = torch.stack(train_log_probs_list)\n        \n        # KL divergence: π_train * (log π_train - log π_ref)\n        # Approximation: (π_train / π_ref).log() ≈ train_log_probs - ref_log_probs\n        kl_div = train_log_probs - ref_log_probs\n        \n        return kl_div if per_token else kl_div.mean()\n    \n    def clear_snapshots(self):\n        \"\"\"Free memory by clearing old snapshots\"\"\"\n        self.snapshots.clear()\n        torch.cuda.empty_cache()\n        print(\"✓ Cleared all snapshots\")\n    \n    def shutdown(self):\n        del self.model\n        self.snapshots.clear()\n        torch.cuda.empty_cache()\n        print(\"✓ Reference model manager shutdown\")\n\nprint(\"✓ Reference model manager defined\")"
  },
  {
   "cell_type": "markdown",
   "id": "ff036fb5",
   "metadata": {},
   "source": [
    "# Multi-Objective Reward System with Pareto Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b7f6c2",
   "metadata": {},
   "outputs": [],
   "source": "class ParetoRewardTracker:\n    \"\"\"\n    Track multi-objective rewards and maintain Pareto frontier.\n    Plug in your own reward functions!\n    \"\"\"\n    \n    def __init__(self, objective_names: List[str]):\n        self.objectives = objective_names\n        self.num_objectives = len(objective_names)\n        self.pareto_frontier: List[np.ndarray] = []\n        self.all_rewards: List[np.ndarray] = []\n        \n        # YOU OVERRIDE THESE - map objective name to function\n        self.reward_functions: Dict[str, Callable[[RolloutState], float]] = {}\n    \n    def register_reward_function(self, name: str, fn: Callable[[RolloutState], float]):\n        \"\"\"Register a reward function for an objective\"\"\"\n        if name not in self.objectives:\n            raise ValueError(f\"Objective {name} not in {self.objectives}\")\n        self.reward_functions[name] = fn\n    \n    def compute_rewards(self, state: RolloutState) -> np.ndarray:\n        \"\"\"Compute reward vector for a completed rollout\"\"\"\n        rewards = np.zeros(self.num_objectives)\n        \n        for i, obj_name in enumerate(self.objectives):\n            if obj_name in self.reward_functions:\n                rewards[i] = self.reward_functions[obj_name](state)\n            else:\n                # Default: use cumulative reward\n                rewards[i] = state.cumulative_reward[i] if i < len(state.cumulative_reward) else 0.0\n        \n        return rewards\n    \n    def update_pareto_frontier(self, reward_vector: np.ndarray):\n        \"\"\"Update Pareto frontier with new reward vector\"\"\"\n        self.all_rewards.append(reward_vector)\n        \n        # Check if new point is dominated\n        is_dominated = False\n        for frontier_point in self.pareto_frontier:\n            if self._dominates(frontier_point, reward_vector):\n                is_dominated = True\n                break\n        \n        if not is_dominated:\n            # Remove points dominated by new vector\n            self.pareto_frontier = [\n                p for p in self.pareto_frontier\n                if not self._dominates(reward_vector, p)\n            ]\n            self.pareto_frontier.append(reward_vector)\n    \n    def _dominates(self, a: np.ndarray, b: np.ndarray) -> bool:\n        \"\"\"Check if a dominates b (a is better in all objectives)\"\"\"\n        return np.all(a >= b) and np.any(a > b)\n    \n    def is_on_pareto_frontier(self, reward_vector: np.ndarray) -> bool:\n        \"\"\"Check if point is on Pareto frontier\"\"\"\n        for p in self.pareto_frontier:\n            if np.allclose(p, reward_vector):\n                return True\n        return False\n    \n    def get_pareto_weights(self, strategy: str = \"random\") -> np.ndarray:\n        \"\"\"\n        Get weight vector for scalarizing multi-objective rewards.\n        Used in training to sample different trade-offs.\n        \"\"\"\n        if strategy == \"random\":\n            # Random scalarization (linear scalarization)\n            return np.random.dirichlet(np.ones(self.num_objectives))\n        elif strategy == \"uniform\":\n            return np.ones(self.num_objectives) / self.num_objectives\n        elif strategy == \"maximize_first\":\n            weights = np.zeros(self.num_objectives)\n            weights[0] = 1.0\n            return weights\n        else:\n            raise ValueError(f\"Unknown strategy: {strategy}\")\n    \n    def scalarize_reward(self, reward_vector: np.ndarray, weights: Optional[np.ndarray] = None) -> float:\n        \"\"\"Convert multi-objective reward to scalar\"\"\"\n        if weights is None:\n            weights = np.ones(self.num_objectives) / self.num_objectives\n        return float(np.dot(weights, reward_vector))\n    \n    def get_frontier_statistics(self) -> Dict:\n        \"\"\"Get statistics about Pareto frontier\"\"\"\n        if not self.pareto_frontier:\n            return {\"size\": 0}\n        \n        frontier_array = np.array(self.pareto_frontier)\n        return {\n            \"size\": len(self.pareto_frontier),\n            \"mean\": frontier_array.mean(axis=0).tolist(),\n            \"std\": frontier_array.std(axis=0).tolist(),\n            \"min\": frontier_array.min(axis=0).tolist(),\n            \"max\": frontier_array.max(axis=0).tolist(),\n        }\n\nprint(\"✓ ParetoRewardTracker defined\")"
  },
  {
   "cell_type": "markdown",
   "id": "784ed069",
   "metadata": {},
   "source": [
    "# SVRL Environment Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f8c87d",
   "metadata": {},
   "outputs": [],
   "source": "class SVRLEnvironment:\n    \"\"\"\n    Self-Verifying RL Environment.\n    Model can query environment for information at a cost.\n    You implement the verification logic!\n    \"\"\"\n    \n    def __init__(\n        self,\n        cost_function: Optional[Callable[[str, RolloutState], float]] = None,\n        verification_handler: Optional[Callable[[str], Any]] = None,\n    ):\n        # YOU OVERRIDE THESE\n        self.cost_fn = cost_function or self._default_cost\n        self.verification_handler = verification_handler or self._default_verification\n        \n        # Tracking\n        self.total_queries = 0\n        self.total_cost_spent = 0.0\n        self.query_history: List[Dict] = []\n    \n    def _default_cost(self, query: str, state: RolloutState) -> float:\n        \"\"\"Default cost: based on query length\"\"\"\n        return len(query.split()) * 0.1\n    \n    def _default_verification(self, query: str) -> str:\n        \"\"\"Default verification: placeholder\"\"\"\n        return f\"[Verification result for: {query}]\"\n    \n    def query_environment(\n        self,\n        query: str,\n        state: RolloutState,\n    ) -> Tuple[Optional[Any], float, bool]:\n        \"\"\"\n        Model queries environment for verification.\n        \n        Returns:\n            result: Verification result (None if over budget)\n            cost: Cost of this query\n            success: Whether query was successful\n        \"\"\"\n        cost = self.cost_fn(query, state)\n        \n        if state.can_verify(cost):\n            state.verification_budget_spent += cost\n            result = self.verification_handler(query)\n            \n            # Track query\n            self.total_queries += 1\n            self.total_cost_spent += cost\n            self.query_history.append({\n                \"rollout_id\": state.rollout_id,\n                \"step\": state.step_count,\n                \"query\": query,\n                \"cost\": cost,\n                \"budget_remaining\": state.max_verification_budget - state.verification_budget_spent,\n            })\n            \n            return result, cost, True\n        else:\n            return None, cost, False\n    \n    def parse_verification_request(self, model_output: str) -> Optional[str]:\n        \"\"\"\n        Parse model output to extract verification request.\n        You can customize this format!\n        \n        Default format: VERIFY: <query>\n        \"\"\"\n        if \"VERIFY:\" in model_output:\n            parts = model_output.split(\"VERIFY:\", 1)\n            if len(parts) > 1:\n                query = parts[1].strip()\n                # Extract until newline or end\n                query = query.split(\"\\n\")[0].strip()\n                return query\n        return None\n    \n    def get_statistics(self) -> Dict:\n        \"\"\"Get environment statistics\"\"\"\n        return {\n            \"total_queries\": self.total_queries,\n            \"total_cost\": self.total_cost_spent,\n            \"avg_cost_per_query\": self.total_cost_spent / max(1, self.total_queries),\n            \"recent_queries\": self.query_history[-10:],\n        }\n    \n    def reset_tracking(self):\n        \"\"\"Reset tracking for new experiment\"\"\"\n        self.total_queries = 0\n        self.total_cost_spent = 0.0\n        self.query_history.clear()\n\nprint(\"✓ SVRLEnvironment defined\")"
  },
  {
   "cell_type": "markdown",
   "id": "ab7813c3",
   "metadata": {},
   "source": [
    "# Efficient Trajectory buffer lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a0b3d8",
   "metadata": {},
   "outputs": [],
   "source": "class TrajectoryBuffer:\n    \"\"\"\n    Buffer for completed trajectories with various sampling strategies.\n    Supports priority sampling for RL training.\n    \"\"\"\n    \n    def __init__(\n        self,\n        max_buffer_size: int = 10000,\n        num_objectives: int = 1,\n    ):\n        self.max_size = max_buffer_size\n        self.num_objectives = num_objectives\n        self.trajectories: deque = deque(maxlen=max_buffer_size)\n        self.step_statistics: Dict[int, List[np.ndarray]] = defaultdict(list)\n        \n        # Optional: Pareto tracker for priority sampling\n        self.pareto_tracker: Optional[ParetoRewardTracker] = None\n    \n    def add_trajectory(self, state: RolloutState):\n        \"\"\"Add completed trajectory to buffer\"\"\"\n        self.trajectories.append(state)\n        \n        # Track statistics by trajectory length\n        self.step_statistics[state.step_count].append(state.cumulative_reward)\n        \n        # Update Pareto frontier if tracker available\n        if self.pareto_tracker:\n            self.pareto_tracker.update_pareto_frontier(state.cumulative_reward)\n    \n    def add_batch(self, states: List[RolloutState]):\n        \"\"\"Add batch of completed trajectories\"\"\"\n        for state in states:\n            self.add_trajectory(state)\n    \n    def sample_batch(\n        self,\n        batch_size: int,\n        strategy: str = \"uniform\",\n    ) -> List[RolloutState]:\n        \"\"\"\n        Sample batch of trajectories for training.\n        \n        Strategies:\n        - uniform: Random sampling\n        - completion_weighted: Prefer longer trajectories\n        - pareto_weighted: Prefer Pareto-optimal trajectories\n        - reward_weighted: Prefer high-reward trajectories\n        \"\"\"\n        if len(self.trajectories) == 0:\n            return []\n        \n        batch_size = min(batch_size, len(self.trajectories))\n        \n        if strategy == \"uniform\":\n            return random.sample(list(self.trajectories), batch_size)\n        \n        elif strategy == \"completion_weighted\":\n            # Prefer longer trajectories\n            weights = np.array([len(t.trajectory) for t in self.trajectories], dtype=float)\n            weights = weights / weights.sum()\n            indices = np.random.choice(len(self.trajectories), size=batch_size, p=weights, replace=False)\n            return [list(self.trajectories)[i] for i in indices]\n        \n        elif strategy == \"pareto_weighted\":\n            # Prefer Pareto-optimal trajectories\n            if not self.pareto_tracker:\n                return self.sample_batch(batch_size, \"uniform\")\n            \n            weights = np.array([\n                2.0 if self.pareto_tracker.is_on_pareto_frontier(t.cumulative_reward) else 1.0\n                for t in self.trajectories\n            ])\n            weights = weights / weights.sum()\n            indices = np.random.choice(len(self.trajectories), size=batch_size, p=weights, replace=False)\n            return [list(self.trajectories)[i] for i in indices]\n        \n        elif strategy == \"reward_weighted\":\n            # Prefer high-reward trajectories (sum across objectives)\n            weights = np.array([t.cumulative_reward.sum() for t in self.trajectories], dtype=float)\n            weights = weights - weights.min() + 1e-6  # Shift to positive\n            weights = weights / weights.sum()\n            indices = np.random.choice(len(self.trajectories), size=batch_size, p=weights, replace=False)\n            return [list(self.trajectories)[i] for i in indices]\n        \n        else:\n            raise ValueError(f\"Unknown sampling strategy: {strategy}\")\n    \n    def prepare_training_batch(\n        self,\n        trajectories: List[RolloutState],\n        weights: Optional[np.ndarray] = None,\n    ) -> TrainingBatch:\n        \"\"\"\n        Convert trajectories to training batch format.\n        Flattens all steps for batch processing.\n        \"\"\"\n        prompts = []\n        responses = []\n        token_ids = []\n        rewards = []\n        \n        for traj in trajectories:\n            for step in traj.trajectory:\n                prompts.append(step.prompt)\n                responses.append(step.response.text)\n                token_ids.append(step.response.token_ids)\n                \n                # Scalarize reward if multi-objective\n                if weights is not None and step.reward is not None:\n                    scalar_reward = np.dot(weights, step.reward)\n                else:\n                    scalar_reward = step.reward.sum() if step.reward is not None else 0.0\n                rewards.append(scalar_reward)\n        \n        return TrainingBatch(\n            prompts=prompts,\n            responses=responses,\n            token_ids=token_ids,\n            rewards=np.array(rewards),\n        )\n    \n    def get_statistics(self) -> Dict:\n        \"\"\"Get buffer statistics\"\"\"\n        if not self.trajectories:\n            return {\"size\": 0}\n        \n        all_lengths = [len(t.trajectory) for t in self.trajectories]\n        all_rewards = [t.cumulative_reward for t in self.trajectories]\n        \n        return {\n            \"size\": len(self.trajectories),\n            \"avg_length\": np.mean(all_lengths),\n            \"std_length\": np.std(all_lengths),\n            \"min_length\": min(all_lengths),\n            \"max_length\": max(all_lengths),\n            \"avg_reward\": np.mean(all_rewards, axis=0).tolist() if all_rewards else [],\n            \"step_distribution\": {k: len(v) for k, v in self.step_statistics.items()},\n        }\n    \n    def clear(self):\n        \"\"\"Clear buffer\"\"\"\n        self.trajectories.clear()\n        self.step_statistics.clear()\n\nprint(\"✓ TrajectoryBuffer defined\")"
  },
  {
   "cell_type": "markdown",
   "id": "571f83a2",
   "metadata": {},
   "source": [
    "# training loop with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92de6ba1",
   "metadata": {},
   "outputs": [],
   "source": "class RLTrainer:\n    \"\"\"\n    Main RL trainer that orchestrates rollouts and gradient updates.\n    Modular design - swap out components as needed.\n    \"\"\"\n    \n    def __init__(\n        self,\n        # Inference backend for fast rollout generation\n        inference_backend: InferenceBackend,\n        # HuggingFace model for gradient computation\n        training_backend: HuggingFaceBackend,\n        # Reference model manager for KL divergence\n        ref_model_manager: ReferenceModelManager,\n        # Pareto reward tracking\n        pareto_tracker: ParetoRewardTracker,\n        # Config\n        num_objectives: int = 1,\n        max_steps: int = 50,\n        batch_size: int = 4,\n        learning_rate: float = 1e-5,\n        kl_coef: float = 0.1,\n        max_grad_norm: float = 1.0,\n        use_wandb: bool = False,\n    ):\n        self.inference_backend = inference_backend\n        self.training_backend = training_backend\n        self.ref_model_manager = ref_model_manager\n        self.pareto_tracker = pareto_tracker\n        \n        self.num_objectives = num_objectives\n        self.max_steps = max_steps\n        self.batch_size = batch_size\n        self.kl_coef = kl_coef\n        self.max_grad_norm = max_grad_norm\n        self.use_wandb = use_wandb\n        \n        # Rollout manager\n        self.rollout_manager = AsyncRolloutManager(\n            inference_backend=inference_backend,\n            max_concurrent_rollouts=batch_size * 2,\n            max_steps=max_steps,\n            num_objectives=num_objectives,\n        )\n        \n        # Trajectory buffer\n        self.buffer = TrajectoryBuffer(\n            max_buffer_size=10000,\n            num_objectives=num_objectives,\n        )\n        self.buffer.pareto_tracker = pareto_tracker\n        \n        # Optimizer (on training model)\n        self.training_backend.model.train()\n        self.optimizer = torch.optim.AdamW(\n            self.training_backend.model.parameters(),\n            lr=learning_rate,\n        )\n        \n        # Metrics\n        self.step_count = 0\n        self.train_metrics: List[Dict] = []\n    \n    def collect_rollouts(\n        self,\n        prompts: List[str],\n        max_new_tokens: int = 256,\n        temperature: float = 1.0,\n    ) -> List[RolloutState]:\n        \"\"\"Collect complete rollouts for given prompts\"\"\"\n        # Start rollouts\n        for prompt in prompts:\n            self.rollout_manager.start_rollout(prompt)\n        \n        # Step until all complete\n        with tqdm(total=self.max_steps, desc=\"Rollout steps\") as pbar:\n            while self.rollout_manager.has_active_rollouts():\n                self.rollout_manager.step_all(\n                    max_new_tokens=max_new_tokens,\n                    temperature=temperature,\n                )\n                \n                # Move completed to buffer\n                completed = self.rollout_manager.complete_terminated_rollouts()\n                for state in completed:\n                    self.buffer.add_trajectory(state)\n                \n                pbar.update(1)\n        \n        # Return all completed from this round\n        return self.rollout_manager.completed_rollouts[-len(prompts):]\n    \n    def compute_policy_gradient_loss(\n        self,\n        batch: TrainingBatch,\n        weights: Optional[np.ndarray] = None,\n    ) -> Tuple[torch.Tensor, Dict]:\n        \"\"\"\n        Compute policy gradient loss.\n        Simple REINFORCE with baseline.\n        You can extend this for PPO, GRPO, etc.\n        \"\"\"\n        # Enable gradients on training model\n        self.training_backend.model.train()\n        \n        # Compute log probabilities\n        total_loss = torch.tensor(0.0, device=self.training_backend.device)\n        \n        for i in range(len(batch.prompts)):\n            prompt = batch.prompts[i]\n            response = batch.responses[i]\n            reward = batch.rewards[i]\n            \n            # Get full encoding\n            full_text = prompt + response\n            prompt_len = len(self.training_backend.tokenizer.encode(prompt))\n            \n            encoding = self.training_backend.tokenizer(\n                full_text,\n                return_tensors=\"pt\",\n                padding=True,\n            )\n            inputs = {k: v.to(self.training_backend.device) for k, v in encoding.items()}\n            \n            # Forward pass\n            outputs = self.training_backend.model(**inputs)\n            logits = outputs.logits\n            \n            # Compute log probs for response tokens\n            start_idx = prompt_len - 1\n            response_logits = logits[0, start_idx:-1]\n            response_ids = encoding[\"input_ids\"][0, prompt_len:]\n            \n            log_probs = F.log_softmax(response_logits, dim=-1)\n            token_log_probs = log_probs.gather(\n                1, response_ids.unsqueeze(1).to(self.training_backend.device)\n            ).squeeze()\n            \n            # Policy gradient: -reward * log_prob\n            # Negative because we want to maximize reward\n            pg_loss = -reward * token_log_probs.sum()\n            total_loss = total_loss + pg_loss\n        \n        avg_loss = total_loss / len(batch.prompts)\n        \n        # Compute KL divergence\n        kl_div = self.ref_model_manager.compute_kl_divergence(\n            batch.prompts,\n            batch.responses,\n            self.training_backend.model,\n            self.training_backend.tokenizer,\n        )\n        \n        # Total loss with KL penalty\n        final_loss = avg_loss + self.kl_coef * kl_div\n        \n        metrics = {\n            \"pg_loss\": avg_loss.item(),\n            \"kl_div\": kl_div.item(),\n            \"total_loss\": final_loss.item(),\n            \"avg_reward\": float(np.mean(batch.rewards)),\n        }\n        \n        return final_loss, metrics\n    \n    def train_step(\n        self,\n        prompts: List[str],\n        scalarization_strategy: str = \"random\",\n    ) -> Dict:\n        \"\"\"\n        Single training step:\n        1. Collect rollouts\n        2. Sample from buffer\n        3. Compute loss and update\n        \"\"\"\n        # 1. Collect rollouts\n        self.collect_rollouts(prompts)\n        \n        # 2. Sample batch from buffer\n        sampled_trajs = self.buffer.sample_batch(\n            batch_size=self.batch_size,\n            strategy=\"completion_weighted\",\n        )\n        \n        # 3. Get scalarization weights for multi-objective\n        weights = self.pareto_tracker.get_pareto_weights(strategy=scalarization_strategy)\n        \n        # 4. Prepare training batch\n        train_batch = self.buffer.prepare_training_batch(sampled_trajs, weights)\n        \n        # 5. Compute loss\n        loss, metrics = self.compute_policy_gradient_loss(train_batch, weights)\n        \n        # 6. Backward pass\n        self.optimizer.zero_grad()\n        loss.backward()\n        \n        # Gradient clipping\n        grad_norm = torch.nn.utils.clip_grad_norm_(\n            self.training_backend.model.parameters(),\n            self.max_grad_norm,\n        )\n        metrics[\"grad_norm\"] = grad_norm.item()\n        \n        # 7. Optimizer step\n        self.optimizer.step()\n        \n        # 8. Update tracking\n        self.step_count += 1\n        metrics[\"step\"] = self.step_count\n        metrics[\"buffer_size\"] = len(self.buffer.trajectories)\n        metrics[\"pareto_frontier_size\"] = len(self.pareto_tracker.pareto_frontier)\n        metrics[\"weights\"] = weights.tolist()\n        \n        self.train_metrics.append(metrics)\n        \n        # Log to wandb if enabled\n        if self.use_wandb:\n            try:\n                wandb.log(metrics, step=self.step_count)\n            except Exception:\n                pass\n        \n        return metrics\n    \n    def train(\n        self,\n        prompt_generator: Callable[[], List[str]],\n        num_iterations: int = 100,\n        eval_every: int = 10,\n        save_every: int = 50,\n        checkpoint_dir: str = \"./checkpoints\",\n    ):\n        \"\"\"\n        Main training loop.\n        \n        Args:\n            prompt_generator: Function that returns batch of prompts\n            num_iterations: Number of training iterations\n        \"\"\"\n        print(f\"Starting training for {num_iterations} iterations\")\n        \n        os.makedirs(checkpoint_dir, exist_ok=True)\n        \n        for i in tqdm(range(num_iterations), desc=\"Training\"):\n            # Get prompts\n            prompts = prompt_generator()\n            \n            # Train step\n            metrics = self.train_step(prompts)\n            \n            # Log progress\n            if (i + 1) % eval_every == 0:\n                print(f\"\\nIteration {i+1}:\")\n                print(f\"  Loss: {metrics['total_loss']:.4f}\")\n                print(f\"  KL: {metrics['kl_div']:.4f}\")\n                print(f\"  Avg Reward: {metrics['avg_reward']:.4f}\")\n                print(f\"  Buffer Size: {metrics['buffer_size']}\")\n                print(f\"  Pareto Frontier: {metrics['pareto_frontier_size']}\")\n            \n            # Save checkpoint\n            if (i + 1) % save_every == 0:\n                self.save_checkpoint(\n                    f\"{checkpoint_dir}/checkpoint_{i+1}.pt\"\n                )\n    \n    def save_checkpoint(self, path: str):\n        \"\"\"Save training checkpoint\"\"\"\n        checkpoint = {\n            \"step\": self.step_count,\n            \"model_state\": self.training_backend.model.state_dict(),\n            \"optimizer_state\": self.optimizer.state_dict(),\n            \"metrics\": self.train_metrics,\n            \"pareto_frontier\": [p.tolist() for p in self.pareto_tracker.pareto_frontier],\n        }\n        torch.save(checkpoint, path)\n        print(f\"✓ Saved checkpoint to {path}\")\n    \n    def load_checkpoint(self, path: str):\n        \"\"\"Load training checkpoint\"\"\"\n        checkpoint = torch.load(path, map_location=self.training_backend.device)\n        self.step_count = checkpoint[\"step\"]\n        self.training_backend.model.load_state_dict(checkpoint[\"model_state\"])\n        self.optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n        self.train_metrics = checkpoint[\"metrics\"]\n        self.pareto_tracker.pareto_frontier = [\n            np.array(p) for p in checkpoint[\"pareto_frontier\"]\n        ]\n        print(f\"✓ Loaded checkpoint from {path}\")\n\nprint(\"✓ RLTrainer defined\")"
  },
  {
   "cell_type": "markdown",
   "id": "ddaa45f6",
   "metadata": {},
   "source": [
    "# Modular Configuration System"
   ]
  },
  {
   "cell_type": "code",
   "id": "c51fd33d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": "# =============================================================================\n# CONFIGURATION: Experiment Settings\n# =============================================================================\n\n@dataclass\nclass ExperimentConfig:\n    \"\"\"Configuration for RL experiments\"\"\"\n    \n    # Model settings\n    model_name: str = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n    dtype: str = \"bfloat16\"\n    gradient_checkpointing: bool = True\n    \n    # Training settings\n    num_objectives: int = 2\n    max_steps: int = 50\n    batch_size: int = 4\n    learning_rate: float = 1e-6\n    kl_coef: float = 0.1\n    max_grad_norm: float = 1.0\n    \n    # Rollout settings\n    max_new_tokens: int = 256\n    temperature: float = 1.0\n    top_p: float = 0.95\n    \n    # SVRL settings (if using)\n    svrl_enabled: bool = False\n    verification_budget: float = 10.0\n    \n    # Buffer settings\n    buffer_size: int = 10000\n    sampling_strategy: str = \"completion_weighted\"  # uniform, pareto_weighted, reward_weighted\n    \n    # Dynamic reference model\n    update_ref_model: bool = False\n    ref_update_frequency: int = 100  # steps\n    \n    # Logging\n    use_wandb: bool = False\n    checkpoint_dir: str = \"./checkpoints\"\n    save_every: int = 50\n    \n    def to_dict(self) -> Dict:\n        return asdict(self)\n    \n    def save(self, path: str):\n        with open(path, \"w\") as f:\n            json.dump(self.to_dict(), f, indent=2)\n        print(f\"✓ Config saved to {path}\")\n    \n    @classmethod\n    def load(cls, path: str) -> \"ExperimentConfig\":\n        with open(path, \"r\") as f:\n            data = json.load(f)\n        return cls(**data)\n\n\n# Example configurations for different experiments\ndef get_pareto_rl_config() -> ExperimentConfig:\n    \"\"\"Config for Pareto/Multi-objective RL experiments\"\"\"\n    return ExperimentConfig(\n        model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n        num_objectives=2,  # task_completion, efficiency\n        max_steps=50,\n        batch_size=4,\n        learning_rate=1e-6,\n        kl_coef=0.1,\n    )\n\n\ndef get_svrl_config() -> ExperimentConfig:\n    \"\"\"Config for Self-Verifying RL experiments\"\"\"\n    return ExperimentConfig(\n        model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n        num_objectives=3,  # correctness, efficiency, verification_cost\n        svrl_enabled=True,\n        verification_budget=10.0,\n        max_steps=30,\n    )\n\n\ndef get_no_kl_config() -> ExperimentConfig:\n    \"\"\"Config for experiments without KL penalty\"\"\"\n    return ExperimentConfig(\n        model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n        kl_coef=0.0,  # No KL penalty\n        max_steps=20,\n    )\n\n\ndef get_dynamic_ref_config() -> ExperimentConfig:\n    \"\"\"Config for dynamic reference model experiments\"\"\"\n    return ExperimentConfig(\n        model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n        update_ref_model=True,\n        ref_update_frequency=50,\n        kl_coef=0.05,  # Lower KL coef since ref model updates\n    )\n\n\nprint(\"✓ Configuration system defined\")\nprint(\"\\\\nPreset configs available:\")\nprint(\"  - get_pareto_rl_config()\")\nprint(\"  - get_svrl_config()\")\nprint(\"  - get_no_kl_config()\")\nprint(\"  - get_dynamic_ref_config()\")"
  },
  {
   "cell_type": "markdown",
   "id": "55d17089",
   "metadata": {},
   "source": "# Implementation Summary\n\n## What's Implemented:\n- **Dynamic batching**: Groups rollouts by step count for efficient inference\n- **Padding masks**: Handled automatically via HuggingFace tokenizers\n- **Priority sampling**: Multiple strategies (completion_weighted, pareto_weighted, reward_weighted)\n- **Variable-length rollouts**: AsyncRolloutManager handles different trajectory lengths\n- **Full parameter updates**: No LoRA - direct gradient computation on all weights\n\n## Key Features:\n- **Pareto frontier tracking**: Maintains non-dominated solutions for multi-objective optimization\n- **Dynamic reference model**: Snapshot and restore training model weights mid-training\n- **KL divergence control**: Optional penalty to keep policy close to reference\n- **SVRL environment**: Query environment for verification at a cost\n- **Checkpointing**: Save/load training state including Pareto frontier"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fcb9c0",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# UTILITY: Visualization and Analysis\n# =============================================================================\n\ndef plot_pareto_frontier(tracker: ParetoRewardTracker):\n    \"\"\"Plot 2D Pareto frontier\"\"\"\n    import matplotlib.pyplot as plt\n    \n    if tracker.num_objectives != 2:\n        print(\"Plotting only supports 2 objectives\")\n        return\n    \n    if not tracker.pareto_frontier:\n        print(\"No Pareto frontier points yet\")\n        return\n    \n    frontier = np.array(tracker.pareto_frontier)\n    all_points = np.array(tracker.all_rewards)\n    \n    fig, ax = plt.subplots(figsize=(10, 8))\n    \n    # Plot all points\n    ax.scatter(all_points[:, 0], all_points[:, 1], alpha=0.3, label=\"All trajectories\")\n    \n    # Plot Pareto frontier\n    ax.scatter(frontier[:, 0], frontier[:, 1], color=\"red\", s=100, marker=\"*\", \n               label=\"Pareto frontier\", zorder=5)\n    \n    # Sort frontier for line plot\n    sorted_idx = np.argsort(frontier[:, 0])\n    ax.plot(frontier[sorted_idx, 0], frontier[sorted_idx, 1], \"r--\", alpha=0.5)\n    \n    ax.set_xlabel(tracker.objectives[0])\n    ax.set_ylabel(tracker.objectives[1])\n    ax.set_title(\"Pareto Frontier\")\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    plt.show()\n\n\ndef plot_training_metrics(trainer: RLTrainer):\n    \"\"\"Plot training metrics over time\"\"\"\n    import matplotlib.pyplot as plt\n    \n    if not trainer.train_metrics:\n        print(\"No training metrics yet\")\n        return\n    \n    df = pd.DataFrame(trainer.train_metrics)\n    \n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    \n    # Loss\n    axes[0, 0].plot(df[\"step\"], df[\"total_loss\"], label=\"Total Loss\")\n    axes[0, 0].plot(df[\"step\"], df[\"pg_loss\"], label=\"PG Loss\", alpha=0.7)\n    axes[0, 0].set_xlabel(\"Step\")\n    axes[0, 0].set_ylabel(\"Loss\")\n    axes[0, 0].set_title(\"Training Loss\")\n    axes[0, 0].legend()\n    axes[0, 0].grid(True, alpha=0.3)\n    \n    # KL Divergence\n    axes[0, 1].plot(df[\"step\"], df[\"kl_div\"])\n    axes[0, 1].set_xlabel(\"Step\")\n    axes[0, 1].set_ylabel(\"KL Divergence\")\n    axes[0, 1].set_title(\"KL Divergence from Reference\")\n    axes[0, 1].grid(True, alpha=0.3)\n    \n    # Average Reward\n    axes[1, 0].plot(df[\"step\"], df[\"avg_reward\"])\n    axes[1, 0].set_xlabel(\"Step\")\n    axes[1, 0].set_ylabel(\"Average Reward\")\n    axes[1, 0].set_title(\"Average Reward per Step\")\n    axes[1, 0].grid(True, alpha=0.3)\n    \n    # Buffer and Pareto size\n    axes[1, 1].plot(df[\"step\"], df[\"buffer_size\"], label=\"Buffer Size\")\n    axes[1, 1].plot(df[\"step\"], df[\"pareto_frontier_size\"], label=\"Pareto Frontier\")\n    axes[1, 1].set_xlabel(\"Step\")\n    axes[1, 1].set_ylabel(\"Count\")\n    axes[1, 1].set_title(\"Buffer & Pareto Statistics\")\n    axes[1, 1].legend()\n    axes[1, 1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\n\ndef analyze_trajectory_lengths(buffer: TrajectoryBuffer):\n    \"\"\"Analyze distribution of trajectory lengths\"\"\"\n    import matplotlib.pyplot as plt\n    \n    if not buffer.trajectories:\n        print(\"No trajectories in buffer\")\n        return\n    \n    lengths = [len(t.trajectory) for t in buffer.trajectories]\n    \n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # Histogram\n    axes[0].hist(lengths, bins=20, edgecolor=\"black\")\n    axes[0].axvline(np.mean(lengths), color=\"red\", linestyle=\"--\", label=f\"Mean: {np.mean(lengths):.1f}\")\n    axes[0].set_xlabel(\"Trajectory Length (steps)\")\n    axes[0].set_ylabel(\"Count\")\n    axes[0].set_title(\"Distribution of Trajectory Lengths\")\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n    \n    # Box plot\n    axes[1].boxplot(lengths)\n    axes[1].set_ylabel(\"Steps\")\n    axes[1].set_title(\"Trajectory Length Statistics\")\n    axes[1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"Length statistics:\")\n    print(f\"  Mean: {np.mean(lengths):.2f}\")\n    print(f\"  Std: {np.std(lengths):.2f}\")\n    print(f\"  Min: {min(lengths)}\")\n    print(f\"  Max: {max(lengths)}\")\n    print(f\"  Median: {np.median(lengths):.1f}\")\n\nprint(\"✓ Visualization utilities defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745523ad",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# EXAMPLE: How to use the RL harness (FULL PARAMETER UPDATES)\n# =============================================================================\n# This shows how to plug everything together.\n# You customize: rewards, termination, prompts\n\ndef example_setup():\n    \"\"\"\n    Example setup for Pareto RL experiment.\n    Uses full parameter updates (no LoRA).\n    \n    NOTE: For 7B model with full updates:\n    - Inference: ~14GB VRAM (bfloat16)\n    - Training: ~28GB VRAM (weights + gradients + optimizer states)\n    - Total with ref model: ~42GB+ VRAM\n    \n    Use gradient_checkpointing=True to reduce memory at cost of speed.\n    Or use smaller model (1B-3B) for experiments.\n    \"\"\"\n    \n    # MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"  # Requires 40GB+ VRAM\n    MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Good for testing\n    # MODEL_NAME = \"facebook/opt-1.3b\"  # Another small option\n    \n    # 1. Setup inference backend (for fast rollout generation)\n    # Option A: Same HuggingFace model (simplest, slower)\n    inference = HuggingFaceBackend(\n        model_name=MODEL_NAME,\n        dtype=\"bfloat16\",\n        gradient_checkpointing=False,\n    )\n    \n    # Option B: vLLM (faster, uncomment if installed)\n    # inference = VLLMBackend(\n    #     model_name=MODEL_NAME,\n    #     tensor_parallel_size=1,\n    #     gpu_memory_utilization=0.3,  # Leave room for training model\n    # )\n    \n    # 2. Setup training backend (for gradient computation) - FULL PARAMS\n    training = HuggingFaceBackend(\n        model_name=MODEL_NAME,\n        dtype=\"bfloat16\",\n        gradient_checkpointing=True,  # Enable for memory efficiency\n    )\n    \n    # 3. Setup reference model (for KL divergence)\n    ref_manager = ReferenceModelManager(\n        model_name=MODEL_NAME,\n    )\n    \n    # 4. Setup Pareto reward tracker with 2 objectives\n    objectives = [\"task_completion\", \"efficiency\"]\n    pareto_tracker = ParetoRewardTracker(objectives)\n    \n    # 5. YOUR REWARD FUNCTIONS - customize these!\n    def task_completion_reward(state: RolloutState) -> float:\n        \"\"\"Example: Did the model solve the task?\"\"\"\n        final_response = state.trajectory[-1].response.text if state.trajectory else \"\"\n        if \"ANSWER:\" in final_response:\n            return 1.0\n        return 0.0\n    \n    def efficiency_reward(state: RolloutState) -> float:\n        \"\"\"Example: How efficient was the solution?\"\"\"\n        max_steps = 50\n        efficiency = 1.0 - (state.step_count / max_steps)\n        return max(0.0, efficiency)\n    \n    pareto_tracker.register_reward_function(\"task_completion\", task_completion_reward)\n    pareto_tracker.register_reward_function(\"efficiency\", efficiency_reward)\n    \n    # 6. Create trainer\n    trainer = RLTrainer(\n        inference_backend=inference,\n        training_backend=training,\n        ref_model_manager=ref_manager,\n        pareto_tracker=pareto_tracker,\n        num_objectives=len(objectives),\n        max_steps=20,\n        batch_size=2,  # Smaller batch for memory\n        learning_rate=1e-6,  # Lower LR for full model updates\n        kl_coef=0.1,\n        use_wandb=False,\n    )\n    \n    # 7. YOUR PROMPT GENERATOR\n    def get_prompts() -> List[str]:\n        problems = [\n            \"Solve step by step: What is 15 + 27?\",\n            \"Solve step by step: What is 8 * 9?\",\n        ]\n        return problems\n    \n    # 8. YOUR TERMINATION CONDITION\n    def should_terminate(state: RolloutState) -> bool:\n        if state.trajectory:\n            last_response = state.trajectory[-1].response.text\n            return \"ANSWER:\" in last_response or \"DONE\" in last_response\n        return False\n    \n    trainer.rollout_manager.should_terminate_fn = should_terminate\n    \n    # 9. YOUR STEP REWARD FUNCTION\n    def step_reward(state: RolloutState, step: StepData) -> np.ndarray:\n        return np.array([-0.01, 0.0])\n    \n    trainer.rollout_manager.get_reward_fn = step_reward\n    \n    return trainer, get_prompts\n\n\ndef example_svrl_setup():\n    \"\"\"\n    Example setup for SVRL (Self-Verifying RL) experiment.\n    Model can query environment for verification at a cost.\n    \"\"\"\n    \n    def verify_math(query: str) -> str:\n        try:\n            result = eval(query)\n            return f\"Result: {result}\"\n        except Exception as e:\n            return f\"Error: {str(e)}\"\n    \n    def verification_cost(query: str, state: RolloutState) -> float:\n        base_cost = 1.0\n        step_penalty = state.step_count * 0.1\n        return base_cost + step_penalty\n    \n    svrl_env = SVRLEnvironment(\n        cost_function=verification_cost,\n        verification_handler=verify_math,\n    )\n    \n    return svrl_env\n\n\ndef example_dynamic_ref_model():\n    \"\"\"\n    Example: Change reference model mid-training.\n    Useful for curriculum learning or adaptive KL penalties.\n    \"\"\"\n    def update_ref_model_periodically(trainer: RLTrainer, interval: int = 100):\n        \"\"\"\n        Snapshot training model as new reference every N steps.\n        This lets the policy drift without strong KL penalty.\n        \"\"\"\n        if trainer.step_count % interval == 0 and trainer.step_count > 0:\n            snapshot_id = trainer.ref_model_manager.snapshot_current_training_model(\n                trainer.training_backend.model\n            )\n            print(f\"✓ Updated reference model at step {trainer.step_count}\")\n            return snapshot_id\n        return None\n    \n    return update_ref_model_periodically\n\n\ndef example_no_kl_training():\n    \"\"\"\n    Example: Train without KL divergence penalty.\n    Set kl_coef=0 or modify compute_policy_gradient_loss.\n    \"\"\"\n    class NoKLTrainer(RLTrainer):\n        def compute_policy_gradient_loss(self, batch, weights=None):\n            # Skip KL computation entirely\n            self.training_backend.model.train()\n            total_loss = torch.tensor(0.0, device=self.training_backend.device)\n            \n            for i in range(len(batch.prompts)):\n                prompt = batch.prompts[i]\n                response = batch.responses[i]\n                reward = batch.rewards[i]\n                \n                full_text = prompt + response\n                prompt_len = len(self.training_backend.tokenizer.encode(prompt))\n                \n                encoding = self.training_backend.tokenizer(full_text, return_tensors=\"pt\")\n                inputs = {k: v.to(self.training_backend.device) for k, v in encoding.items()}\n                \n                outputs = self.training_backend.model(**inputs)\n                logits = outputs.logits\n                \n                start_idx = prompt_len - 1\n                response_logits = logits[0, start_idx:-1]\n                response_ids = encoding[\"input_ids\"][0, prompt_len:]\n                \n                log_probs = F.log_softmax(response_logits, dim=-1)\n                token_log_probs = log_probs.gather(\n                    1, response_ids.unsqueeze(1).to(self.training_backend.device)\n                ).squeeze()\n                \n                pg_loss = -reward * token_log_probs.sum()\n                total_loss = total_loss + pg_loss\n            \n            avg_loss = total_loss / len(batch.prompts)\n            \n            metrics = {\n                \"pg_loss\": avg_loss.item(),\n                \"kl_div\": 0.0,  # No KL\n                \"total_loss\": avg_loss.item(),\n                \"avg_reward\": float(np.mean(batch.rewards)),\n            }\n            \n            return avg_loss, metrics\n    \n    return NoKLTrainer\n\n\n# =============================================================================\n# RUN TRAINING (uncomment to execute)\n# =============================================================================\n\"\"\"\n# Setup\ntrainer, prompt_generator = example_setup()\n\n# Optional: Disable KL penalty\n# trainer.kl_coef = 0.0\n\n# Optional: Dynamic reference model updates\n# update_ref_fn = example_dynamic_ref_model()\n\n# Train!\nfor i in range(10):  # Short run\n    prompts = prompt_generator()\n    metrics = trainer.train_step(prompts)\n    print(f\"Step {metrics['step']}: Loss={metrics['total_loss']:.4f}, Reward={metrics['avg_reward']:.4f}\")\n    \n    # Optional: Update ref model\n    # update_ref_fn(trainer, interval=5)\n\n# Check results\nprint(\"\\\\nPareto Frontier:\", trainer.pareto_tracker.get_frontier_statistics())\nprint(\"Buffer Stats:\", trainer.buffer.get_statistics())\n\"\"\"\n\nprint(\"✓ Example setup defined - ready for full parameter updates!\")\nprint(\"\\\\nYour TODO:\")\nprint(\"1. Choose your model (MODEL_NAME) - TinyLlama for testing, larger for real experiments\")\nprint(\"2. Implement YOUR reward functions\")\nprint(\"3. Implement YOUR prompt generator\")\nprint(\"4. Implement YOUR termination conditions\")\nprint(\"5. Run training loop\")\nprint(\"\\\\nMemory requirements (bfloat16):\")\nprint(\"  - 1B model: ~8GB total\")\nprint(\"  - 3B model: ~24GB total\")\nprint(\"  - 7B model: ~42GB total (use gradient_checkpointing)\")\nprint(\"\\\\nFor experiments without KL penalty: set trainer.kl_coef = 0.0\")"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}