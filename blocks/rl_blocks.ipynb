{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dce099",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi -L || true\n",
    "\n",
    "import sys\n",
    "print(\"Python:\", sys.version)\n",
    "\n",
    "# Install required packages\n",
    "# Core ML\n",
    "!uv pip install -q transformers>=4.51.3 accelerate>=1.4.0 peft>=0.14.0 \\\n",
    "                 datasets>=3.3.2 torch wandb huggingface_hub \\\n",
    "                 sentencepiece protobuf tqdm matplotlib pandas\n",
    "\n",
    "# Inference backends (install based on what you'll use)\n",
    "# Uncomment the backend you want:\n",
    "# !uv pip install -q vllm>=0.6.0  # For vLLM backend\n",
    "# !uv pip install -q \"sglang[all]>=0.4.0\"  # For SGLang backend\n",
    "\n",
    "print(\"\\n=== Environment ===\")\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "assert DEVICE == \"cuda\", \"Please connect a GPU for RL training.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zzj88o3ci6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports for RL harness\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import asyncio\n",
    "import uuid\n",
    "from abc import ABC, abstractmethod\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import List, Dict, Optional, Any, Tuple, Callable\n",
    "from collections import defaultdict, deque\n",
    "from queue import Queue\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# HuggingFace\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Optional: vLLM (comment out if not installed)\n",
    "try:\n",
    "    from vllm import LLM, SamplingParams\n",
    "    VLLM_AVAILABLE = True\n",
    "    print(\"✓ vLLM available\")\n",
    "except ImportError:\n",
    "    VLLM_AVAILABLE = False\n",
    "    print(\"⚠ vLLM not installed\")\n",
    "\n",
    "# Optional: SGLang (comment out if not installed)\n",
    "try:\n",
    "    import sglang as sgl\n",
    "    SGLANG_AVAILABLE = True\n",
    "    print(\"✓ SGLang available\")\n",
    "except ImportError:\n",
    "    SGLANG_AVAILABLE = False\n",
    "    print(\"⚠ SGLang not installed\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8vl1q5nqmla",
   "metadata": {},
   "source": [
    "# Core Data Structures\n",
    "Immutable data structures for tracking rollouts and generation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603kjhznm4s",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GenerationResult:\n",
    "    \"\"\"Output from inference backend\"\"\"\n",
    "    text: str\n",
    "    token_ids: List[int]\n",
    "    logprobs: Optional[List[float]] = None  # Log probabilities per token\n",
    "    top_logprobs: Optional[List[Dict[int, float]]] = None  # Top-k logprobs\n",
    "    finish_reason: str = \"length\"  # \"stop\", \"length\", \"error\"\n",
    "    \n",
    "@dataclass\n",
    "class StepData:\n",
    "    \"\"\"Single step in a trajectory\"\"\"\n",
    "    prompt: str\n",
    "    response: GenerationResult\n",
    "    action_logprobs: Optional[torch.Tensor] = None  # For gradient computation\n",
    "    reward: Optional[np.ndarray] = None  # Multi-dimensional reward vector\n",
    "    value_estimate: Optional[float] = None\n",
    "    timestamp: float = field(default_factory=time.time)\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "@dataclass\n",
    "class RolloutState:\n",
    "    \"\"\"Complete state of a single rollout\"\"\"\n",
    "    rollout_id: str\n",
    "    initial_prompt: str\n",
    "    trajectory: List[StepData] = field(default_factory=list)\n",
    "    step_count: int = 0\n",
    "    terminated: bool = False\n",
    "    termination_reason: Optional[str] = None\n",
    "    cumulative_reward: np.ndarray = field(default_factory=lambda: np.zeros(1))\n",
    "    verification_budget_spent: float = 0.0\n",
    "    max_verification_budget: float = 10.0\n",
    "    ref_model_snapshot_id: Optional[str] = None  # For dynamic ref changes\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    def checkpoint(self) -> 'RolloutState':\n",
    "        \"\"\"Create a deep copy for branching\"\"\"\n",
    "        return deepcopy(self)\n",
    "    \n",
    "    def can_verify(self, cost: float) -> bool:\n",
    "        \"\"\"Check if verification is within budget\"\"\"\n",
    "        return self.verification_budget_spent + cost <= self.max_verification_budget\n",
    "    \n",
    "    def add_step(self, step: StepData):\n",
    "        \"\"\"Add a step to trajectory\"\"\"\n",
    "        self.trajectory.append(step)\n",
    "        self.step_count += 1\n",
    "        if step.reward is not None:\n",
    "            self.cumulative_reward = self.cumulative_reward + step.reward\n",
    "    \n",
    "    def get_full_context(self) -> str:\n",
    "        \"\"\"Get the full conversation context\"\"\"\n",
    "        context = self.initial_prompt\n",
    "        for step in self.trajectory:\n",
    "            context += step.response.text\n",
    "        return context\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Serialize for storage\"\"\"\n",
    "        return asdict(self)\n",
    "\n",
    "@dataclass\n",
    "class TrainingBatch:\n",
    "    \"\"\"Batch of data for training update\"\"\"\n",
    "    prompts: List[str]\n",
    "    responses: List[str]\n",
    "    token_ids: List[List[int]]\n",
    "    rewards: np.ndarray  # (batch_size, num_objectives)\n",
    "    old_logprobs: Optional[torch.Tensor] = None  # For importance sampling\n",
    "    advantages: Optional[torch.Tensor] = None\n",
    "    returns: Optional[torch.Tensor] = None\n",
    "    \n",
    "print(\"✓ Data structures defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ac055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# WandB API key - get from https://wandb.ai/authorize\n",
    "WANDB_API_KEY = \"\"  # Your WandB API key\n",
    "if WANDB_API_KEY:\n",
    "    os.environ['WANDB_API_KEY'] = WANDB_API_KEY\n",
    "\n",
    "# HuggingFace token - get from https://huggingface.co/settings/tokens\n",
    "HF_TOKEN = \"\"  # Your HuggingFace token\n",
    "if HF_TOKEN:\n",
    "    os.environ['HF_TOKEN'] = HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5610c0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from huggingface_hub import login\n",
    "\n",
    "# WandB login\n",
    "try:\n",
    "    wandb.login()\n",
    "    print(\"✓ WandB login successful\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ WandB login failed: {e}\")\n",
    "    print(\"Training will continue without WandB logging\")\n",
    "\n",
    "# HuggingFace login\n",
    "try:\n",
    "    if os.environ.get('HF_TOKEN'):\n",
    "        login(token=os.environ['HF_TOKEN'])\n",
    "        print(\"✓ HuggingFace login successful\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ HuggingFace login failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b3a9e8",
   "metadata": {},
   "source": [
    "# Async Rollout manager with dynamic batching\n",
    "Since rollouts vary dramatically in length (6-50 steps), use an async architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d59df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncRolloutManager:\n",
    "    \"\"\"\n",
    "    Manages concurrent rollouts with dynamic batching.\n",
    "    Groups rollouts by step count for efficient batch inference.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        inference_backend: InferenceBackend,\n",
    "        max_concurrent_rollouts: int = 32,\n",
    "        max_steps: int = 50,\n",
    "        num_objectives: int = 1,\n",
    "    ):\n",
    "        self.backend = inference_backend\n",
    "        self.max_concurrent = max_concurrent_rollouts\n",
    "        self.max_steps = max_steps\n",
    "        self.num_objectives = num_objectives\n",
    "        \n",
    "        self.active_rollouts: Dict[str, RolloutState] = {}\n",
    "        self.completed_rollouts: List[RolloutState] = []\n",
    "        self.rollout_counter = 0\n",
    "        \n",
    "        # Customizable hooks - you override these!\n",
    "        self.should_terminate_fn: Optional[Callable[[RolloutState], bool]] = None\n",
    "        self.get_reward_fn: Optional[Callable[[RolloutState, StepData], np.ndarray]] = None\n",
    "        self.process_response_fn: Optional[Callable[[str], str]] = None\n",
    "    \n",
    "    def start_rollout(self, prompt: str, metadata: Optional[Dict] = None) -> str:\n",
    "        \"\"\"Start a new rollout from prompt\"\"\"\n",
    "        if len(self.active_rollouts) >= self.max_concurrent:\n",
    "            raise RuntimeError(f\"Max concurrent rollouts ({self.max_concurrent}) reached\")\n",
    "        \n",
    "        rollout_id = f\"rollout_{self.rollout_counter}\"\n",
    "        self.rollout_counter += 1\n",
    "        \n",
    "        state = RolloutState(\n",
    "            rollout_id=rollout_id,\n",
    "            initial_prompt=prompt,\n",
    "            cumulative_reward=np.zeros(self.num_objectives),\n",
    "            metadata=metadata or {},\n",
    "        )\n",
    "        \n",
    "        self.active_rollouts[rollout_id] = state\n",
    "        return rollout_id\n",
    "    \n",
    "    def step_all(\n",
    "        self,\n",
    "        max_new_tokens: int = 256,\n",
    "        temperature: float = 1.0,\n",
    "        top_p: float = 0.95,\n",
    "        **gen_kwargs\n",
    "    ) -> Dict[str, GenerationResult]:\n",
    "        \"\"\"\n",
    "        Step all active rollouts.\n",
    "        Groups by step count for efficient batching.\n",
    "        Returns: Dict mapping rollout_id to generation result\n",
    "        \"\"\"\n",
    "        if not self.active_rollouts:\n",
    "            return {}\n",
    "        \n",
    "        # Group active rollouts by step count for batching\n",
    "        step_groups: Dict[int, List[str]] = defaultdict(list)\n",
    "        for rid, state in self.active_rollouts.items():\n",
    "            if not state.terminated:\n",
    "                step_groups[state.step_count].append(rid)\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        # Process each step group as a batch\n",
    "        for step_num, rollout_ids in step_groups.items():\n",
    "            # Gather prompts (full context so far)\n",
    "            prompts = [self.active_rollouts[rid].get_full_context() for rid in rollout_ids]\n",
    "            \n",
    "            # Batch generation\n",
    "            gen_results = self.backend.generate(\n",
    "                prompts,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                **gen_kwargs\n",
    "            )\n",
    "            \n",
    "            # Process results for each rollout\n",
    "            for rid, gen_result in zip(rollout_ids, gen_results):\n",
    "                state = self.active_rollouts[rid]\n",
    "                \n",
    "                # Optionally process response\n",
    "                if self.process_response_fn:\n",
    "                    gen_result.text = self.process_response_fn(gen_result.text)\n",
    "                \n",
    "                # Create step data\n",
    "                step_data = StepData(\n",
    "                    prompt=state.get_full_context(),\n",
    "                    response=gen_result,\n",
    "                )\n",
    "                \n",
    "                # Compute reward if function provided\n",
    "                if self.get_reward_fn:\n",
    "                    step_data.reward = self.get_reward_fn(state, step_data)\n",
    "                else:\n",
    "                    step_data.reward = np.zeros(self.num_objectives)\n",
    "                \n",
    "                # Add step to trajectory\n",
    "                state.add_step(step_data)\n",
    "                \n",
    "                # Check termination conditions\n",
    "                terminated = False\n",
    "                termination_reason = None\n",
    "                \n",
    "                # Max steps reached\n",
    "                if state.step_count >= self.max_steps:\n",
    "                    terminated = True\n",
    "                    termination_reason = \"max_steps\"\n",
    "                \n",
    "                # EOS token generated\n",
    "                if gen_result.finish_reason == \"stop\":\n",
    "                    terminated = True\n",
    "                    termination_reason = \"eos\"\n",
    "                \n",
    "                # Custom termination\n",
    "                if self.should_terminate_fn and self.should_terminate_fn(state):\n",
    "                    terminated = True\n",
    "                    termination_reason = \"custom\"\n",
    "                \n",
    "                if terminated:\n",
    "                    state.terminated = True\n",
    "                    state.termination_reason = termination_reason\n",
    "                \n",
    "                all_results[rid] = gen_result\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def complete_terminated_rollouts(self) -> List[RolloutState]:\n",
    "        \"\"\"Move terminated rollouts to completed list\"\"\"\n",
    "        newly_completed = []\n",
    "        to_remove = []\n",
    "        \n",
    "        for rid, state in self.active_rollouts.items():\n",
    "            if state.terminated:\n",
    "                self.completed_rollouts.append(state)\n",
    "                newly_completed.append(state)\n",
    "                to_remove.append(rid)\n",
    "        \n",
    "        for rid in to_remove:\n",
    "            del self.active_rollouts[rid]\n",
    "        \n",
    "        return newly_completed\n",
    "    \n",
    "    def has_active_rollouts(self) -> bool:\n",
    "        \"\"\"Check if there are active rollouts\"\"\"\n",
    "        return len(self.active_rollouts) > 0\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"Get current rollout statistics\"\"\"\n",
    "        active_steps = [s.step_count for s in self.active_rollouts.values()]\n",
    "        completed_steps = [s.step_count for s in self.completed_rollouts]\n",
    "        \n",
    "        return {\n",
    "            \"active\": len(self.active_rollouts),\n",
    "            \"completed\": len(self.completed_rollouts),\n",
    "            \"active_avg_steps\": np.mean(active_steps) if active_steps else 0,\n",
    "            \"completed_avg_steps\": np.mean(completed_steps) if completed_steps else 0,\n",
    "            \"completed_rewards\": [s.cumulative_reward.tolist() for s in self.completed_rollouts[-10:]],\n",
    "        }\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset manager for new training iteration\"\"\"\n",
    "        self.active_rollouts.clear()\n",
    "        self.completed_rollouts.clear()\n",
    "\n",
    "print(\"✓ AsyncRolloutManager defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baedb9df",
   "metadata": {},
   "source": [
    "# Flexible Rollout State with Checkpointing\n",
    "Track everything per-rollout with ability to branch/restore:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814a9571",
   "metadata": {},
   "source": [
    "# Multi-Backend Inference with Unified Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d9add0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceBackend(ABC):\n",
    "    \"\"\"Abstract base class for inference backends\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def generate(self, prompts: List[str], **kwargs) -> List[GenerationResult]:\n",
    "        \"\"\"Generate responses for a batch of prompts\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def shutdown(self):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class HuggingFaceBackend(InferenceBackend):\n",
    "    \"\"\"\n",
    "    HuggingFace backend - full gradient access.\n",
    "    Use this for:\n",
    "    - Gradient computation during training (FULL UPDATES)\n",
    "    - Reference model logit computation\n",
    "    - Small-scale experiments\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        device: str = \"cuda\",\n",
    "        dtype: str = \"bfloat16\",  # \"float32\", \"bfloat16\", \"float16\"\n",
    "        gradient_checkpointing: bool = False,  # Save memory during backward pass\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "        self.dtype_map = {\n",
    "            \"float32\": torch.float32,\n",
    "            \"bfloat16\": torch.bfloat16,\n",
    "            \"float16\": torch.float16,\n",
    "        }\n",
    "        self.torch_dtype = self.dtype_map.get(dtype, torch.bfloat16)\n",
    "        \n",
    "        # Tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Load model - FULL WEIGHTS\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=self.torch_dtype,\n",
    "            device_map=None,  # We'll move to device manually for full control\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        self.model = self.model.to(device)\n",
    "        \n",
    "        # Optional: Gradient checkpointing for memory efficiency\n",
    "        if gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "            print(f\"✓ Gradient checkpointing enabled\")\n",
    "        \n",
    "        # Count trainable parameters\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        print(f\"✓ HuggingFace backend loaded: {model_name}\")\n",
    "        print(f\"  Total params: {total_params:,}\")\n",
    "        print(f\"  Trainable params: {trainable_params:,}\")\n",
    "        print(f\"  Dtype: {dtype}\")\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        max_new_tokens: int = 256,\n",
    "        temperature: float = 1.0,\n",
    "        top_p: float = 0.95,\n",
    "        do_sample: bool = True,\n",
    "        return_logprobs: bool = False,\n",
    "        **kwargs\n",
    "    ) -> List[GenerationResult]:\n",
    "        \"\"\"Generate responses with optional logprobs\"\"\"\n",
    "        self.model.eval()  # Set to eval mode for generation\n",
    "        results = []\n",
    "        \n",
    "        for prompt in prompts:\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    temperature=temperature if do_sample else 1.0,\n",
    "                    top_p=top_p if do_sample else 1.0,\n",
    "                    do_sample=do_sample,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    output_scores=return_logprobs,\n",
    "                    return_dict_in_generate=True,\n",
    "                )\n",
    "            \n",
    "            # Decode\n",
    "            generated_ids = outputs.sequences[0][inputs[\"input_ids\"].shape[1]:]\n",
    "            text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "            \n",
    "            # Extract logprobs if requested\n",
    "            logprobs = None\n",
    "            if return_logprobs and hasattr(outputs, \"scores\"):\n",
    "                logprobs = []\n",
    "                for i, score in enumerate(outputs.scores):\n",
    "                    probs = F.log_softmax(score[0], dim=-1)\n",
    "                    token_id = generated_ids[i].item()\n",
    "                    logprobs.append(probs[token_id].item())\n",
    "            \n",
    "            results.append(GenerationResult(\n",
    "                text=text,\n",
    "                token_ids=generated_ids.tolist(),\n",
    "                logprobs=logprobs,\n",
    "                finish_reason=\"stop\" if generated_ids[-1] == self.tokenizer.eos_token_id else \"length\"\n",
    "            ))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_logits(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        responses: List[str],\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get logits for prompt+response pairs.\n",
    "        Returns: (logits, attention_mask)\n",
    "        Essential for KL divergence computation in RL.\n",
    "        \"\"\"\n",
    "        full_texts = [p + r for p, r in zip(prompts, responses)]\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            full_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        return outputs.logits, inputs[\"attention_mask\"]\n",
    "    \n",
    "    def compute_log_probs(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        responses: List[str],\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute log probabilities of responses given prompts.\n",
    "        Critical for RL training - stable numerical computation.\n",
    "        \"\"\"\n",
    "        full_texts = [p + r for p, r in zip(prompts, responses)]\n",
    "        \n",
    "        # Tokenize\n",
    "        prompt_encodings = self.tokenizer(prompts, padding=True, return_tensors=\"pt\")\n",
    "        full_encodings = self.tokenizer(full_texts, padding=True, return_tensors=\"pt\")\n",
    "        \n",
    "        prompt_lens = [len(self.tokenizer.encode(p)) for p in prompts]\n",
    "        \n",
    "        inputs = {k: v.to(self.device) for k, v in full_encodings.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits  # (batch, seq_len, vocab)\n",
    "        \n",
    "        # Compute log probs for each response token\n",
    "        log_probs_list = []\n",
    "        for i in range(len(prompts)):\n",
    "            # Response starts after prompt\n",
    "            start_idx = prompt_lens[i] - 1  # -1 because we predict next token\n",
    "            response_logits = logits[i, start_idx:-1]  # (response_len, vocab)\n",
    "            response_ids = full_encodings[\"input_ids\"][i, prompt_lens[i]:]\n",
    "            \n",
    "            # Filter out padding\n",
    "            mask = full_encodings[\"attention_mask\"][i, prompt_lens[i]:]\n",
    "            \n",
    "            # Compute log softmax\n",
    "            log_probs = F.log_softmax(response_logits, dim=-1)\n",
    "            \n",
    "            # Gather log probs of actual tokens\n",
    "            token_log_probs = log_probs.gather(1, response_ids.unsqueeze(1).to(self.device)).squeeze()\n",
    "            \n",
    "            # Apply mask and sum\n",
    "            masked_log_probs = token_log_probs * mask.to(self.device).float()\n",
    "            log_probs_list.append(masked_log_probs.sum())\n",
    "        \n",
    "        return torch.stack(log_probs_list)\n",
    "    \n",
    "    def shutdown(self):\n",
    "        del self.model\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"✓ HuggingFace backend shutdown\")\n",
    "\n",
    "\n",
    "class VLLMBackend(InferenceBackend):\n",
    "    \"\"\"\n",
    "    vLLM backend - fast inference with PagedAttention.\n",
    "    Use this for:\n",
    "    - Fast rollout generation\n",
    "    - Large batch inference\n",
    "    Note: vLLM doesn't support gradient computation!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        tensor_parallel_size: int = 1,\n",
    "        gpu_memory_utilization: float = 0.9,\n",
    "        max_model_len: int = 4096,\n",
    "    ):\n",
    "        if not VLLM_AVAILABLE:\n",
    "            raise ImportError(\"vLLM not installed. Run: pip install vllm\")\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.llm = LLM(\n",
    "            model=model_name,\n",
    "            tensor_parallel_size=tensor_parallel_size,\n",
    "            gpu_memory_utilization=gpu_memory_utilization,\n",
    "            max_model_len=max_model_len,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        print(f\"✓ vLLM backend loaded: {model_name}\")\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        max_new_tokens: int = 256,\n",
    "        temperature: float = 1.0,\n",
    "        top_p: float = 0.95,\n",
    "        return_logprobs: bool = False,\n",
    "        n_logprobs: int = 5,\n",
    "        **kwargs\n",
    "    ) -> List[GenerationResult]:\n",
    "        \"\"\"Batch generate with vLLM\"\"\"\n",
    "        \n",
    "        sampling_params = SamplingParams(\n",
    "            max_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            logprobs=n_logprobs if return_logprobs else None,\n",
    "        )\n",
    "        \n",
    "        outputs = self.llm.generate(prompts, sampling_params)\n",
    "        \n",
    "        results = []\n",
    "        for output in outputs:\n",
    "            completion = output.outputs[0]\n",
    "            \n",
    "            logprobs = None\n",
    "            if return_logprobs and completion.logprobs:\n",
    "                logprobs = [lp[completion.token_ids[i]].logprob \n",
    "                           for i, lp in enumerate(completion.logprobs)]\n",
    "            \n",
    "            results.append(GenerationResult(\n",
    "                text=completion.text,\n",
    "                token_ids=list(completion.token_ids),\n",
    "                logprobs=logprobs,\n",
    "                finish_reason=completion.finish_reason or \"length\"\n",
    "            ))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def shutdown(self):\n",
    "        del self.llm\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"✓ vLLM backend shutdown\")\n",
    "\n",
    "\n",
    "class SGLangBackend(InferenceBackend):\n",
    "    \"\"\"\n",
    "    SGLang backend - RadixAttention for efficient KV caching.\n",
    "    Use this for:\n",
    "    - Fast rollout generation\n",
    "    - Prefix caching (good for multi-turn)\n",
    "    Note: SGLang doesn't support gradient computation!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        mem_fraction_static: float = 0.8,\n",
    "        tp_size: int = 1,\n",
    "    ):\n",
    "        if not SGLANG_AVAILABLE:\n",
    "            raise ImportError(\"SGLang not installed. Run: pip install 'sglang[all]'\")\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.engine = sgl.Engine(\n",
    "            model_path=model_name,\n",
    "            mem_fraction_static=mem_fraction_static,\n",
    "            tp_size=tp_size,\n",
    "        )\n",
    "        print(f\"✓ SGLang backend loaded: {model_name}\")\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        max_new_tokens: int = 256,\n",
    "        temperature: float = 1.0,\n",
    "        top_p: float = 0.95,\n",
    "        return_logprobs: bool = False,\n",
    "        n_logprobs: int = 5,\n",
    "        **kwargs\n",
    "    ) -> List[GenerationResult]:\n",
    "        \"\"\"Batch generate with SGLang\"\"\"\n",
    "        \n",
    "        sampling_params = {\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "        }\n",
    "        \n",
    "        if return_logprobs:\n",
    "            outputs = self.engine.generate(\n",
    "                prompts,\n",
    "                sampling_params,\n",
    "                return_logprob=True,\n",
    "                top_logprobs_num=n_logprobs,\n",
    "            )\n",
    "        else:\n",
    "            outputs = self.engine.generate(prompts, sampling_params)\n",
    "        \n",
    "        results = []\n",
    "        for output in outputs:\n",
    "            logprobs = None\n",
    "            if return_logprobs and \"meta_info\" in output:\n",
    "                # SGLang returns: (logprob, token_id, text)\n",
    "                logprobs = [lp[0] for lp in output[\"meta_info\"][\"output_token_logprobs\"]]\n",
    "            \n",
    "            results.append(GenerationResult(\n",
    "                text=output[\"text\"],\n",
    "                token_ids=output.get(\"output_ids\", []),\n",
    "                logprobs=logprobs,\n",
    "                finish_reason=output.get(\"meta_info\", {}).get(\"finish_reason\", {}).get(\"type\", \"length\")\n",
    "            ))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def shutdown(self):\n",
    "        self.engine.shutdown()\n",
    "        print(\"✓ SGLang backend shutdown\")\n",
    "\n",
    "\n",
    "print(\"✓ Inference backends defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8699fc5d",
   "metadata": {},
   "source": [
    "# Dynamic Reference Model Manager\n",
    "Handle changing reference models mid-training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9a5cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReferenceModelManager:\n",
    "    \"\"\"\n",
    "    Manages reference model for KL divergence computation.\n",
    "    Supports dynamic snapshots mid-training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, device: str = \"cuda\"):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.snapshots: Dict[str, Dict] = {}  # snapshot_id -> state_dict\n",
    "        \n",
    "        # Load reference model (frozen)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Freeze all parameters\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        print(f\"✓ Reference model loaded: {model_name}\")\n",
    "    \n",
    "    def snapshot_current_training_model(self, training_model: nn.Module) -> str:\n",
    "        \"\"\"\n",
    "        Create snapshot of current training model weights.\n",
    "        Useful for dynamic reference model updates.\n",
    "        \"\"\"\n",
    "        snapshot_id = f\"snapshot_{int(time.time() * 1000)}\"\n",
    "        # Store only trainable parameters (e.g., LoRA weights)\n",
    "        self.snapshots[snapshot_id] = {\n",
    "            k: v.cpu().clone() for k, v in training_model.state_dict().items()\n",
    "        }\n",
    "        print(f\"✓ Created snapshot: {snapshot_id}\")\n",
    "        return snapshot_id\n",
    "    \n",
    "    def load_snapshot(self, snapshot_id: str):\n",
    "        \"\"\"Load a previously saved snapshot into reference model\"\"\"\n",
    "        if snapshot_id not in self.snapshots:\n",
    "            raise ValueError(f\"Snapshot {snapshot_id} not found\")\n",
    "        \n",
    "        self.model.load_state_dict(self.snapshots[snapshot_id], strict=False)\n",
    "        print(f\"✓ Loaded snapshot: {snapshot_id}\")\n",
    "    \n",
    "    def compute_log_probs(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        responses: List[str],\n",
    "        snapshot_id: Optional[str] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute log probabilities from reference model.\n",
    "        Optionally uses a specific snapshot.\n",
    "        \"\"\"\n",
    "        # Load snapshot if specified\n",
    "        if snapshot_id and snapshot_id in self.snapshots:\n",
    "            original_state = {k: v.clone() for k, v in self.model.state_dict().items()}\n",
    "            self.model.load_state_dict(self.snapshots[snapshot_id], strict=False)\n",
    "        \n",
    "        # Compute log probs\n",
    "        full_texts = [p + r for p, r in zip(prompts, responses)]\n",
    "        prompt_lens = [len(self.tokenizer.encode(p)) for p in prompts]\n",
    "        \n",
    "        full_encodings = self.tokenizer(full_texts, padding=True, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.device) for k, v in full_encodings.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        log_probs_list = []\n",
    "        for i in range(len(prompts)):\n",
    "            start_idx = prompt_lens[i] - 1\n",
    "            response_logits = logits[i, start_idx:-1]\n",
    "            response_ids = full_encodings[\"input_ids\"][i, prompt_lens[i]:]\n",
    "            mask = full_encodings[\"attention_mask\"][i, prompt_lens[i]:]\n",
    "            \n",
    "            log_probs = F.log_softmax(response_logits, dim=-1)\n",
    "            token_log_probs = log_probs.gather(1, response_ids.unsqueeze(1).to(self.device)).squeeze()\n",
    "            masked_log_probs = token_log_probs * mask.to(self.device).float()\n",
    "            log_probs_list.append(masked_log_probs.sum())\n",
    "        \n",
    "        # Restore original state if snapshot was used\n",
    "        if snapshot_id and snapshot_id in self.snapshots:\n",
    "            self.model.load_state_dict(original_state, strict=False)\n",
    "        \n",
    "        return torch.stack(log_probs_list)\n",
    "    \n",
    "    def compute_kl_divergence(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        responses: List[str],\n",
    "        train_model: nn.Module,\n",
    "        train_tokenizer,\n",
    "        snapshot_id: Optional[str] = None,\n",
    "        per_token: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute KL divergence: KL(π_train || π_ref)\n",
    "        \n",
    "        Args:\n",
    "            per_token: If True, return per-token KL. Otherwise, sum over sequence.\n",
    "        \"\"\"\n",
    "        # Get reference log probs\n",
    "        ref_log_probs = self.compute_log_probs(prompts, responses, snapshot_id)\n",
    "        \n",
    "        # Get training model log probs (with gradients if needed)\n",
    "        full_texts = [p + r for p, r in zip(prompts, responses)]\n",
    "        prompt_lens = [len(train_tokenizer.encode(p)) for p in prompts]\n",
    "        \n",
    "        full_encodings = train_tokenizer(full_texts, padding=True, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.device) for k, v in full_encodings.items()}\n",
    "        \n",
    "        train_outputs = train_model(**inputs)\n",
    "        train_logits = train_outputs.logits\n",
    "        \n",
    "        train_log_probs_list = []\n",
    "        for i in range(len(prompts)):\n",
    "            start_idx = prompt_lens[i] - 1\n",
    "            response_logits = train_logits[i, start_idx:-1]\n",
    "            response_ids = full_encodings[\"input_ids\"][i, prompt_lens[i]:]\n",
    "            mask = full_encodings[\"attention_mask\"][i, prompt_lens[i]:]\n",
    "            \n",
    "            log_probs = F.log_softmax(response_logits, dim=-1)\n",
    "            token_log_probs = log_probs.gather(1, response_ids.unsqueeze(1).to(self.device)).squeeze()\n",
    "            masked_log_probs = token_log_probs * mask.to(self.device).float()\n",
    "            train_log_probs_list.append(masked_log_probs.sum())\n",
    "        \n",
    "        train_log_probs = torch.stack(train_log_probs_list)\n",
    "        \n",
    "        # KL divergence: π_train * (log π_train - log π_ref)\n",
    "        # Approximation: (π_train / π_ref).log() ≈ train_log_probs - ref_log_probs\n",
    "        kl_div = train_log_probs - ref_log_probs\n",
    "        \n",
    "        return kl_div if per_token else kl_div.mean()\n",
    "    \n",
    "    def clear_snapshots(self):\n",
    "        \"\"\"Free memory by clearing old snapshots\"\"\"\n",
    "        self.snapshots.clear()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"✓ Cleared all snapshots\")\n",
    "    \n",
    "    def shutdown(self):\n",
    "        del self.model\n",
    "        self.snapshots.clear()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"✓ Reference model manager shutdown\")\n",
    "\n",
    "print(\"✓ Reference model manager defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff036fb5",
   "metadata": {},
   "source": [
    "# Multi-Objective Reward System with Pareto Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b7f6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParetoRewardTracker:\n",
    "    \"\"\"\n",
    "    Track multi-objective rewards and maintain Pareto frontier.\n",
    "    Plug in your own reward functions!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, objective_names: List[str]):\n",
    "        self.objectives = objective_names\n",
    "        self.num_objectives = len(objective_names)\n",
    "        self.pareto_frontier: List[np.ndarray] = []\n",
    "        self.all_rewards: List[np.ndarray] = []\n",
    "        \n",
    "        # YOU OVERRIDE THESE - map objective name to function\n",
    "        self.reward_functions: Dict[str, Callable[[RolloutState], float]] = {}\n",
    "    \n",
    "    def register_reward_function(self, name: str, fn: Callable[[RolloutState], float]):\n",
    "        \"\"\"Register a reward function for an objective\"\"\"\n",
    "        if name not in self.objectives:\n",
    "            raise ValueError(f\"Objective {name} not in {self.objectives}\")\n",
    "        self.reward_functions[name] = fn\n",
    "    \n",
    "    def compute_rewards(self, state: RolloutState) -> np.ndarray:\n",
    "        \"\"\"Compute reward vector for a completed rollout\"\"\"\n",
    "        rewards = np.zeros(self.num_objectives)\n",
    "        \n",
    "        for i, obj_name in enumerate(self.objectives):\n",
    "            if obj_name in self.reward_functions:\n",
    "                rewards[i] = self.reward_functions[obj_name](state)\n",
    "            else:\n",
    "                # Default: use cumulative reward\n",
    "                rewards[i] = state.cumulative_reward[i] if i < len(state.cumulative_reward) else 0.0\n",
    "        \n",
    "        return rewards\n",
    "    \n",
    "    def update_pareto_frontier(self, reward_vector: np.ndarray):\n",
    "        \"\"\"Update Pareto frontier with new reward vector\"\"\"\n",
    "        self.all_rewards.append(reward_vector)\n",
    "        \n",
    "        # Check if new point is dominated\n",
    "        is_dominated = False\n",
    "        for frontier_point in self.pareto_frontier:\n",
    "            if self._dominates(frontier_point, reward_vector):\n",
    "                is_dominated = True\n",
    "                break\n",
    "        \n",
    "        if not is_dominated:\n",
    "            # Remove points dominated by new vector\n",
    "            self.pareto_frontier = [\n",
    "                p for p in self.pareto_frontier\n",
    "                if not self._dominates(reward_vector, p)\n",
    "            ]\n",
    "            self.pareto_frontier.append(reward_vector)\n",
    "    \n",
    "    def _dominates(self, a: np.ndarray, b: np.ndarray) -> bool:\n",
    "        \"\"\"Check if a dominates b (a is better in all objectives)\"\"\"\n",
    "        return np.all(a >= b) and np.any(a > b)\n",
    "    \n",
    "    def is_on_pareto_frontier(self, reward_vector: np.ndarray) -> bool:\n",
    "        \"\"\"Check if point is on Pareto frontier\"\"\"\n",
    "        for p in self.pareto_frontier:\n",
    "            if np.allclose(p, reward_vector):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def get_pareto_weights(self, strategy: str = \"random\") -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get weight vector for scalarizing multi-objective rewards.\n",
    "        Used in training to sample different trade-offs.\n",
    "        \"\"\"\n",
    "        if strategy == \"random\":\n",
    "            # Random scalarization (linear scalarization)\n",
    "            return np.random.dirichlet(np.ones(self.num_objectives))\n",
    "        elif strategy == \"uniform\":\n",
    "            return np.ones(self.num_objectives) / self.num_objectives\n",
    "        elif strategy == \"maximize_first\":\n",
    "            weights = np.zeros(self.num_objectives)\n",
    "            weights[0] = 1.0\n",
    "            return weights\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "    \n",
    "    def scalarize_reward(self, reward_vector: np.ndarray, weights: Optional[np.ndarray] = None) -> float:\n",
    "        \"\"\"Convert multi-objective reward to scalar\"\"\"\n",
    "        if weights is None:\n",
    "            weights = np.ones(self.num_objectives) / self.num_objectives\n",
    "        return float(np.dot(weights, reward_vector))\n",
    "    \n",
    "    def get_frontier_statistics(self) -> Dict:\n",
    "        \"\"\"Get statistics about Pareto frontier\"\"\"\n",
    "        if not self.pareto_frontier:\n",
    "            return {\"size\": 0}\n",
    "        \n",
    "        frontier_array = np.array(self.pareto_frontier)\n",
    "        return {\n",
    "            \"size\": len(self.pareto_frontier),\n",
    "            \"mean\": frontier_array.mean(axis=0).tolist(),\n",
    "            \"std\": frontier_array.std(axis=0).tolist(),\n",
    "            \"min\": frontier_array.min(axis=0).tolist(),\n",
    "            \"max\": frontier_array.max(axis=0).tolist(),\n",
    "        }\n",
    "\n",
    "print(\"✓ ParetoRewardTracker defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784ed069",
   "metadata": {},
   "source": [
    "# SVRL Environment Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f8c87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVRLEnvironment:\n",
    "    \"\"\"\n",
    "    Self-Verifying RL Environment.\n",
    "    Model can query environment for information at a cost.\n",
    "    You implement the verification logic!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        cost_function: Optional[Callable[[str, RolloutState], float]] = None,\n",
    "        verification_handler: Optional[Callable[[str], Any]] = None,\n",
    "    ):\n",
    "        # YOU OVERRIDE THESE\n",
    "        self.cost_fn = cost_function or self._default_cost\n",
    "        self.verification_handler = verification_handler or self._default_verification\n",
    "        \n",
    "        # Tracking\n",
    "        self.total_queries = 0\n",
    "        self.total_cost_spent = 0.0\n",
    "        self.query_history: List[Dict] = []\n",
    "    \n",
    "    def _default_cost(self, query: str, state: RolloutState) -> float:\n",
    "        \"\"\"Default cost: based on query length\"\"\"\n",
    "        return len(query.split()) * 0.1\n",
    "    \n",
    "    def _default_verification(self, query: str) -> str:\n",
    "        \"\"\"Default verification: placeholder\"\"\"\n",
    "        return f\"[Verification result for: {query}]\"\n",
    "    \n",
    "    def query_environment(\n",
    "        self,\n",
    "        query: str,\n",
    "        state: RolloutState,\n",
    "    ) -> Tuple[Optional[Any], float, bool]:\n",
    "        \"\"\"\n",
    "        Model queries environment for verification.\n",
    "        \n",
    "        Returns:\n",
    "            result: Verification result (None if over budget)\n",
    "            cost: Cost of this query\n",
    "            success: Whether query was successful\n",
    "        \"\"\"\n",
    "        cost = self.cost_fn(query, state)\n",
    "        \n",
    "        if state.can_verify(cost):\n",
    "            state.verification_budget_spent += cost\n",
    "            result = self.verification_handler(query)\n",
    "            \n",
    "            # Track query\n",
    "            self.total_queries += 1\n",
    "            self.total_cost_spent += cost\n",
    "            self.query_history.append({\n",
    "                \"rollout_id\": state.rollout_id,\n",
    "                \"step\": state.step_count,\n",
    "                \"query\": query,\n",
    "                \"cost\": cost,\n",
    "                \"budget_remaining\": state.max_verification_budget - state.verification_budget_spent,\n",
    "            })\n",
    "            \n",
    "            return result, cost, True\n",
    "        else:\n",
    "            return None, cost, False\n",
    "    \n",
    "    def parse_verification_request(self, model_output: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Parse model output to extract verification request.\n",
    "        You can customize this format!\n",
    "        \n",
    "        Default format: VERIFY: <query>\n",
    "        \"\"\"\n",
    "        if \"VERIFY:\" in model_output:\n",
    "            parts = model_output.split(\"VERIFY:\", 1)\n",
    "            if len(parts) > 1:\n",
    "                query = parts[1].strip()\n",
    "                # Extract until newline or end\n",
    "                query = query.split(\"\\n\")[0].strip()\n",
    "                return query\n",
    "        return None\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"Get environment statistics\"\"\"\n",
    "        return {\n",
    "            \"total_queries\": self.total_queries,\n",
    "            \"total_cost\": self.total_cost_spent,\n",
    "            \"avg_cost_per_query\": self.total_cost_spent / max(1, self.total_queries),\n",
    "            \"recent_queries\": self.query_history[-10:],\n",
    "        }\n",
    "    \n",
    "    def reset_tracking(self):\n",
    "        \"\"\"Reset tracking for new experiment\"\"\"\n",
    "        self.total_queries = 0\n",
    "        self.total_cost_spent = 0.0\n",
    "        self.query_history.clear()\n",
    "\n",
    "print(\"✓ SVRLEnvironment defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7813c3",
   "metadata": {},
   "source": [
    "# Efficient Trajectory buffer lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a0b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryBuffer:\n",
    "    \"\"\"\n",
    "    Buffer for completed trajectories with various sampling strategies.\n",
    "    Supports priority sampling for RL training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        max_buffer_size: int = 10000,\n",
    "        num_objectives: int = 1,\n",
    "    ):\n",
    "        self.max_size = max_buffer_size\n",
    "        self.num_objectives = num_objectives\n",
    "        self.trajectories: deque = deque(maxlen=max_buffer_size)\n",
    "        self.step_statistics: Dict[int, List[np.ndarray]] = defaultdict(list)\n",
    "        \n",
    "        # Optional: Pareto tracker for priority sampling\n",
    "        self.pareto_tracker: Optional[ParetoRewardTracker] = None\n",
    "    \n",
    "    def add_trajectory(self, state: RolloutState):\n",
    "        \"\"\"Add completed trajectory to buffer\"\"\"\n",
    "        self.trajectories.append(state)\n",
    "        \n",
    "        # Track statistics by trajectory length\n",
    "        self.step_statistics[state.step_count].append(state.cumulative_reward)\n",
    "        \n",
    "        # Update Pareto frontier if tracker available\n",
    "        if self.pareto_tracker:\n",
    "            self.pareto_tracker.update_pareto_frontier(state.cumulative_reward)\n",
    "    \n",
    "    def add_batch(self, states: List[RolloutState]):\n",
    "        \"\"\"Add batch of completed trajectories\"\"\"\n",
    "        for state in states:\n",
    "            self.add_trajectory(state)\n",
    "    \n",
    "    def sample_batch(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        strategy: str = \"uniform\",\n",
    "    ) -> List[RolloutState]:\n",
    "        \"\"\"\n",
    "        Sample batch of trajectories for training.\n",
    "        \n",
    "        Strategies:\n",
    "        - uniform: Random sampling\n",
    "        - completion_weighted: Prefer longer trajectories\n",
    "        - pareto_weighted: Prefer Pareto-optimal trajectories\n",
    "        - reward_weighted: Prefer high-reward trajectories\n",
    "        \"\"\"\n",
    "        if len(self.trajectories) == 0:\n",
    "            return []\n",
    "        \n",
    "        batch_size = min(batch_size, len(self.trajectories))\n",
    "        \n",
    "        if strategy == \"uniform\":\n",
    "            return random.sample(list(self.trajectories), batch_size)\n",
    "        \n",
    "        elif strategy == \"completion_weighted\":\n",
    "            # Prefer longer trajectories\n",
    "            weights = np.array([len(t.trajectory) for t in self.trajectories], dtype=float)\n",
    "            weights = weights / weights.sum()\n",
    "            indices = np.random.choice(len(self.trajectories), size=batch_size, p=weights, replace=False)\n",
    "            return [list(self.trajectories)[i] for i in indices]\n",
    "        \n",
    "        elif strategy == \"pareto_weighted\":\n",
    "            # Prefer Pareto-optimal trajectories\n",
    "            if not self.pareto_tracker:\n",
    "                return self.sample_batch(batch_size, \"uniform\")\n",
    "            \n",
    "            weights = np.array([\n",
    "                2.0 if self.pareto_tracker.is_on_pareto_frontier(t.cumulative_reward) else 1.0\n",
    "                for t in self.trajectories\n",
    "            ])\n",
    "            weights = weights / weights.sum()\n",
    "            indices = np.random.choice(len(self.trajectories), size=batch_size, p=weights, replace=False)\n",
    "            return [list(self.trajectories)[i] for i in indices]\n",
    "        \n",
    "        elif strategy == \"reward_weighted\":\n",
    "            # Prefer high-reward trajectories (sum across objectives)\n",
    "            weights = np.array([t.cumulative_reward.sum() for t in self.trajectories], dtype=float)\n",
    "            weights = weights - weights.min() + 1e-6  # Shift to positive\n",
    "            weights = weights / weights.sum()\n",
    "            indices = np.random.choice(len(self.trajectories), size=batch_size, p=weights, replace=False)\n",
    "            return [list(self.trajectories)[i] for i in indices]\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown sampling strategy: {strategy}\")\n",
    "    \n",
    "    def prepare_training_batch(\n",
    "        self,\n",
    "        trajectories: List[RolloutState],\n",
    "        weights: Optional[np.ndarray] = None,\n",
    "    ) -> TrainingBatch:\n",
    "        \"\"\"\n",
    "        Convert trajectories to training batch format.\n",
    "        Flattens all steps for batch processing.\n",
    "        \"\"\"\n",
    "        prompts = []\n",
    "        responses = []\n",
    "        token_ids = []\n",
    "        rewards = []\n",
    "        \n",
    "        for traj in trajectories:\n",
    "            for step in traj.trajectory:\n",
    "                prompts.append(step.prompt)\n",
    "                responses.append(step.response.text)\n",
    "                token_ids.append(step.response.token_ids)\n",
    "                \n",
    "                # Scalarize reward if multi-objective\n",
    "                if weights is not None and step.reward is not None:\n",
    "                    scalar_reward = np.dot(weights, step.reward)\n",
    "                else:\n",
    "                    scalar_reward = step.reward.sum() if step.reward is not None else 0.0\n",
    "                rewards.append(scalar_reward)\n",
    "        \n",
    "        return TrainingBatch(\n",
    "            prompts=prompts,\n",
    "            responses=responses,\n",
    "            token_ids=token_ids,\n",
    "            rewards=np.array(rewards),\n",
    "        )\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"Get buffer statistics\"\"\"\n",
    "        if not self.trajectories:\n",
    "            return {\"size\": 0}\n",
    "        \n",
    "        all_lengths = [len(t.trajectory) for t in self.trajectories]\n",
    "        all_rewards = [t.cumulative_reward for t in self.trajectories]\n",
    "        \n",
    "        return {\n",
    "            \"size\": len(self.trajectories),\n",
    "            \"avg_length\": np.mean(all_lengths),\n",
    "            \"std_length\": np.std(all_lengths),\n",
    "            \"min_length\": min(all_lengths),\n",
    "            \"max_length\": max(all_lengths),\n",
    "            \"avg_reward\": np.mean(all_rewards, axis=0).tolist() if all_rewards else [],\n",
    "            \"step_distribution\": {k: len(v) for k, v in self.step_statistics.items()},\n",
    "        }\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear buffer\"\"\"\n",
    "        self.trajectories.clear()\n",
    "        self.step_statistics.clear()\n",
    "\n",
    "print(\"✓ TrajectoryBuffer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571f83a2",
   "metadata": {},
   "source": [
    "# training loop with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92de6ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PURE FUNCTIONS: Core RL Primitives (Stateless, Composable)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_sequence_log_probs(\n",
    "    model: nn.Module,\n",
    "    tokenizer,\n",
    "    prompts: List[str],\n",
    "    responses: List[str],\n",
    "    device: str = \"cuda\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute log probabilities of response tokens given prompts.\n",
    "    Pure function - no side effects.\n",
    "    \n",
    "    Returns: Tensor of shape (batch_size,) with total log prob per sequence\n",
    "    \"\"\"\n",
    "    full_texts = [p + r for p, r in zip(prompts, responses)]\n",
    "    prompt_lens = [len(tokenizer.encode(p)) for p in prompts]\n",
    "    \n",
    "    full_encodings = tokenizer(full_texts, padding=True, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in full_encodings.items()}\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits  # (batch, seq_len, vocab)\n",
    "    \n",
    "    log_probs_list = []\n",
    "    for i in range(len(prompts)):\n",
    "        start_idx = prompt_lens[i] - 1\n",
    "        response_logits = logits[i, start_idx:-1]\n",
    "        response_ids = full_encodings[\"input_ids\"][i, prompt_lens[i]:]\n",
    "        mask = full_encodings[\"attention_mask\"][i, prompt_lens[i]:]\n",
    "        \n",
    "        log_probs = F.log_softmax(response_logits, dim=-1)\n",
    "        token_log_probs = log_probs.gather(1, response_ids.unsqueeze(1).to(device)).squeeze(-1)\n",
    "        masked_log_probs = token_log_probs * mask.to(device).float()\n",
    "        log_probs_list.append(masked_log_probs.sum())\n",
    "    \n",
    "    return torch.stack(log_probs_list)\n",
    "\n",
    "\n",
    "def compute_kl_divergence(\n",
    "    train_log_probs: torch.Tensor,\n",
    "    ref_log_probs: torch.Tensor,\n",
    "    reduction: str = \"mean\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute KL divergence from log probabilities.\n",
    "    Pure function.\n",
    "    \n",
    "    KL(π_train || π_ref) ≈ train_log_probs - ref_log_probs\n",
    "    \"\"\"\n",
    "    kl_div = train_log_probs - ref_log_probs\n",
    "    \n",
    "    if reduction == \"mean\":\n",
    "        return kl_div.mean()\n",
    "    elif reduction == \"sum\":\n",
    "        return kl_div.sum()\n",
    "    elif reduction == \"none\":\n",
    "        return kl_div\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown reduction: {reduction}\")\n",
    "\n",
    "\n",
    "def reinforce_loss(\n",
    "    log_probs: torch.Tensor,\n",
    "    rewards: torch.Tensor,\n",
    "    baseline: Optional[torch.Tensor] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    REINFORCE policy gradient loss.\n",
    "    Pure function.\n",
    "    \n",
    "    Loss = -E[reward * log_prob]\n",
    "    \"\"\"\n",
    "    if baseline is not None:\n",
    "        advantages = rewards - baseline\n",
    "    else:\n",
    "        advantages = rewards\n",
    "    \n",
    "    return -(advantages * log_probs).mean()\n",
    "\n",
    "\n",
    "def ppo_loss(\n",
    "    log_probs: torch.Tensor,\n",
    "    old_log_probs: torch.Tensor,\n",
    "    advantages: torch.Tensor,\n",
    "    clip_epsilon: float = 0.2,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    PPO clipped surrogate loss.\n",
    "    Pure function.\n",
    "    \"\"\"\n",
    "    ratio = torch.exp(log_probs - old_log_probs)\n",
    "    clipped_ratio = torch.clamp(ratio, 1.0 - clip_epsilon, 1.0 + clip_epsilon)\n",
    "    \n",
    "    loss = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def grpo_loss(\n",
    "    log_probs: torch.Tensor,\n",
    "    rewards: torch.Tensor,\n",
    "    group_size: int = 4,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Group Relative Policy Optimization loss.\n",
    "    Advantages computed relative to group.\n",
    "    Pure function.\n",
    "    \"\"\"\n",
    "    # Reshape into groups\n",
    "    batch_size = log_probs.shape[0]\n",
    "    num_groups = batch_size // group_size\n",
    "    \n",
    "    grouped_rewards = rewards[:num_groups * group_size].view(num_groups, group_size)\n",
    "    grouped_log_probs = log_probs[:num_groups * group_size].view(num_groups, group_size)\n",
    "    \n",
    "    # Compute advantages relative to group mean\n",
    "    group_means = grouped_rewards.mean(dim=1, keepdim=True)\n",
    "    group_stds = grouped_rewards.std(dim=1, keepdim=True) + 1e-8\n",
    "    advantages = (grouped_rewards - group_means) / group_stds\n",
    "    \n",
    "    loss = -(advantages * grouped_log_probs).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ABSTRACT BASE CLASSES: Extend These\n",
    "# =============================================================================\n",
    "\n",
    "class LossFunction(ABC):\n",
    "    \"\"\"\n",
    "    Abstract loss function. Subclass to implement your own.\n",
    "    \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __call__(\n",
    "        self,\n",
    "        batch: TrainingBatch,\n",
    "        model: nn.Module,\n",
    "        tokenizer,\n",
    "        device: str = \"cuda\",\n",
    "    ) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Compute loss and return metrics.\n",
    "        \n",
    "        Returns:\n",
    "            loss: Scalar tensor with gradient\n",
    "            metrics: Dict of metrics for logging\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class ReinforceLoss(LossFunction):\n",
    "    \"\"\"Simple REINFORCE loss\"\"\"\n",
    "    \n",
    "    def __init__(self, normalize_rewards: bool = True):\n",
    "        self.normalize_rewards = normalize_rewards\n",
    "    \n",
    "    def __call__(self, batch, model, tokenizer, device=\"cuda\"):\n",
    "        model.train()\n",
    "        \n",
    "        log_probs = compute_sequence_log_probs(\n",
    "            model, tokenizer, batch.prompts, batch.responses, device\n",
    "        )\n",
    "        \n",
    "        rewards = torch.tensor(batch.rewards, dtype=torch.float32, device=device)\n",
    "        \n",
    "        if self.normalize_rewards:\n",
    "            rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n",
    "        \n",
    "        loss = reinforce_loss(log_probs, rewards)\n",
    "        \n",
    "        return loss, {\n",
    "            \"pg_loss\": loss.item(),\n",
    "            \"avg_reward\": float(batch.rewards.mean()),\n",
    "            \"reward_std\": float(batch.rewards.std()),\n",
    "        }\n",
    "\n",
    "\n",
    "class PPOLoss(LossFunction):\n",
    "    \"\"\"PPO with clipping\"\"\"\n",
    "    \n",
    "    def __init__(self, clip_epsilon: float = 0.2):\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "    \n",
    "    def __call__(self, batch, model, tokenizer, device=\"cuda\"):\n",
    "        model.train()\n",
    "        \n",
    "        log_probs = compute_sequence_log_probs(\n",
    "            model, tokenizer, batch.prompts, batch.responses, device\n",
    "        )\n",
    "        \n",
    "        if batch.old_logprobs is None:\n",
    "            raise ValueError(\"PPO requires old_logprobs in batch\")\n",
    "        if batch.advantages is None:\n",
    "            raise ValueError(\"PPO requires advantages in batch\")\n",
    "        \n",
    "        loss = ppo_loss(log_probs, batch.old_logprobs, batch.advantages, self.clip_epsilon)\n",
    "        \n",
    "        return loss, {\n",
    "            \"ppo_loss\": loss.item(),\n",
    "            \"avg_advantage\": float(batch.advantages.mean()),\n",
    "        }\n",
    "\n",
    "\n",
    "class GRPOLoss(LossFunction):\n",
    "    \"\"\"Group Relative Policy Optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, group_size: int = 4):\n",
    "        self.group_size = group_size\n",
    "    \n",
    "    def __call__(self, batch, model, tokenizer, device=\"cuda\"):\n",
    "        model.train()\n",
    "        \n",
    "        log_probs = compute_sequence_log_probs(\n",
    "            model, tokenizer, batch.prompts, batch.responses, device\n",
    "        )\n",
    "        \n",
    "        rewards = torch.tensor(batch.rewards, dtype=torch.float32, device=device)\n",
    "        loss = grpo_loss(log_probs, rewards, self.group_size)\n",
    "        \n",
    "        return loss, {\n",
    "            \"grpo_loss\": loss.item(),\n",
    "            \"avg_reward\": float(batch.rewards.mean()),\n",
    "        }\n",
    "\n",
    "\n",
    "class TerminationCondition(ABC):\n",
    "    \"\"\"\n",
    "    Abstract termination condition. Subclass to implement your own.\n",
    "    \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def should_terminate(self, state: RolloutState) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Check if rollout should terminate.\n",
    "        \n",
    "        Returns:\n",
    "            should_stop: bool\n",
    "            reason: Optional reason string\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class MaxStepsTermination(TerminationCondition):\n",
    "    \"\"\"Terminate after max steps\"\"\"\n",
    "    \n",
    "    def __init__(self, max_steps: int):\n",
    "        self.max_steps = max_steps\n",
    "    \n",
    "    def should_terminate(self, state):\n",
    "        if state.step_count >= self.max_steps:\n",
    "            return True, \"max_steps\"\n",
    "        return False, None\n",
    "\n",
    "\n",
    "class KeywordTermination(TerminationCondition):\n",
    "    \"\"\"Terminate on specific keywords in response\"\"\"\n",
    "    \n",
    "    def __init__(self, keywords: List[str]):\n",
    "        self.keywords = keywords\n",
    "    \n",
    "    def should_terminate(self, state):\n",
    "        if not state.trajectory:\n",
    "            return False, None\n",
    "        \n",
    "        last_text = state.trajectory[-1].response.text\n",
    "        for kw in self.keywords:\n",
    "            if kw in last_text:\n",
    "                return True, f\"keyword:{kw}\"\n",
    "        return False, None\n",
    "\n",
    "\n",
    "class CompositeTermination(TerminationCondition):\n",
    "    \"\"\"Combine multiple termination conditions (OR logic)\"\"\"\n",
    "    \n",
    "    def __init__(self, conditions: List[TerminationCondition]):\n",
    "        self.conditions = conditions\n",
    "    \n",
    "    def should_terminate(self, state):\n",
    "        for cond in self.conditions:\n",
    "            should_stop, reason = cond.should_terminate(state)\n",
    "            if should_stop:\n",
    "                return True, reason\n",
    "        return False, None\n",
    "\n",
    "\n",
    "class RewardFunction(ABC):\n",
    "    \"\"\"\n",
    "    Abstract reward function. Subclass to implement your own.\n",
    "    \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def compute_reward(self, state: RolloutState, step: StepData) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute reward vector for a step.\n",
    "        \n",
    "        Returns:\n",
    "            reward: np.ndarray of shape (num_objectives,)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class SparseReward(RewardFunction):\n",
    "    \"\"\"Only give reward at end of trajectory\"\"\"\n",
    "    \n",
    "    def __init__(self, final_reward_fn: Callable[[RolloutState], np.ndarray]):\n",
    "        self.final_reward_fn = final_reward_fn\n",
    "    \n",
    "    def compute_reward(self, state, step):\n",
    "        if state.terminated:\n",
    "            return self.final_reward_fn(state)\n",
    "        return np.zeros_like(state.cumulative_reward)\n",
    "\n",
    "\n",
    "class StepPenaltyReward(RewardFunction):\n",
    "    \"\"\"Small penalty for each step (encourages efficiency)\"\"\"\n",
    "    \n",
    "    def __init__(self, penalty: float = -0.01, num_objectives: int = 1):\n",
    "        self.penalty = penalty\n",
    "        self.num_objectives = num_objectives\n",
    "    \n",
    "    def compute_reward(self, state, step):\n",
    "        reward = np.zeros(self.num_objectives)\n",
    "        reward[0] = self.penalty  # Penalty in first objective\n",
    "        return reward\n",
    "\n",
    "\n",
    "class CompositeReward(RewardFunction):\n",
    "    \"\"\"Combine multiple reward functions\"\"\"\n",
    "    \n",
    "    def __init__(self, reward_fns: List[RewardFunction]):\n",
    "        self.reward_fns = reward_fns\n",
    "    \n",
    "    def compute_reward(self, state, step):\n",
    "        rewards = [fn.compute_reward(state, step) for fn in self.reward_fns]\n",
    "        return sum(rewards)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# COMPOSABLE ROLLOUT COLLECTOR: Inject Your Dependencies\n",
    "# =============================================================================\n",
    "\n",
    "def collect_single_rollout(\n",
    "    backend: InferenceBackend,\n",
    "    initial_prompt: str,\n",
    "    termination_condition: TerminationCondition,\n",
    "    reward_function: RewardFunction,\n",
    "    max_steps: int = 50,\n",
    "    num_objectives: int = 1,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 1.0,\n",
    "    **gen_kwargs,\n",
    ") -> RolloutState:\n",
    "    \"\"\"\n",
    "    Collect a single complete rollout.\n",
    "    Pure function - no global state.\n",
    "    \n",
    "    You inject:\n",
    "    - backend: How to generate\n",
    "    - termination_condition: When to stop\n",
    "    - reward_function: How to compute rewards\n",
    "    \"\"\"\n",
    "    state = RolloutState(\n",
    "        rollout_id=str(uuid.uuid4()),\n",
    "        initial_prompt=initial_prompt,\n",
    "        cumulative_reward=np.zeros(num_objectives),\n",
    "    )\n",
    "    \n",
    "    for step_num in range(max_steps):\n",
    "        # Generate response\n",
    "        context = state.get_full_context()\n",
    "        results = backend.generate(\n",
    "            [context],\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            **gen_kwargs,\n",
    "        )\n",
    "        gen_result = results[0]\n",
    "        \n",
    "        # Create step\n",
    "        step_data = StepData(\n",
    "            prompt=context,\n",
    "            response=gen_result,\n",
    "        )\n",
    "        \n",
    "        # Add step (before checking termination)\n",
    "        state.add_step(step_data)\n",
    "        \n",
    "        # Check termination\n",
    "        should_stop, reason = termination_condition.should_terminate(state)\n",
    "        if gen_result.finish_reason == \"stop\":\n",
    "            should_stop, reason = True, \"eos\"\n",
    "        \n",
    "        if should_stop:\n",
    "            state.terminated = True\n",
    "            state.termination_reason = reason\n",
    "            # Compute final reward\n",
    "            step_data.reward = reward_function.compute_reward(state, step_data)\n",
    "            state.cumulative_reward = state.cumulative_reward + step_data.reward\n",
    "            break\n",
    "        else:\n",
    "            # Compute step reward\n",
    "            step_data.reward = reward_function.compute_reward(state, step_data)\n",
    "            state.cumulative_reward = state.cumulative_reward + step_data.reward\n",
    "    \n",
    "    # If we hit max steps without termination\n",
    "    if not state.terminated:\n",
    "        state.terminated = True\n",
    "        state.termination_reason = \"max_steps\"\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def collect_rollout_batch(\n",
    "    backend: InferenceBackend,\n",
    "    prompts: List[str],\n",
    "    termination_condition: TerminationCondition,\n",
    "    reward_function: RewardFunction,\n",
    "    **kwargs,\n",
    ") -> List[RolloutState]:\n",
    "    \"\"\"\n",
    "    Collect multiple rollouts (sequential, for simplicity).\n",
    "    Use AsyncRolloutManager for parallel collection.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        collect_single_rollout(\n",
    "            backend, prompt, termination_condition, reward_function, **kwargs\n",
    "        )\n",
    "        for prompt in tqdm(prompts, desc=\"Collecting rollouts\")\n",
    "    ]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING STEP: Compose Your Own Pipeline\n",
    "# =============================================================================\n",
    "\n",
    "def training_step(\n",
    "    batch: TrainingBatch,\n",
    "    model: nn.Module,\n",
    "    tokenizer,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: LossFunction,\n",
    "    ref_model: Optional[nn.Module] = None,\n",
    "    ref_tokenizer = None,\n",
    "    kl_coef: float = 0.0,\n",
    "    max_grad_norm: float = 1.0,\n",
    "    device: str = \"cuda\",\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Single training step. Pure function.\n",
    "    \n",
    "    You inject:\n",
    "    - loss_fn: How to compute loss (REINFORCE, PPO, GRPO, custom)\n",
    "    - ref_model: Optional reference for KL penalty\n",
    "    - kl_coef: KL penalty coefficient (0 = no penalty)\n",
    "    \"\"\"\n",
    "    # Compute main loss\n",
    "    loss, metrics = loss_fn(batch, model, tokenizer, device)\n",
    "    \n",
    "    # Add KL penalty if requested\n",
    "    if kl_coef > 0 and ref_model is not None:\n",
    "        with torch.no_grad():\n",
    "            ref_log_probs = compute_sequence_log_probs(\n",
    "                ref_model, ref_tokenizer or tokenizer,\n",
    "                batch.prompts, batch.responses, device\n",
    "            )\n",
    "        \n",
    "        train_log_probs = compute_sequence_log_probs(\n",
    "            model, tokenizer, batch.prompts, batch.responses, device\n",
    "        )\n",
    "        \n",
    "        kl_div = compute_kl_divergence(train_log_probs, ref_log_probs)\n",
    "        loss = loss + kl_coef * kl_div\n",
    "        metrics[\"kl_div\"] = kl_div.item()\n",
    "    else:\n",
    "        metrics[\"kl_div\"] = 0.0\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Gradient clipping\n",
    "    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "    metrics[\"grad_norm\"] = grad_norm.item()\n",
    "    \n",
    "    # Step\n",
    "    optimizer.step()\n",
    "    \n",
    "    metrics[\"total_loss\"] = loss.item()\n",
    "    return metrics\n",
    "\n",
    "\n",
    "print(\"✓ Core primitives defined\")\n",
    "print(\"\\\\nAvailable:\")\n",
    "print(\"  Pure functions: compute_sequence_log_probs, compute_kl_divergence, reinforce_loss, ppo_loss, grpo_loss\")\n",
    "print(\"  Loss classes: ReinforceLoss, PPOLoss, GRPOLoss (or extend LossFunction)\")\n",
    "print(\"  Termination: MaxStepsTermination, KeywordTermination, CompositeTermination (or extend TerminationCondition)\")\n",
    "print(\"  Rewards: SparseReward, StepPenaltyReward, CompositeReward (or extend RewardFunction)\")\n",
    "print(\"  Rollout: collect_single_rollout, collect_rollout_batch\")\n",
    "print(\"  Training: training_step (compose your own pipeline)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaa45f6",
   "metadata": {},
   "source": [
    "# Modular Configuration System"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c51fd33d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION: Experiment Settings\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration for RL experiments\"\"\"\n",
    "    \n",
    "    # Model settings\n",
    "    model_name: str = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "    dtype: str = \"bfloat16\"\n",
    "    gradient_checkpointing: bool = True\n",
    "    \n",
    "    # Training settings\n",
    "    num_objectives: int = 2\n",
    "    max_steps: int = 50\n",
    "    batch_size: int = 4\n",
    "    learning_rate: float = 1e-6\n",
    "    kl_coef: float = 0.1\n",
    "    max_grad_norm: float = 1.0\n",
    "    \n",
    "    # Rollout settings\n",
    "    max_new_tokens: int = 256\n",
    "    temperature: float = 1.0\n",
    "    top_p: float = 0.95\n",
    "    \n",
    "    # SVRL settings (if using)\n",
    "    svrl_enabled: bool = False\n",
    "    verification_budget: float = 10.0\n",
    "    \n",
    "    # Buffer settings\n",
    "    buffer_size: int = 10000\n",
    "    sampling_strategy: str = \"completion_weighted\"  # uniform, pareto_weighted, reward_weighted\n",
    "    \n",
    "    # Dynamic reference model\n",
    "    update_ref_model: bool = False\n",
    "    ref_update_frequency: int = 100  # steps\n",
    "    \n",
    "    # Logging\n",
    "    use_wandb: bool = False\n",
    "    checkpoint_dir: str = \"./checkpoints\"\n",
    "    save_every: int = 50\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return asdict(self)\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(self.to_dict(), f, indent=2)\n",
    "        print(f\"✓ Config saved to {path}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> \"ExperimentConfig\":\n",
    "        with open(path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        return cls(**data)\n",
    "\n",
    "\n",
    "# Example configurations for different experiments\n",
    "def get_pareto_rl_config() -> ExperimentConfig:\n",
    "    \"\"\"Config for Pareto/Multi-objective RL experiments\"\"\"\n",
    "    return ExperimentConfig(\n",
    "        model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "        num_objectives=2,  # task_completion, efficiency\n",
    "        max_steps=50,\n",
    "        batch_size=4,\n",
    "        learning_rate=1e-6,\n",
    "        kl_coef=0.1,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_svrl_config() -> ExperimentConfig:\n",
    "    \"\"\"Config for Self-Verifying RL experiments\"\"\"\n",
    "    return ExperimentConfig(\n",
    "        model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "        num_objectives=3,  # correctness, efficiency, verification_cost\n",
    "        svrl_enabled=True,\n",
    "        verification_budget=10.0,\n",
    "        max_steps=30,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_no_kl_config() -> ExperimentConfig:\n",
    "    \"\"\"Config for experiments without KL penalty\"\"\"\n",
    "    return ExperimentConfig(\n",
    "        model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "        kl_coef=0.0,  # No KL penalty\n",
    "        max_steps=20,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_dynamic_ref_config() -> ExperimentConfig:\n",
    "    \"\"\"Config for dynamic reference model experiments\"\"\"\n",
    "    return ExperimentConfig(\n",
    "        model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "        update_ref_model=True,\n",
    "        ref_update_frequency=50,\n",
    "        kl_coef=0.05,  # Lower KL coef since ref model updates\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"✓ Configuration system defined\")\n",
    "print(\"\\\\nPreset configs available:\")\n",
    "print(\"  - get_pareto_rl_config()\")\n",
    "print(\"  - get_svrl_config()\")\n",
    "print(\"  - get_no_kl_config()\")\n",
    "print(\"  - get_dynamic_ref_config()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d17089",
   "metadata": {},
   "source": [
    "# Implementation Summary\n",
    "\n",
    "## What's Implemented:\n",
    "- **Dynamic batching**: Groups rollouts by step count for efficient inference\n",
    "- **Padding masks**: Handled automatically via HuggingFace tokenizers\n",
    "- **Priority sampling**: Multiple strategies (completion_weighted, pareto_weighted, reward_weighted)\n",
    "- **Variable-length rollouts**: AsyncRolloutManager handles different trajectory lengths\n",
    "- **Full parameter updates**: No LoRA - direct gradient computation on all weights\n",
    "\n",
    "## Key Features:\n",
    "- **Pareto frontier tracking**: Maintains non-dominated solutions for multi-objective optimization\n",
    "- **Dynamic reference model**: Snapshot and restore training model weights mid-training\n",
    "- **KL divergence control**: Optional penalty to keep policy close to reference\n",
    "- **SVRL environment**: Query environment for verification at a cost\n",
    "- **Checkpointing**: Save/load training state including Pareto frontier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fcb9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UTILITY: Visualization and Analysis\n",
    "# =============================================================================\n",
    "\n",
    "def plot_pareto_frontier(tracker: ParetoRewardTracker):\n",
    "    \"\"\"Plot 2D Pareto frontier\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    if tracker.num_objectives != 2:\n",
    "        print(\"Plotting only supports 2 objectives\")\n",
    "        return\n",
    "    \n",
    "    if not tracker.pareto_frontier:\n",
    "        print(\"No Pareto frontier points yet\")\n",
    "        return\n",
    "    \n",
    "    frontier = np.array(tracker.pareto_frontier)\n",
    "    all_points = np.array(tracker.all_rewards)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Plot all points\n",
    "    ax.scatter(all_points[:, 0], all_points[:, 1], alpha=0.3, label=\"All trajectories\")\n",
    "    \n",
    "    # Plot Pareto frontier\n",
    "    ax.scatter(frontier[:, 0], frontier[:, 1], color=\"red\", s=100, marker=\"*\", \n",
    "               label=\"Pareto frontier\", zorder=5)\n",
    "    \n",
    "    # Sort frontier for line plot\n",
    "    sorted_idx = np.argsort(frontier[:, 0])\n",
    "    ax.plot(frontier[sorted_idx, 0], frontier[sorted_idx, 1], \"r--\", alpha=0.5)\n",
    "    \n",
    "    ax.set_xlabel(tracker.objectives[0])\n",
    "    ax.set_ylabel(tracker.objectives[1])\n",
    "    ax.set_title(\"Pareto Frontier\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_training_metrics(trainer: RLTrainer):\n",
    "    \"\"\"Plot training metrics over time\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    if not trainer.train_metrics:\n",
    "        print(\"No training metrics yet\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(trainer.train_metrics)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(df[\"step\"], df[\"total_loss\"], label=\"Total Loss\")\n",
    "    axes[0, 0].plot(df[\"step\"], df[\"pg_loss\"], label=\"PG Loss\", alpha=0.7)\n",
    "    axes[0, 0].set_xlabel(\"Step\")\n",
    "    axes[0, 0].set_ylabel(\"Loss\")\n",
    "    axes[0, 0].set_title(\"Training Loss\")\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # KL Divergence\n",
    "    axes[0, 1].plot(df[\"step\"], df[\"kl_div\"])\n",
    "    axes[0, 1].set_xlabel(\"Step\")\n",
    "    axes[0, 1].set_ylabel(\"KL Divergence\")\n",
    "    axes[0, 1].set_title(\"KL Divergence from Reference\")\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Average Reward\n",
    "    axes[1, 0].plot(df[\"step\"], df[\"avg_reward\"])\n",
    "    axes[1, 0].set_xlabel(\"Step\")\n",
    "    axes[1, 0].set_ylabel(\"Average Reward\")\n",
    "    axes[1, 0].set_title(\"Average Reward per Step\")\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Buffer and Pareto size\n",
    "    axes[1, 1].plot(df[\"step\"], df[\"buffer_size\"], label=\"Buffer Size\")\n",
    "    axes[1, 1].plot(df[\"step\"], df[\"pareto_frontier_size\"], label=\"Pareto Frontier\")\n",
    "    axes[1, 1].set_xlabel(\"Step\")\n",
    "    axes[1, 1].set_ylabel(\"Count\")\n",
    "    axes[1, 1].set_title(\"Buffer & Pareto Statistics\")\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def analyze_trajectory_lengths(buffer: TrajectoryBuffer):\n",
    "    \"\"\"Analyze distribution of trajectory lengths\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    if not buffer.trajectories:\n",
    "        print(\"No trajectories in buffer\")\n",
    "        return\n",
    "    \n",
    "    lengths = [len(t.trajectory) for t in buffer.trajectories]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    axes[0].hist(lengths, bins=20, edgecolor=\"black\")\n",
    "    axes[0].axvline(np.mean(lengths), color=\"red\", linestyle=\"--\", label=f\"Mean: {np.mean(lengths):.1f}\")\n",
    "    axes[0].set_xlabel(\"Trajectory Length (steps)\")\n",
    "    axes[0].set_ylabel(\"Count\")\n",
    "    axes[0].set_title(\"Distribution of Trajectory Lengths\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Box plot\n",
    "    axes[1].boxplot(lengths)\n",
    "    axes[1].set_ylabel(\"Steps\")\n",
    "    axes[1].set_title(\"Trajectory Length Statistics\")\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Length statistics:\")\n",
    "    print(f\"  Mean: {np.mean(lengths):.2f}\")\n",
    "    print(f\"  Std: {np.std(lengths):.2f}\")\n",
    "    print(f\"  Min: {min(lengths)}\")\n",
    "    print(f\"  Max: {max(lengths)}\")\n",
    "    print(f\"  Median: {np.median(lengths):.1f}\")\n",
    "\n",
    "print(\"✓ Visualization utilities defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745523ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXAMPLE: Composing Your Own Training Pipeline\n",
    "# =============================================================================\n",
    "# These are PRIMITIVES - you compose them however you want\n",
    "\n",
    "# -------------------------\n",
    "# 1. CUSTOM REWARD FUNCTION (Extend RewardFunction)\n",
    "# -------------------------\n",
    "\n",
    "class MyTaskReward(RewardFunction):\n",
    "    \"\"\"Your custom reward logic\"\"\"\n",
    "    \n",
    "    def __init__(self, num_objectives: int = 2):\n",
    "        self.num_objectives = num_objectives\n",
    "    \n",
    "    def compute_reward(self, state: RolloutState, step: StepData) -> np.ndarray:\n",
    "        reward = np.zeros(self.num_objectives)\n",
    "        \n",
    "        # YOUR LOGIC HERE\n",
    "        response_text = step.response.text\n",
    "        \n",
    "        # Objective 0: Task completion\n",
    "        if \"ANSWER:\" in response_text:\n",
    "            reward[0] = 1.0\n",
    "        \n",
    "        # Objective 1: Efficiency (penalty per step)\n",
    "        reward[1] = -0.01\n",
    "        \n",
    "        return reward\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 2. CUSTOM TERMINATION (Extend TerminationCondition)\n",
    "# -------------------------\n",
    "\n",
    "class MyTermination(TerminationCondition):\n",
    "    \"\"\"Your custom termination logic\"\"\"\n",
    "    \n",
    "    def should_terminate(self, state: RolloutState) -> Tuple[bool, Optional[str]]:\n",
    "        if not state.trajectory:\n",
    "            return False, None\n",
    "        \n",
    "        last_text = state.trajectory[-1].response.text\n",
    "        \n",
    "        # YOUR LOGIC HERE\n",
    "        if \"ANSWER:\" in last_text:\n",
    "            return True, \"task_complete\"\n",
    "        if \"I give up\" in last_text:\n",
    "            return True, \"gave_up\"\n",
    "        if state.step_count >= 20:\n",
    "            return True, \"max_steps\"\n",
    "        \n",
    "        return False, None\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3. CUSTOM LOSS FUNCTION (Extend LossFunction)\n",
    "# -------------------------\n",
    "\n",
    "class MyCustomLoss(LossFunction):\n",
    "    \"\"\"Your custom loss computation\"\"\"\n",
    "    \n",
    "    def __init__(self, entropy_coef: float = 0.01):\n",
    "        self.entropy_coef = entropy_coef\n",
    "    \n",
    "    def __call__(self, batch, model, tokenizer, device=\"cuda\"):\n",
    "        model.train()\n",
    "        \n",
    "        # Compute log probs using the primitive\n",
    "        log_probs = compute_sequence_log_probs(\n",
    "            model, tokenizer, batch.prompts, batch.responses, device\n",
    "        )\n",
    "        \n",
    "        # YOUR LOSS LOGIC HERE\n",
    "        rewards = torch.tensor(batch.rewards, dtype=torch.float32, device=device)\n",
    "        \n",
    "        # Example: REINFORCE with entropy bonus\n",
    "        pg_loss = reinforce_loss(log_probs, rewards)\n",
    "        \n",
    "        # Add entropy term (optional)\n",
    "        # entropy = ... (compute if needed)\n",
    "        # loss = pg_loss - self.entropy_coef * entropy\n",
    "        \n",
    "        loss = pg_loss\n",
    "        \n",
    "        return loss, {\n",
    "            \"my_loss\": loss.item(),\n",
    "            \"avg_reward\": float(batch.rewards.mean()),\n",
    "        }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE: Simple Training Loop (Compose the Primitives)\n",
    "# =============================================================================\n",
    "\n",
    "def example_training_loop():\n",
    "    \"\"\"\n",
    "    Shows how to compose primitives into a training loop.\n",
    "    No god-class - you control everything.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Load model\n",
    "    MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "    \n",
    "    backend = HuggingFaceBackend(MODEL_NAME, dtype=\"bfloat16\")\n",
    "    model = backend.model\n",
    "    tokenizer = backend.tokenizer\n",
    "    \n",
    "    # 2. Create optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
    "    \n",
    "    # 3. Your injected dependencies\n",
    "    termination = CompositeTermination([\n",
    "        KeywordTermination([\"ANSWER:\", \"DONE\"]),\n",
    "        MaxStepsTermination(20),\n",
    "    ])\n",
    "    \n",
    "    reward_fn = MyTaskReward(num_objectives=2)\n",
    "    loss_fn = ReinforceLoss(normalize_rewards=True)\n",
    "    \n",
    "    # 4. Optional: Reference model for KL\n",
    "    # ref_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, ...)\n",
    "    ref_model = None  # Set to None for no KL penalty\n",
    "    \n",
    "    # 5. Training buffer\n",
    "    buffer = TrajectoryBuffer(max_buffer_size=1000, num_objectives=2)\n",
    "    pareto = ParetoRewardTracker([\"completion\", \"efficiency\"])\n",
    "    buffer.pareto_tracker = pareto\n",
    "    \n",
    "    # 6. YOUR TRAINING LOOP - you control it\n",
    "    for iteration in range(10):\n",
    "        # Get prompts (YOUR DATA)\n",
    "        prompts = [\n",
    "            \"Solve: What is 15 + 27? Think step by step, then give ANSWER:\",\n",
    "            \"Solve: What is 8 * 9? Think step by step, then give ANSWER:\",\n",
    "        ]\n",
    "        \n",
    "        # Collect rollouts using the primitive\n",
    "        rollouts = collect_rollout_batch(\n",
    "            backend=backend,\n",
    "            prompts=prompts,\n",
    "            termination_condition=termination,\n",
    "            reward_function=reward_fn,\n",
    "            max_steps=20,\n",
    "            num_objectives=2,\n",
    "            max_new_tokens=128,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        \n",
    "        # Add to buffer\n",
    "        buffer.add_batch(rollouts)\n",
    "        \n",
    "        # Sample from buffer\n",
    "        sampled = buffer.sample_batch(batch_size=4, strategy=\"uniform\")\n",
    "        \n",
    "        # Prepare batch\n",
    "        weights = pareto.get_pareto_weights(\"random\")  # Random scalarization\n",
    "        train_batch = buffer.prepare_training_batch(sampled, weights)\n",
    "        \n",
    "        # Training step using the primitive\n",
    "        metrics = training_step(\n",
    "            batch=train_batch,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            optimizer=optimizer,\n",
    "            loss_fn=loss_fn,\n",
    "            ref_model=ref_model,\n",
    "            kl_coef=0.0,  # No KL penalty\n",
    "            max_grad_norm=1.0,\n",
    "        )\n",
    "        \n",
    "        print(f\"Iter {iteration}: loss={metrics['total_loss']:.4f}, reward={metrics.get('avg_reward', 0):.4f}\")\n",
    "    \n",
    "    return model, buffer, pareto\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE: SVRL (Self-Verifying RL) with Primitives\n",
    "# =============================================================================\n",
    "\n",
    "class SVRLReward(RewardFunction):\n",
    "    \"\"\"Reward that accounts for verification cost\"\"\"\n",
    "    \n",
    "    def __init__(self, env: SVRLEnvironment, correctness_fn: Callable):\n",
    "        self.env = env\n",
    "        self.correctness_fn = correctness_fn\n",
    "    \n",
    "    def compute_reward(self, state: RolloutState, step: StepData) -> np.ndarray:\n",
    "        # 3 objectives: correctness, efficiency, verification_cost\n",
    "        reward = np.zeros(3)\n",
    "        \n",
    "        # Check for verification request\n",
    "        query = self.env.parse_verification_request(step.response.text)\n",
    "        if query:\n",
    "            result, cost, success = self.env.query_environment(query, state)\n",
    "            if success:\n",
    "                # Penalize verification cost\n",
    "                reward[2] = -cost\n",
    "        \n",
    "        # Final rewards on termination\n",
    "        if state.terminated:\n",
    "            reward[0] = self.correctness_fn(state)  # Task correctness\n",
    "            reward[1] = -state.step_count * 0.1  # Efficiency\n",
    "        \n",
    "        return reward\n",
    "\n",
    "\n",
    "def example_svrl():\n",
    "    \"\"\"SVRL experiment using primitives\"\"\"\n",
    "    \n",
    "    # Your verification environment\n",
    "    def verify_math(query: str) -> str:\n",
    "        try:\n",
    "            return f\"Result: {eval(query)}\"\n",
    "        except:\n",
    "            return \"Error: Invalid expression\"\n",
    "    \n",
    "    svrl_env = SVRLEnvironment(\n",
    "        cost_function=lambda q, s: 1.0 + s.step_count * 0.1,\n",
    "        verification_handler=verify_math,\n",
    "    )\n",
    "    \n",
    "    def check_correctness(state: RolloutState) -> float:\n",
    "        # YOUR LOGIC: check if final answer is correct\n",
    "        return 1.0 if \"42\" in state.get_full_context() else 0.0\n",
    "    \n",
    "    reward_fn = SVRLReward(svrl_env, check_correctness)\n",
    "    \n",
    "    # Use with collect_single_rollout...\n",
    "    return svrl_env, reward_fn\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE: PPO Training (Just Change the Loss Function)\n",
    "# =============================================================================\n",
    "\n",
    "def example_ppo_training():\n",
    "    \"\"\"PPO training - just swap the loss function\"\"\"\n",
    "    \n",
    "    # ... setup as before ...\n",
    "    \n",
    "    # Use PPO loss instead\n",
    "    loss_fn = PPOLoss(clip_epsilon=0.2)\n",
    "    \n",
    "    # You need to track old_logprobs and compute advantages\n",
    "    # This is where you'd add value network, GAE, etc.\n",
    "    \n",
    "    # Same training loop, different loss function\n",
    "    pass\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE: No KL Penalty (Just Set kl_coef=0)\n",
    "# =============================================================================\n",
    "\n",
    "def example_no_kl():\n",
    "    \"\"\"Training without KL penalty\"\"\"\n",
    "    \n",
    "    # Just call training_step with kl_coef=0\n",
    "    # metrics = training_step(batch, model, tokenizer, optimizer, loss_fn, kl_coef=0.0)\n",
    "    pass\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE: Dynamic Reference Model\n",
    "# =============================================================================\n",
    "\n",
    "def example_dynamic_ref():\n",
    "    \"\"\"Update reference model during training\"\"\"\n",
    "    \n",
    "    # Clone weights periodically\n",
    "    # ref_model.load_state_dict(train_model.state_dict())\n",
    "    \n",
    "    # Or use ReferenceModelManager for snapshots\n",
    "    pass\n",
    "\n",
    "\n",
    "print(\"✓ Composition examples defined\")\n",
    "print(\"\\\\nKey insight: No monolithic trainer class!\")\n",
    "print(\"You compose these primitives:\")\n",
    "print(\"  1. collect_single_rollout(backend, prompt, termination, reward, ...)\")\n",
    "print(\"  2. buffer.sample_batch(...)\")\n",
    "print(\"  3. buffer.prepare_training_batch(trajectories, weights)\")\n",
    "print(\"  4. training_step(batch, model, tokenizer, optimizer, loss_fn, ...)\")\n",
    "print(\"\\\\nSwap any component:\")\n",
    "print(\"  - Different loss? Use PPOLoss, GRPOLoss, or write your own\")\n",
    "print(\"  - Different termination? Extend TerminationCondition\")\n",
    "print(\"  - Different rewards? Extend RewardFunction\")\n",
    "print(\"  - No KL penalty? Set kl_coef=0\")\n",
    "print(\"  - Different sampling? Use buffer.sample_batch with different strategy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k1b4ye5ygcr",
   "metadata": {},
   "source": [
    "# Thinking Token Handling (Qwen, DeepSeek, etc.)\n",
    "Models with reasoning/thinking tokens need special handling in multi-turn rollouts:\n",
    "- Thinking tokens are generated but NOT sent back in the next turn\n",
    "- Only the actual response is added to conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5kim3z61az9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# THINKING TOKEN HANDLING\n",
    "# =============================================================================\n",
    "\n",
    "# Qwen thinking tokens\n",
    "QWEN_THINK_START_TOKEN = 151667  # <think>\n",
    "QWEN_THINK_END_TOKEN = 151668    # </think>\n",
    "\n",
    "@dataclass\n",
    "class ThinkingResponse:\n",
    "    \"\"\"Response split into thinking and content parts\"\"\"\n",
    "    thinking: str\n",
    "    content: str\n",
    "    full_text: str\n",
    "    thinking_token_ids: List[int]\n",
    "    content_token_ids: List[int]\n",
    "\n",
    "\n",
    "def parse_thinking_tokens(\n",
    "    output_ids: List[int],\n",
    "    tokenizer,\n",
    "    think_end_token: int = QWEN_THINK_END_TOKEN,\n",
    ") -> ThinkingResponse:\n",
    "    \"\"\"\n",
    "    Parse model output to separate thinking from response.\n",
    "    Critical for multi-turn: only content goes back to model.\n",
    "    \n",
    "    Args:\n",
    "        output_ids: Generated token IDs\n",
    "        tokenizer: Tokenizer for decoding\n",
    "        think_end_token: Token ID marking end of thinking\n",
    "    \n",
    "    Returns:\n",
    "        ThinkingResponse with separated thinking and content\n",
    "    \"\"\"\n",
    "    if isinstance(output_ids, torch.Tensor):\n",
    "        output_ids = output_ids.tolist()\n",
    "    \n",
    "    # Find thinking end token (search backwards for robustness)\n",
    "    try:\n",
    "        reversed_ids = output_ids[::-1]\n",
    "        reverse_idx = reversed_ids.index(think_end_token)\n",
    "        split_idx = len(output_ids) - reverse_idx\n",
    "    except ValueError:\n",
    "        # No thinking token found - entire output is content\n",
    "        split_idx = 0\n",
    "    \n",
    "    thinking_ids = output_ids[:split_idx]\n",
    "    content_ids = output_ids[split_idx:]\n",
    "    \n",
    "    thinking_text = tokenizer.decode(thinking_ids, skip_special_tokens=True).strip()\n",
    "    content_text = tokenizer.decode(content_ids, skip_special_tokens=True).strip()\n",
    "    full_text = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "    \n",
    "    return ThinkingResponse(\n",
    "        thinking=thinking_text,\n",
    "        content=content_text,\n",
    "        full_text=full_text,\n",
    "        thinking_token_ids=thinking_ids,\n",
    "        content_token_ids=content_ids,\n",
    "    )\n",
    "\n",
    "\n",
    "def apply_chat_template(\n",
    "    messages: List[Dict[str, str]],\n",
    "    tokenizer,\n",
    "    enable_thinking: bool = True,\n",
    "    add_generation_prompt: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Apply chat template to messages.\n",
    "    For models with thinking support, enables thinking mode.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of {\"role\": \"user/assistant/system\", \"content\": \"...\"}\n",
    "        tokenizer: Tokenizer with chat template\n",
    "        enable_thinking: Enable reasoning/thinking for supported models\n",
    "        add_generation_prompt: Add generation prompt at end\n",
    "    \n",
    "    Returns:\n",
    "        Formatted prompt string\n",
    "    \"\"\"\n",
    "    if not hasattr(tokenizer, 'apply_chat_template'):\n",
    "        # Fallback for tokenizers without chat template\n",
    "        return \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in messages])\n",
    "    \n",
    "    # Try to pass enable_thinking if tokenizer supports it\n",
    "    try:\n",
    "        return tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=add_generation_prompt,\n",
    "            enable_thinking=enable_thinking,\n",
    "        )\n",
    "    except TypeError:\n",
    "        # Tokenizer doesn't support enable_thinking parameter\n",
    "        return tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=add_generation_prompt,\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Message:\n",
    "    \"\"\"Single message in conversation\"\"\"\n",
    "    role: str  # \"user\", \"assistant\", \"system\", \"tool\"\n",
    "    content: str\n",
    "    thinking: Optional[str] = None  # Stored but not sent back\n",
    "    tool_calls: Optional[List[Dict]] = None\n",
    "    tool_call_id: Optional[str] = None\n",
    "    name: Optional[str] = None  # For tool responses\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert to dict for chat template (without thinking)\"\"\"\n",
    "        d = {\"role\": self.role, \"content\": self.content}\n",
    "        if self.tool_calls:\n",
    "            d[\"tool_calls\"] = self.tool_calls\n",
    "        if self.tool_call_id:\n",
    "            d[\"tool_call_id\"] = self.tool_call_id\n",
    "        if self.name:\n",
    "            d[\"name\"] = self.name\n",
    "        return d\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Conversation:\n",
    "    \"\"\"Conversation history with thinking separation\"\"\"\n",
    "    messages: List[Message] = field(default_factory=list)\n",
    "    \n",
    "    def add_user_message(self, content: str) -> 'Conversation':\n",
    "        \"\"\"Add user message\"\"\"\n",
    "        new_msgs = self.messages + [Message(role=\"user\", content=content)]\n",
    "        return Conversation(messages=new_msgs)\n",
    "    \n",
    "    def add_assistant_message(\n",
    "        self,\n",
    "        content: str,\n",
    "        thinking: Optional[str] = None,\n",
    "        tool_calls: Optional[List[Dict]] = None,\n",
    "    ) -> 'Conversation':\n",
    "        \"\"\"Add assistant message (content only, thinking stored separately)\"\"\"\n",
    "        msg = Message(\n",
    "            role=\"assistant\",\n",
    "            content=content,\n",
    "            thinking=thinking,\n",
    "            tool_calls=tool_calls,\n",
    "        )\n",
    "        new_msgs = self.messages + [msg]\n",
    "        return Conversation(messages=new_msgs)\n",
    "    \n",
    "    def add_tool_result(\n",
    "        self,\n",
    "        tool_call_id: str,\n",
    "        name: str,\n",
    "        content: str,\n",
    "    ) -> 'Conversation':\n",
    "        \"\"\"Add tool/function result\"\"\"\n",
    "        msg = Message(\n",
    "            role=\"tool\",\n",
    "            content=content,\n",
    "            tool_call_id=tool_call_id,\n",
    "            name=name,\n",
    "        )\n",
    "        new_msgs = self.messages + [msg]\n",
    "        return Conversation(messages=new_msgs)\n",
    "    \n",
    "    def to_messages_list(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Convert to list of dicts for chat template (NO thinking)\"\"\"\n",
    "        return [m.to_dict() for m in self.messages]\n",
    "    \n",
    "    def format_prompt(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        enable_thinking: bool = True,\n",
    "    ) -> str:\n",
    "        \"\"\"Format conversation as prompt string\"\"\"\n",
    "        return apply_chat_template(\n",
    "            self.to_messages_list(),\n",
    "            tokenizer,\n",
    "            enable_thinking=enable_thinking,\n",
    "        )\n",
    "    \n",
    "    def get_full_text_with_thinking(self) -> str:\n",
    "        \"\"\"Get full text including thinking (for analysis)\"\"\"\n",
    "        parts = []\n",
    "        for m in self.messages:\n",
    "            if m.thinking:\n",
    "                parts.append(f\"{m.role}: <think>{m.thinking}</think>{m.content}\")\n",
    "            else:\n",
    "                parts.append(f\"{m.role}: {m.content}\")\n",
    "        return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "def generate_with_thinking(\n",
    "    backend: InferenceBackend,\n",
    "    conversation: Conversation,\n",
    "    tokenizer,\n",
    "    enable_thinking: bool = True,\n",
    "    max_new_tokens: int = 512,\n",
    "    temperature: float = 0.6,  # Lower for focused reasoning\n",
    "    **gen_kwargs,\n",
    ") -> Tuple[ThinkingResponse, GenerationResult]:\n",
    "    \"\"\"\n",
    "    Generate response with thinking token parsing.\n",
    "    \n",
    "    Returns:\n",
    "        thinking_response: Parsed thinking and content\n",
    "        gen_result: Full generation result with all metadata\n",
    "    \"\"\"\n",
    "    # Format prompt with chat template\n",
    "    prompt = conversation.format_prompt(tokenizer, enable_thinking=enable_thinking)\n",
    "    \n",
    "    # Generate\n",
    "    results = backend.generate(\n",
    "        [prompt],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        return_logprobs=True,  # Important for RL\n",
    "        **gen_kwargs,\n",
    "    )\n",
    "    gen_result = results[0]\n",
    "    \n",
    "    # Parse thinking tokens\n",
    "    thinking_response = parse_thinking_tokens(\n",
    "        gen_result.token_ids,\n",
    "        tokenizer,\n",
    "    )\n",
    "    \n",
    "    return thinking_response, gen_result\n",
    "\n",
    "\n",
    "def collect_multiturn_rollout(\n",
    "    backend: InferenceBackend,\n",
    "    tokenizer,\n",
    "    initial_messages: List[Dict[str, str]],\n",
    "    termination_condition: TerminationCondition,\n",
    "    reward_function: RewardFunction,\n",
    "    max_turns: int = 10,\n",
    "    enable_thinking: bool = True,\n",
    "    max_new_tokens: int = 512,\n",
    "    temperature: float = 0.6,\n",
    "    num_objectives: int = 1,\n",
    "    **gen_kwargs,\n",
    ") -> RolloutState:\n",
    "    \"\"\"\n",
    "    Collect multi-turn rollout with thinking token handling.\n",
    "    \n",
    "    Key: Only content (not thinking) is added to conversation for next turn.\n",
    "    Thinking is stored in metadata for analysis.\n",
    "    \"\"\"\n",
    "    # Initialize conversation\n",
    "    conversation = Conversation(\n",
    "        messages=[Message(**m) for m in initial_messages]\n",
    "    )\n",
    "    \n",
    "    # Initialize rollout state\n",
    "    state = RolloutState(\n",
    "        rollout_id=str(uuid.uuid4()),\n",
    "        initial_prompt=conversation.format_prompt(tokenizer, enable_thinking),\n",
    "        cumulative_reward=np.zeros(num_objectives),\n",
    "        metadata={\"conversation\": conversation, \"enable_thinking\": enable_thinking},\n",
    "    )\n",
    "    \n",
    "    for turn in range(max_turns):\n",
    "        # Generate with thinking\n",
    "        thinking_resp, gen_result = generate_with_thinking(\n",
    "            backend,\n",
    "            conversation,\n",
    "            tokenizer,\n",
    "            enable_thinking=enable_thinking,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            **gen_kwargs,\n",
    "        )\n",
    "        \n",
    "        # Add ONLY content to conversation (not thinking!)\n",
    "        conversation = conversation.add_assistant_message(\n",
    "            content=thinking_resp.content,\n",
    "            thinking=thinking_resp.thinking,\n",
    "        )\n",
    "        \n",
    "        # Create step data with thinking info\n",
    "        step_data = StepData(\n",
    "            prompt=state.get_full_context(),\n",
    "            response=gen_result,\n",
    "            metadata={\n",
    "                \"thinking\": thinking_resp.thinking,\n",
    "                \"content\": thinking_resp.content,\n",
    "                \"turn\": turn,\n",
    "            },\n",
    "        )\n",
    "        \n",
    "        # Add step\n",
    "        state.add_step(step_data)\n",
    "        state.metadata[\"conversation\"] = conversation\n",
    "        \n",
    "        # Check termination\n",
    "        should_stop, reason = termination_condition.should_terminate(state)\n",
    "        if gen_result.finish_reason == \"stop\":\n",
    "            should_stop, reason = True, \"eos\"\n",
    "        \n",
    "        if should_stop:\n",
    "            state.terminated = True\n",
    "            state.termination_reason = reason\n",
    "            step_data.reward = reward_function.compute_reward(state, step_data)\n",
    "            state.cumulative_reward = state.cumulative_reward + step_data.reward\n",
    "            break\n",
    "        else:\n",
    "            step_data.reward = reward_function.compute_reward(state, step_data)\n",
    "            state.cumulative_reward = state.cumulative_reward + step_data.reward\n",
    "    \n",
    "    if not state.terminated:\n",
    "        state.terminated = True\n",
    "        state.termination_reason = \"max_turns\"\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "print(\"✓ Thinking token handling defined\")\n",
    "print(\"  - parse_thinking_tokens(output_ids, tokenizer)\")\n",
    "print(\"  - apply_chat_template(messages, tokenizer, enable_thinking)\")\n",
    "print(\"  - Conversation class for multi-turn with thinking separation\")\n",
    "print(\"  - generate_with_thinking(backend, conversation, tokenizer)\")\n",
    "print(\"  - collect_multiturn_rollout(...) for complete rollout with thinking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1phruzwuxfbj",
   "metadata": {},
   "source": [
    "# Tool/Function Calling Support\n",
    "Enable LLMs to call tools/functions during rollouts. Supports vLLM and SGLang with model-specific parsers (Qwen, Llama, DeepSeek)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zdiaq1hokpc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TOOL/FUNCTION CALLING SUPPORT\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ToolDefinition:\n",
    "    \"\"\"Definition of a tool/function the model can call\"\"\"\n",
    "    name: str\n",
    "    description: str\n",
    "    parameters: Dict[str, Any]  # JSON Schema\n",
    "    \n",
    "    def to_openai_format(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert to OpenAI-compatible format\"\"\"\n",
    "        return {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": self.name,\n",
    "                \"description\": self.description,\n",
    "                \"parameters\": self.parameters,\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ToolCall:\n",
    "    \"\"\"A tool call made by the model\"\"\"\n",
    "    id: str\n",
    "    name: str\n",
    "    arguments: Dict[str, Any]\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"id\": self.id,\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": self.name,\n",
    "                \"arguments\": json.dumps(self.arguments),\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ToolResult:\n",
    "    \"\"\"Result from executing a tool\"\"\"\n",
    "    tool_call_id: str\n",
    "    name: str\n",
    "    content: str\n",
    "    success: bool = True\n",
    "    error: Optional[str] = None\n",
    "\n",
    "\n",
    "class ToolExecutor(ABC):\n",
    "    \"\"\"Abstract tool executor. Implement your tools here.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def execute(self, name: str, arguments: Dict[str, Any]) -> str:\n",
    "        \"\"\"Execute tool and return result string\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_tools(self) -> List[ToolDefinition]:\n",
    "        \"\"\"Get list of available tools\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class SimpleToolExecutor(ToolExecutor):\n",
    "    \"\"\"Simple tool executor with registered functions\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tools: Dict[str, ToolDefinition] = {}\n",
    "        self.handlers: Dict[str, Callable] = {}\n",
    "    \n",
    "    def register_tool(\n",
    "        self,\n",
    "        name: str,\n",
    "        description: str,\n",
    "        parameters: Dict[str, Any],\n",
    "        handler: Callable[[Dict], str],\n",
    "    ):\n",
    "        \"\"\"Register a tool\"\"\"\n",
    "        self.tools[name] = ToolDefinition(name, description, parameters)\n",
    "        self.handlers[name] = handler\n",
    "    \n",
    "    def execute(self, name: str, arguments: Dict[str, Any]) -> str:\n",
    "        if name not in self.handlers:\n",
    "            return f\"Error: Unknown tool '{name}'\"\n",
    "        \n",
    "        try:\n",
    "            return self.handlers[name](arguments)\n",
    "        except Exception as e:\n",
    "            return f\"Error executing {name}: {str(e)}\"\n",
    "    \n",
    "    def get_tools(self) -> List[ToolDefinition]:\n",
    "        return list(self.tools.values())\n",
    "\n",
    "\n",
    "def parse_tool_calls_qwen(text: str) -> List[ToolCall]:\n",
    "    \"\"\"\n",
    "    Parse tool calls from Qwen model output.\n",
    "    \n",
    "    Qwen format:\n",
    "    <|plugin|>\n",
    "    {\"name\": \"tool_name\", \"parameters\": {...}}\n",
    "    \"\"\"\n",
    "    calls = []\n",
    "    \n",
    "    if \"<|plugin|>\" in text:\n",
    "        parts = text.split(\"<|plugin|>\")\n",
    "        for i, part in enumerate(parts[1:], start=1):  # Skip first part\n",
    "            part = part.strip()\n",
    "            if part:\n",
    "                try:\n",
    "                    # Try to parse JSON\n",
    "                    # Find the JSON object\n",
    "                    json_start = part.find(\"{\")\n",
    "                    json_end = part.rfind(\"}\") + 1\n",
    "                    if json_start >= 0 and json_end > json_start:\n",
    "                        json_str = part[json_start:json_end]\n",
    "                        data = json.loads(json_str)\n",
    "                        \n",
    "                        calls.append(ToolCall(\n",
    "                            id=f\"call_{i}\",\n",
    "                            name=data.get(\"name\", \"\"),\n",
    "                            arguments=data.get(\"parameters\", {}),\n",
    "                        ))\n",
    "                except json.JSONDecodeError:\n",
    "                    pass\n",
    "    \n",
    "    return calls\n",
    "\n",
    "\n",
    "def parse_tool_calls_llama(text: str) -> List[ToolCall]:\n",
    "    \"\"\"\n",
    "    Parse tool calls from Llama 3.x model output.\n",
    "    \n",
    "    Llama format (pythonic):\n",
    "    [function_name(arg1=\"value1\", arg2=\"value2\")]\n",
    "    \"\"\"\n",
    "    import re\n",
    "    calls = []\n",
    "    \n",
    "    # Match pattern: [function_name(...)]\n",
    "    pattern = r'\\[(\\w+)\\((.*?)\\)\\]'\n",
    "    matches = re.findall(pattern, text)\n",
    "    \n",
    "    for i, (func_name, args_str) in enumerate(matches):\n",
    "        try:\n",
    "            # Parse pythonic arguments\n",
    "            args = {}\n",
    "            if args_str:\n",
    "                # Simple parsing: key=\"value\" or key=value\n",
    "                for part in args_str.split(\",\"):\n",
    "                    part = part.strip()\n",
    "                    if \"=\" in part:\n",
    "                        key, value = part.split(\"=\", 1)\n",
    "                        key = key.strip()\n",
    "                        value = value.strip().strip('\"\\'')\n",
    "                        args[key] = value\n",
    "            \n",
    "            calls.append(ToolCall(\n",
    "                id=f\"call_{i}\",\n",
    "                name=func_name,\n",
    "                arguments=args,\n",
    "            ))\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    return calls\n",
    "\n",
    "\n",
    "def parse_tool_calls_deepseek(text: str) -> List[ToolCall]:\n",
    "    \"\"\"\n",
    "    Parse tool calls from DeepSeek model output.\n",
    "    \n",
    "    DeepSeek format:\n",
    "    <｜tool_calls_begin｜><｜tool_call_begin｜>function<｜tool_sep｜>name\n",
    "    {\"arg\": \"value\"}\n",
    "    <｜tool_call_end｜><｜tool_calls_end｜>\n",
    "    \"\"\"\n",
    "    calls = []\n",
    "    \n",
    "    if \"<｜tool_calls_begin｜>\" in text:\n",
    "        # Extract between markers\n",
    "        start_marker = \"<｜tool_call_begin｜>\"\n",
    "        end_marker = \"<｜tool_call_end｜>\"\n",
    "        sep_marker = \"<｜tool_sep｜>\"\n",
    "        \n",
    "        parts = text.split(start_marker)\n",
    "        for i, part in enumerate(parts[1:], start=1):\n",
    "            if end_marker in part:\n",
    "                call_text = part.split(end_marker)[0]\n",
    "                \n",
    "                if sep_marker in call_text:\n",
    "                    func_type, rest = call_text.split(sep_marker, 1)\n",
    "                    lines = rest.strip().split(\"\\n\")\n",
    "                    \n",
    "                    if lines:\n",
    "                        func_name = lines[0].strip()\n",
    "                        json_str = \"\\n\".join(lines[1:]).strip()\n",
    "                        \n",
    "                        try:\n",
    "                            args = json.loads(json_str) if json_str else {}\n",
    "                            calls.append(ToolCall(\n",
    "                                id=f\"call_{i}\",\n",
    "                                name=func_name,\n",
    "                                arguments=args,\n",
    "                            ))\n",
    "                        except json.JSONDecodeError:\n",
    "                            pass\n",
    "    \n",
    "    return calls\n",
    "\n",
    "\n",
    "def parse_tool_calls(text: str, parser: str = \"qwen25\") -> List[ToolCall]:\n",
    "    \"\"\"\n",
    "    Parse tool calls from model output using specified parser.\n",
    "    \n",
    "    Args:\n",
    "        text: Model output text\n",
    "        parser: Parser type (\"qwen25\", \"llama3\", \"pythonic\", \"deepseekv3\")\n",
    "    \n",
    "    Returns:\n",
    "        List of ToolCall objects\n",
    "    \"\"\"\n",
    "    if parser in [\"qwen25\", \"qwen3\"]:\n",
    "        return parse_tool_calls_qwen(text)\n",
    "    elif parser in [\"llama3\", \"llama4\"]:\n",
    "        return parse_tool_calls_llama(text)\n",
    "    elif parser == \"pythonic\":\n",
    "        return parse_tool_calls_llama(text)  # Same format\n",
    "    elif parser in [\"deepseekv3\", \"deepseekv31\"]:\n",
    "        return parse_tool_calls_deepseek(text)\n",
    "    else:\n",
    "        # Try all parsers\n",
    "        calls = parse_tool_calls_qwen(text)\n",
    "        if not calls:\n",
    "            calls = parse_tool_calls_llama(text)\n",
    "        if not calls:\n",
    "            calls = parse_tool_calls_deepseek(text)\n",
    "        return calls\n",
    "\n",
    "\n",
    "def format_tools_for_prompt(\n",
    "    tools: List[ToolDefinition],\n",
    "    format_type: str = \"openai\",\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Format tools for model prompt.\n",
    "    \n",
    "    Args:\n",
    "        tools: List of tool definitions\n",
    "        format_type: \"openai\" (standard) or \"json_schema\"\n",
    "    \n",
    "    Returns:\n",
    "        Formatted tools list\n",
    "    \"\"\"\n",
    "    if format_type == \"openai\":\n",
    "        return [tool.to_openai_format() for tool in tools]\n",
    "    elif format_type == \"json_schema\":\n",
    "        return {\n",
    "            \"tools\": [\n",
    "                {\n",
    "                    \"name\": tool.name,\n",
    "                    \"description\": tool.description,\n",
    "                    \"parameters\": tool.parameters,\n",
    "                }\n",
    "                for tool in tools\n",
    "            ]\n",
    "        }\n",
    "    else:\n",
    "        return [tool.to_openai_format() for tool in tools]\n",
    "\n",
    "\n",
    "class ToolUseRolloutManager:\n",
    "    \"\"\"\n",
    "    Rollout manager that supports tool/function calling.\n",
    "    \n",
    "    Flow:\n",
    "    1. Generate response\n",
    "    2. Check for tool calls\n",
    "    3. Execute tools\n",
    "    4. Add results to conversation\n",
    "    5. Continue until no more tool calls or termination\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        backend: InferenceBackend,\n",
    "        tokenizer,\n",
    "        tool_executor: ToolExecutor,\n",
    "        tool_call_parser: str = \"qwen25\",\n",
    "        max_tool_calls_per_turn: int = 5,\n",
    "    ):\n",
    "        self.backend = backend\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tool_executor = tool_executor\n",
    "        self.tool_call_parser = tool_call_parser\n",
    "        self.max_tool_calls_per_turn = max_tool_calls_per_turn\n",
    "    \n",
    "    def generate_with_tools(\n",
    "        self,\n",
    "        conversation: Conversation,\n",
    "        max_new_tokens: int = 512,\n",
    "        temperature: float = 0.7,\n",
    "        enable_thinking: bool = True,\n",
    "        **gen_kwargs,\n",
    "    ) -> Tuple[GenerationResult, List[ToolCall], ThinkingResponse]:\n",
    "        \"\"\"\n",
    "        Generate response that may contain tool calls.\n",
    "        \n",
    "        Returns:\n",
    "            gen_result: Full generation result\n",
    "            tool_calls: List of parsed tool calls\n",
    "            thinking_response: Parsed thinking and content\n",
    "        \"\"\"\n",
    "        # Format prompt with tools\n",
    "        prompt = conversation.format_prompt(self.tokenizer, enable_thinking)\n",
    "        \n",
    "        # Generate\n",
    "        results = self.backend.generate(\n",
    "            [prompt],\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            return_logprobs=True,\n",
    "            **gen_kwargs,\n",
    "        )\n",
    "        gen_result = results[0]\n",
    "        \n",
    "        # Parse thinking\n",
    "        thinking_response = parse_thinking_tokens(gen_result.token_ids, self.tokenizer)\n",
    "        \n",
    "        # Parse tool calls from content\n",
    "        tool_calls = parse_tool_calls(thinking_response.content, self.tool_call_parser)\n",
    "        \n",
    "        return gen_result, tool_calls, thinking_response\n",
    "    \n",
    "    def execute_tool_calls(\n",
    "        self,\n",
    "        tool_calls: List[ToolCall],\n",
    "    ) -> List[ToolResult]:\n",
    "        \"\"\"Execute all tool calls\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for call in tool_calls[:self.max_tool_calls_per_turn]:\n",
    "            result_str = self.tool_executor.execute(call.name, call.arguments)\n",
    "            \n",
    "            results.append(ToolResult(\n",
    "                tool_call_id=call.id,\n",
    "                name=call.name,\n",
    "                content=result_str,\n",
    "            ))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def collect_rollout_with_tools(\n",
    "        self,\n",
    "        initial_messages: List[Dict[str, str]],\n",
    "        termination_condition: TerminationCondition,\n",
    "        reward_function: RewardFunction,\n",
    "        max_turns: int = 10,\n",
    "        max_new_tokens: int = 512,\n",
    "        temperature: float = 0.7,\n",
    "        enable_thinking: bool = True,\n",
    "        num_objectives: int = 1,\n",
    "        **gen_kwargs,\n",
    "    ) -> RolloutState:\n",
    "        \"\"\"\n",
    "        Collect rollout with tool use support.\n",
    "        \n",
    "        Each turn:\n",
    "        1. Generate (may include tool calls)\n",
    "        2. If tool calls: execute and add results\n",
    "        3. If no tool calls: check termination\n",
    "        \"\"\"\n",
    "        conversation = Conversation(\n",
    "            messages=[Message(**m) for m in initial_messages]\n",
    "        )\n",
    "        \n",
    "        state = RolloutState(\n",
    "            rollout_id=str(uuid.uuid4()),\n",
    "            initial_prompt=conversation.format_prompt(self.tokenizer, enable_thinking),\n",
    "            cumulative_reward=np.zeros(num_objectives),\n",
    "            metadata={\n",
    "                \"conversation\": conversation,\n",
    "                \"tool_calls\": [],\n",
    "                \"tool_results\": [],\n",
    "            },\n",
    "        )\n",
    "        \n",
    "        for turn in range(max_turns):\n",
    "            # Generate\n",
    "            gen_result, tool_calls, thinking_resp = self.generate_with_tools(\n",
    "                conversation,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                enable_thinking=enable_thinking,\n",
    "                **gen_kwargs,\n",
    "            )\n",
    "            \n",
    "            # Add assistant message\n",
    "            if tool_calls:\n",
    "                # Has tool calls\n",
    "                conversation = conversation.add_assistant_message(\n",
    "                    content=thinking_resp.content,\n",
    "                    thinking=thinking_resp.thinking,\n",
    "                    tool_calls=[tc.to_dict() for tc in tool_calls],\n",
    "                )\n",
    "                \n",
    "                # Execute tools\n",
    "                tool_results = self.execute_tool_calls(tool_calls)\n",
    "                \n",
    "                # Add tool results to conversation\n",
    "                for result in tool_results:\n",
    "                    conversation = conversation.add_tool_result(\n",
    "                        tool_call_id=result.tool_call_id,\n",
    "                        name=result.name,\n",
    "                        content=result.content,\n",
    "                    )\n",
    "                \n",
    "                # Track\n",
    "                state.metadata[\"tool_calls\"].append(tool_calls)\n",
    "                state.metadata[\"tool_results\"].append(tool_results)\n",
    "                \n",
    "                # Create step\n",
    "                step_data = StepData(\n",
    "                    prompt=state.get_full_context(),\n",
    "                    response=gen_result,\n",
    "                    metadata={\n",
    "                        \"thinking\": thinking_resp.thinking,\n",
    "                        \"content\": thinking_resp.content,\n",
    "                        \"tool_calls\": [tc.to_dict() for tc in tool_calls],\n",
    "                        \"tool_results\": [r.content for r in tool_results],\n",
    "                        \"turn\": turn,\n",
    "                    },\n",
    "                )\n",
    "                state.add_step(step_data)\n",
    "                state.metadata[\"conversation\"] = conversation\n",
    "                \n",
    "                # Compute reward for tool use step\n",
    "                step_data.reward = reward_function.compute_reward(state, step_data)\n",
    "                state.cumulative_reward = state.cumulative_reward + step_data.reward\n",
    "                \n",
    "                # Continue to next turn (don't check termination yet)\n",
    "                continue\n",
    "            \n",
    "            else:\n",
    "                # No tool calls - regular response\n",
    "                conversation = conversation.add_assistant_message(\n",
    "                    content=thinking_resp.content,\n",
    "                    thinking=thinking_resp.thinking,\n",
    "                )\n",
    "                \n",
    "                step_data = StepData(\n",
    "                    prompt=state.get_full_context(),\n",
    "                    response=gen_result,\n",
    "                    metadata={\n",
    "                        \"thinking\": thinking_resp.thinking,\n",
    "                        \"content\": thinking_resp.content,\n",
    "                        \"turn\": turn,\n",
    "                    },\n",
    "                )\n",
    "                state.add_step(step_data)\n",
    "                state.metadata[\"conversation\"] = conversation\n",
    "                \n",
    "                # Check termination\n",
    "                should_stop, reason = termination_condition.should_terminate(state)\n",
    "                if gen_result.finish_reason == \"stop\":\n",
    "                    should_stop, reason = True, \"eos\"\n",
    "                \n",
    "                if should_stop:\n",
    "                    state.terminated = True\n",
    "                    state.termination_reason = reason\n",
    "                    step_data.reward = reward_function.compute_reward(state, step_data)\n",
    "                    state.cumulative_reward = state.cumulative_reward + step_data.reward\n",
    "                    break\n",
    "                else:\n",
    "                    step_data.reward = reward_function.compute_reward(state, step_data)\n",
    "                    state.cumulative_reward = state.cumulative_reward + step_data.reward\n",
    "        \n",
    "        if not state.terminated:\n",
    "            state.terminated = True\n",
    "            state.termination_reason = \"max_turns\"\n",
    "        \n",
    "        return state\n",
    "\n",
    "\n",
    "# Example tool setup\n",
    "def example_tool_setup():\n",
    "    \"\"\"Example: Set up tools for math verification\"\"\"\n",
    "    \n",
    "    executor = SimpleToolExecutor()\n",
    "    \n",
    "    # Register math calculator\n",
    "    executor.register_tool(\n",
    "        name=\"calculate\",\n",
    "        description=\"Perform mathematical calculations\",\n",
    "        parameters={\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"expression\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Mathematical expression to evaluate\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"expression\"]\n",
    "        },\n",
    "        handler=lambda args: str(eval(args[\"expression\"]))\n",
    "    )\n",
    "    \n",
    "    # Register web search (placeholder)\n",
    "    executor.register_tool(\n",
    "        name=\"search\",\n",
    "        description=\"Search the web for information\",\n",
    "        parameters={\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Search query\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"]\n",
    "        },\n",
    "        handler=lambda args: f\"Search results for: {args['query']}\"\n",
    "    )\n",
    "    \n",
    "    return executor\n",
    "\n",
    "\n",
    "print(\"✓ Tool/function calling support defined\")\n",
    "print(\"  - ToolDefinition, ToolCall, ToolResult dataclasses\")\n",
    "print(\"  - SimpleToolExecutor for registering tools\")\n",
    "print(\"  - parse_tool_calls(text, parser) for Qwen/Llama/DeepSeek\")\n",
    "print(\"  - ToolUseRolloutManager.collect_rollout_with_tools(...)\")\n",
    "print(\"  - format_tools_for_prompt(tools)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "an85hft6i79",
   "metadata": {},
   "source": [
    "# Supabase Database Logging\n",
    "Log every step, rollout, and training update to Supabase for analysis. Raw data storage - you control the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cokpa00zitv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SUPABASE DATABASE LOGGING\n",
    "# =============================================================================\n",
    "\n",
    "# Install supabase if needed\n",
    "# !pip install -q supabase\n",
    "\n",
    "try:\n",
    "    from supabase import create_client, Client\n",
    "    SUPABASE_AVAILABLE = True\n",
    "    print(\"✓ Supabase client available\")\n",
    "except ImportError:\n",
    "    SUPABASE_AVAILABLE = False\n",
    "    print(\"⚠ Supabase not installed. Run: pip install supabase\")\n",
    "\n",
    "\n",
    "class SupabaseLogger:\n",
    "    \"\"\"\n",
    "    Log RL training data to Supabase.\n",
    "    \n",
    "    Collects data in batches for efficient writes.\n",
    "    All data stored raw - you control analysis later.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        supabase_url: str,\n",
    "        supabase_key: str,\n",
    "        experiment_id: Optional[str] = None,\n",
    "        batch_size: int = 100,\n",
    "        auto_flush: bool = True,\n",
    "    ):\n",
    "        if not SUPABASE_AVAILABLE:\n",
    "            raise ImportError(\"Supabase not installed. Run: pip install supabase\")\n",
    "        \n",
    "        self.client: Client = create_client(supabase_url, supabase_key)\n",
    "        self.experiment_id = experiment_id or f\"exp_{int(time.time())}\"\n",
    "        self.batch_size = batch_size\n",
    "        self.auto_flush = auto_flush\n",
    "        \n",
    "        # Batch buffers\n",
    "        self.step_buffer: List[Dict] = []\n",
    "        self.rollout_buffer: List[Dict] = []\n",
    "        self.training_buffer: List[Dict] = []\n",
    "        \n",
    "        print(f\"✓ SupabaseLogger initialized\")\n",
    "        print(f\"  Experiment ID: {self.experiment_id}\")\n",
    "    \n",
    "    def log_step(\n",
    "        self,\n",
    "        rollout_id: str,\n",
    "        step_num: int,\n",
    "        prompt: str,\n",
    "        response: str,\n",
    "        thinking: Optional[str] = None,\n",
    "        reward: Optional[np.ndarray] = None,\n",
    "        tool_calls: Optional[List[Dict]] = None,\n",
    "        metadata: Optional[Dict] = None,\n",
    "    ):\n",
    "        \"\"\"Log a single step in a rollout\"\"\"\n",
    "        step_data = {\n",
    "            \"experiment_id\": self.experiment_id,\n",
    "            \"rollout_id\": rollout_id,\n",
    "            \"step_num\": step_num,\n",
    "            \"timestamp\": time.time(),\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response,\n",
    "            \"thinking\": thinking,\n",
    "            \"reward\": reward.tolist() if isinstance(reward, np.ndarray) else reward,\n",
    "            \"tool_calls\": json.dumps(tool_calls) if tool_calls else None,\n",
    "            \"metadata\": json.dumps(metadata) if metadata else None,\n",
    "        }\n",
    "        \n",
    "        self.step_buffer.append(step_data)\n",
    "        \n",
    "        if self.auto_flush and len(self.step_buffer) >= self.batch_size:\n",
    "            self.flush_steps()\n",
    "    \n",
    "    def log_rollout(\n",
    "        self,\n",
    "        state: RolloutState,\n",
    "        final_reward: Optional[np.ndarray] = None,\n",
    "    ):\n",
    "        \"\"\"Log a complete rollout\"\"\"\n",
    "        rollout_data = {\n",
    "            \"experiment_id\": self.experiment_id,\n",
    "            \"rollout_id\": state.rollout_id,\n",
    "            \"timestamp\": time.time(),\n",
    "            \"initial_prompt\": state.initial_prompt,\n",
    "            \"num_steps\": state.step_count,\n",
    "            \"terminated\": state.terminated,\n",
    "            \"termination_reason\": state.termination_reason,\n",
    "            \"cumulative_reward\": state.cumulative_reward.tolist(),\n",
    "            \"final_reward\": final_reward.tolist() if isinstance(final_reward, np.ndarray) else final_reward,\n",
    "            \"verification_budget_spent\": state.verification_budget_spent,\n",
    "            \"metadata\": json.dumps(state.metadata) if state.metadata else None,\n",
    "        }\n",
    "        \n",
    "        self.rollout_buffer.append(rollout_data)\n",
    "        \n",
    "        # Also log each step\n",
    "        for i, step in enumerate(state.trajectory):\n",
    "            self.log_step(\n",
    "                rollout_id=state.rollout_id,\n",
    "                step_num=i,\n",
    "                prompt=step.prompt,\n",
    "                response=step.response.text,\n",
    "                thinking=step.metadata.get(\"thinking\") if step.metadata else None,\n",
    "                reward=step.reward,\n",
    "                tool_calls=step.metadata.get(\"tool_calls\") if step.metadata else None,\n",
    "                metadata=step.metadata,\n",
    "            )\n",
    "        \n",
    "        if self.auto_flush and len(self.rollout_buffer) >= self.batch_size:\n",
    "            self.flush_rollouts()\n",
    "    \n",
    "    def log_training_step(\n",
    "        self,\n",
    "        step_num: int,\n",
    "        loss: float,\n",
    "        metrics: Dict[str, float],\n",
    "        learning_rate: Optional[float] = None,\n",
    "        batch_size: Optional[int] = None,\n",
    "    ):\n",
    "        \"\"\"Log a training update\"\"\"\n",
    "        training_data = {\n",
    "            \"experiment_id\": self.experiment_id,\n",
    "            \"step_num\": step_num,\n",
    "            \"timestamp\": time.time(),\n",
    "            \"loss\": loss,\n",
    "            \"metrics\": json.dumps(metrics),\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"batch_size\": batch_size,\n",
    "        }\n",
    "        \n",
    "        self.training_buffer.append(training_data)\n",
    "        \n",
    "        if self.auto_flush and len(self.training_buffer) >= self.batch_size:\n",
    "            self.flush_training()\n",
    "    \n",
    "    def flush_steps(self):\n",
    "        \"\"\"Flush step buffer to database\"\"\"\n",
    "        if not self.step_buffer:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            self.client.table(\"rl_steps\").insert(self.step_buffer).execute()\n",
    "            print(f\"✓ Flushed {len(self.step_buffer)} steps to Supabase\")\n",
    "            self.step_buffer.clear()\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Error flushing steps: {e}\")\n",
    "    \n",
    "    def flush_rollouts(self):\n",
    "        \"\"\"Flush rollout buffer to database\"\"\"\n",
    "        if not self.rollout_buffer:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            self.client.table(\"rl_rollouts\").insert(self.rollout_buffer).execute()\n",
    "            print(f\"✓ Flushed {len(self.rollout_buffer)} rollouts to Supabase\")\n",
    "            self.rollout_buffer.clear()\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Error flushing rollouts: {e}\")\n",
    "    \n",
    "    def flush_training(self):\n",
    "        \"\"\"Flush training buffer to database\"\"\"\n",
    "        if not self.training_buffer:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            self.client.table(\"rl_training\").insert(self.training_buffer).execute()\n",
    "            print(f\"✓ Flushed {len(self.training_buffer)} training steps to Supabase\")\n",
    "            self.training_buffer.clear()\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Error flushing training: {e}\")\n",
    "    \n",
    "    def flush_all(self):\n",
    "        \"\"\"Flush all buffers\"\"\"\n",
    "        self.flush_steps()\n",
    "        self.flush_rollouts()\n",
    "        self.flush_training()\n",
    "    \n",
    "    def log_experiment_config(self, config: Dict[str, Any]):\n",
    "        \"\"\"Log experiment configuration\"\"\"\n",
    "        try:\n",
    "            self.client.table(\"rl_experiments\").insert({\n",
    "                \"experiment_id\": self.experiment_id,\n",
    "                \"timestamp\": time.time(),\n",
    "                \"config\": json.dumps(config),\n",
    "            }).execute()\n",
    "            print(f\"✓ Logged experiment config\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Error logging config: {e}\")\n",
    "    \n",
    "    def get_experiment_data(self, table: str = \"rl_rollouts\") -> List[Dict]:\n",
    "        \"\"\"Retrieve experiment data from Supabase\"\"\"\n",
    "        try:\n",
    "            response = self.client.table(table).select(\"*\").eq(\n",
    "                \"experiment_id\", self.experiment_id\n",
    "            ).execute()\n",
    "            return response.data\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Error retrieving data: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "def create_supabase_tables_sql():\n",
    "    \"\"\"\n",
    "    SQL to create tables in Supabase.\n",
    "    Run this in your Supabase SQL editor.\n",
    "    \"\"\"\n",
    "    return '''\n",
    "-- Experiments table\n",
    "CREATE TABLE IF NOT EXISTS rl_experiments (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    experiment_id TEXT NOT NULL,\n",
    "    timestamp FLOAT NOT NULL,\n",
    "    config JSONB,\n",
    "    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n",
    ");\n",
    "\n",
    "-- Rollouts table\n",
    "CREATE TABLE IF NOT EXISTS rl_rollouts (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    experiment_id TEXT NOT NULL,\n",
    "    rollout_id TEXT NOT NULL,\n",
    "    timestamp FLOAT NOT NULL,\n",
    "    initial_prompt TEXT,\n",
    "    num_steps INTEGER,\n",
    "    terminated BOOLEAN,\n",
    "    termination_reason TEXT,\n",
    "    cumulative_reward JSONB,\n",
    "    final_reward JSONB,\n",
    "    verification_budget_spent FLOAT,\n",
    "    metadata JSONB,\n",
    "    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n",
    ");\n",
    "\n",
    "-- Steps table\n",
    "CREATE TABLE IF NOT EXISTS rl_steps (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    experiment_id TEXT NOT NULL,\n",
    "    rollout_id TEXT NOT NULL,\n",
    "    step_num INTEGER NOT NULL,\n",
    "    timestamp FLOAT NOT NULL,\n",
    "    prompt TEXT,\n",
    "    response TEXT,\n",
    "    thinking TEXT,\n",
    "    reward JSONB,\n",
    "    tool_calls JSONB,\n",
    "    metadata JSONB,\n",
    "    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n",
    ");\n",
    "\n",
    "-- Training updates table\n",
    "CREATE TABLE IF NOT EXISTS rl_training (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    experiment_id TEXT NOT NULL,\n",
    "    step_num INTEGER NOT NULL,\n",
    "    timestamp FLOAT NOT NULL,\n",
    "    loss FLOAT,\n",
    "    metrics JSONB,\n",
    "    learning_rate FLOAT,\n",
    "    batch_size INTEGER,\n",
    "    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n",
    ");\n",
    "\n",
    "-- Indexes for faster queries\n",
    "CREATE INDEX IF NOT EXISTS idx_rollouts_experiment ON rl_rollouts(experiment_id);\n",
    "CREATE INDEX IF NOT EXISTS idx_steps_rollout ON rl_steps(rollout_id);\n",
    "CREATE INDEX IF NOT EXISTS idx_training_experiment ON rl_training(experiment_id);\n",
    "'''\n",
    "\n",
    "\n",
    "class LoggingRewardWrapper(RewardFunction):\n",
    "    \"\"\"\n",
    "    Wrapper that logs rewards to Supabase.\n",
    "    Compose with your actual reward function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        reward_fn: RewardFunction,\n",
    "        logger: SupabaseLogger,\n",
    "    ):\n",
    "        self.reward_fn = reward_fn\n",
    "        self.logger = logger\n",
    "    \n",
    "    def compute_reward(self, state: RolloutState, step: StepData) -> np.ndarray:\n",
    "        reward = self.reward_fn.compute_reward(state, step)\n",
    "        \n",
    "        # Log step with reward\n",
    "        self.logger.log_step(\n",
    "            rollout_id=state.rollout_id,\n",
    "            step_num=state.step_count - 1,  # Already added\n",
    "            prompt=step.prompt,\n",
    "            response=step.response.text,\n",
    "            thinking=step.metadata.get(\"thinking\") if step.metadata else None,\n",
    "            reward=reward,\n",
    "            tool_calls=step.metadata.get(\"tool_calls\") if step.metadata else None,\n",
    "            metadata=step.metadata,\n",
    "        )\n",
    "        \n",
    "        return reward\n",
    "\n",
    "\n",
    "def example_supabase_usage():\n",
    "    \"\"\"Example: Using Supabase logger in training loop\"\"\"\n",
    "    \n",
    "    # Setup (replace with your credentials)\n",
    "    SUPABASE_URL = \"https://your-project.supabase.co\"\n",
    "    SUPABASE_KEY = \"your-anon-key\"\n",
    "    \n",
    "    # Create logger\n",
    "    logger = SupabaseLogger(\n",
    "        supabase_url=SUPABASE_URL,\n",
    "        supabase_key=SUPABASE_KEY,\n",
    "        experiment_id=\"pareto_rl_exp_001\",\n",
    "        batch_size=50,  # Flush every 50 items\n",
    "    )\n",
    "    \n",
    "    # Log experiment config\n",
    "    logger.log_experiment_config({\n",
    "        \"model\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "        \"num_objectives\": 2,\n",
    "        \"max_steps\": 20,\n",
    "        \"learning_rate\": 1e-6,\n",
    "    })\n",
    "    \n",
    "    # Wrap your reward function with logging\n",
    "    my_reward = MyTaskReward(num_objectives=2)\n",
    "    logged_reward = LoggingRewardWrapper(my_reward, logger)\n",
    "    \n",
    "    # Use in training loop\n",
    "    # rollout = collect_single_rollout(..., reward_function=logged_reward)\n",
    "    # logger.log_rollout(rollout)\n",
    "    \n",
    "    # After training step\n",
    "    # logger.log_training_step(step_num, loss, metrics)\n",
    "    \n",
    "    # Don't forget to flush at end\n",
    "    # logger.flush_all()\n",
    "    \n",
    "    return logger\n",
    "\n",
    "\n",
    "print(\"✓ Supabase logging defined\")\n",
    "print(\"  - SupabaseLogger for batched writes\")\n",
    "print(\"  - log_step(), log_rollout(), log_training_step()\")\n",
    "print(\"  - LoggingRewardWrapper to auto-log rewards\")\n",
    "print(\"  - create_supabase_tables_sql() for schema\")\n",
    "print(\"\\\\nSetup:\")\n",
    "print(\"  1. Create tables in Supabase using create_supabase_tables_sql()\")\n",
    "print(\"  2. Get your SUPABASE_URL and SUPABASE_KEY\")\n",
    "print(\"  3. Create SupabaseLogger(url, key)\")\n",
    "print(\"  4. Wrap reward function or call log methods manually\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
