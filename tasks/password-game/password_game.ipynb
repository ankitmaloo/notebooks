{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Password Game RL Training with VERL\n\nTrain Qwen3-0.6B to solve the Password Game using PPO.\n\n**Rules**: 9 progressive password rules  \n**Reward**: +1 per rule passed, -0.1 per character"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q flash-attn --no-build-isolation\n",
    "!pip install -q transformers accelerate datasets tokenizers wandb tqdm numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592f5ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install google-genai openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random, time, re\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "assert DEVICE == \"cuda\", \"GPU required\""
   ]
  },
  {
   "id": "b332e500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "id": "fb43885d",
   "metadata": {},
   "source": [
    "# Gemini for eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass Config:\n    # Model\n    model_name: str = \"Qwen/Qwen3-0.6B\"\n    precision: str = \"bfloat16\"\n    use_flash_attn: bool = True\n    \n    # Training\n    num_epochs: int = 3\n    num_steps_per_epoch: int = 100\n    batch_size: int = 4\n    samples_per_prompt: int = 4\n    learning_rate: float = 1e-6\n    weight_decay: float = 0.01\n    max_grad_norm: float = 1.0\n    warmup_steps: int = 50\n    \n    # PPO\n    ppo_epochs: int = 4\n    clip_range: float = 0.2\n    value_loss_coef: float = 0.1\n    kl_coef: float = 0.05\n    gamma: float = 0.99\n    gae_lambda: float = 0.95\n    normalize_advantages: bool = True\n    \n    # Generation\n    max_prompt_length: int = 1024\n    max_new_tokens: int = 256\n    temperature: float = 0.8\n    top_p: float = 0.9\n    top_k: int = 50\n    \n    # Password Game\n    num_rules: int = 9\n    reward_per_rule: float = 1.0\n    length_penalty: float = 0.1\n    \n    # Data\n    num_train_samples: int = 1000\n    num_val_samples: int = 200\n    \n    # Logging\n    wandb_project: str = \"password-game-rl\"\n    wandb_run_name: Optional[str] = None\n    log_interval: int = 10\n    eval_interval: int = 50\n    save_interval: int = 100\n    output_dir: str = f\"./password_game_{int(time.time())}\"\n    seed: int = 42\n    \n    def __post_init__(self):\n        if self.wandb_run_name is None:\n            self.wandb_run_name = f\"password_ppo_{int(time.time())}\"\n        os.makedirs(self.output_dir, exist_ok=True)\n\nconfig = Config()\nprint(f\"Model: {config.model_name}\")\nprint(f\"Batch: {config.batch_size} x {config.samples_per_prompt} = {config.batch_size * config.samples_per_prompt}\")\nprint(f\"Output: {config.output_dir}\")\n\nwith open(os.path.join(config.output_dir, \"config.json\"), \"w\") as f:\n    json.dump(asdict(config), f, indent=2)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "id": "a6cb7abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = \"\" #add gemini api key\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.bfloat16 if config.precision == \"bfloat16\" else torch.float32\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "print(f\"Tokenizer: {len(tokenizer)} tokens\")"
   ]
  },
  {
   "id": "a9abe479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "system_instruction = \"\"\"\n",
    "  You are an expert at creating password based on the given rules. You will get the instructions of the game at teh start, follow those and play the game till you can. \n",
    "\"\"\"\n",
    "\n",
    "chat_config = types.GenerateContentConfig(\n",
    "    system_instruction=system_instruction,\n",
    "    tools=[{\"google_search\": {}}],\n",
    "    thinking_config=types.ThinkingConfig(\n",
    "      include_thoughts=True\n",
    "    )\n",
    ")\n",
    "\n",
    "chat = client.chats.create(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    config=chat_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\" if config.use_flash_attn else \"eager\"\n",
    ")\n",
    "policy_model.config.use_cache = False\n",
    "print(f\"Policy: {sum(p.numel() for p in policy_model.parameters())/1e9:.2f}B params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\" if config.use_flash_attn else \"eager\"\n",
    ")\n",
    "reference_model.eval()\n",
    "for param in reference_model.parameters():\n",
    "    param.requires_grad = False\n",
    "print(\"Reference: frozen\")"
   ]
  },
  {
   "id": "9ddbe43b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown"
  },
  {
   "id": "95371394",
   "metadata": {},
   "source": [
    "# Openai utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueHead(nn.Module):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "        nn.init.orthogonal_(self.linear.weight, gain=0.01)\n",
    "        nn.init.constant_(self.linear.bias, 0.0)\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        return self.linear(hidden_states)\n",
    "\n",
    "value_head = ValueHead(policy_model.config.hidden_size).to(DEVICE).to(dtype)\n",
    "print(f\"Value head: {policy_model.config.hidden_size}\")"
   ]
  },
  {
   "id": "7afff10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"\"\n",
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "SCHEMA = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"strict\":True,\n",
    "    \"name\": \"AnswerAndDate\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"additionalProperties\": False,\n",
    "            \"properties\": {\n",
    "                \"answer\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The concise answer or summary.\"\n",
    "                },\n",
    "                \"date\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"format\": \"date\",\n",
    "                    \"description\": \"Today's date in YYYY-MM-DD.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"answer\", \"date\"]\n",
    "        }\n",
    "}\n",
    "\n",
    "def ask_with_search(prompt: str,  model: str = \"gpt-4.1\", temperature: float = 0.1) -> dict:\n",
    "    \"\"\"\n",
    "    Generic function that calls OpenAI Responses API with web search enabled.\n",
    "\n",
    "    Args:\n",
    "        prompt: The search query/prompt\n",
    "        schema: The JSON schema for response format\n",
    "        model: OpenAI model to use\n",
    "        temperature: Temperature for response generation\n",
    "\n",
    "    Returns:\n",
    "        dict: Parsed JSON response based on provided schema\n",
    "    \"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        input=prompt,\n",
    "        text={\n",
    "            \"format\": SCHEMA\n",
    "        },\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"web_search\",\n",
    "                \"user_location\": {\n",
    "                    \"type\": \"approximate\"\n",
    "                },\n",
    "                \"search_context_size\": \"medium\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        top_p=1,\n",
    "        store=True,\n",
    "        include=[\"web_search_call.action.sources\"]\n",
    "    )\n",
    "    return json.loads(response.output_text)\n",
    "\n",
    "def utils_get_wordle() -> dict:\n",
    "    \"\"\"\n",
    "    Gets today's Wordle answer using web search.\n",
    "    Returns: { \"answer\": str, \"date\": \"YYYY-MM-DD\" }\n",
    "    \"\"\"\n",
    "    prompt = \"What is today's wordle answer? In your answer only include the word, no other text\"\n",
    "    return ask_with_search(prompt,temperature=1)\n",
    "\n",
    "def utils_get_emoji() -> dict:\n",
    "    \"\"\"\n",
    "    Gets today's actual moon phase emoji using web search.\n",
    "    Returns: { \"emoji\": str, \"date\": \"YYYY-MM-DD\" }\n",
    "    \"\"\"\n",
    "    prompt = \"What is today's current moon phase? Return only the appropriate emoji: \ud83c\udf11 \ud83c\udf12 \ud83c\udf13 \ud83c\udf14 \ud83c\udf15 \ud83c\udf16 \ud83c\udf17 \ud83c\udf18\"\n",
    "    return ask_with_search(prompt, temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Password Game Environment"
   ]
  },
  {
   "id": "5a1521f0",
   "metadata": {},
   "source": [
    "# game class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PASSWORD_RULES = [\n"
   ]
  },
  {
   "id": "e2bdbaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import re\n",
    "import requests\n",
    "from typing import List, Dict, Optional, Set\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def create_captcha():\n",
    "    \"\"\"Generates a random five-character combination of 3 letters and 2 numbers.\"\"\"\n",
    "    letters = [random.choice(string.ascii_letters) for _ in range(3)]\n",
    "    numbers = [random.choice(string.digits) for _ in range(2)]\n",
    "    captcha_list = letters + numbers\n",
    "    random.shuffle(captcha_list)\n",
    "    captcha = ''.join(captcha_list)\n",
    "    return captcha.lower()\n",
    "\n",
    "countries = [\n",
    "    \"Albania\", \"Algeria\", \"Andorra\", \"Armenia\", \"Austria\", \"Bahrain\", \"Belarus\",\n",
    "    \"Belgium\", \"Bolivia\", \"Burundi\", \"Comoros\", \"Croatia\", \"Czechia\", \"Denmark\",\n",
    "    \"Ecuador\", \"Eritrea\", \"Estonia\", \"Finland\", \"Georgia\", \"Germany\", \"Grenada\",\n",
    "    \"Hungary\", \"Iceland\", \"Ireland\", \"Jamaica\", \"Lebanon\", \"Lesotho\", \"Liberia\",\n",
    "    \"Moldova\", \"Morocco\", \"Myanmar\", \"Namibia\", \"Nigeria\", \"Romania\", \"Senegal\",\n",
    "    \"Somalia\", \"Tunisia\", \"Ukraine\", \"Uruguay\", \"Vanuatu\", \"Vietnam\"\n",
    "]\n",
    "\n",
    "def random_country():\n",
    "    \"\"\"Gives options of random seven letter countries for the game.\"\"\"\n",
    "    return random.choice(countries)\n",
    "\n",
    "def get_wordle_answer():\n",
    "    \"\"\"Fetch today's Wordle answer from web.\"\"\"\n",
    "    try:\n",
    "        response = utils_get_wordle()\n",
    "        if response and \"answer\" in response:\n",
    "            return response[\"answer\"].lower()\n",
    "    except:\n",
    "        pass\n",
    "    return \"crane\"  # Fallback answer\n",
    "\n",
    "def get_current_moon_phase():\n",
    "    \"\"\"Get current moon phase emoji using OpenAI + search.\"\"\"\n",
    "    try:\n",
    "        #result = utils_get_emoji()\n",
    "        #return result[\"answer\"]\n",
    "        return \"\ud83c\udf15\"\n",
    "    except:\n",
    "        raise Exception(\"Cannot determine real moon phase - API unavailable\")\n",
    "\n",
    "rules = [\n",
    "    \"Your password must be at least 5 characters.\",\n",
    "    \"Your password must include a number.\",\n",
    "    \"Your password must include an uppercase letter.\",\n",
    "    \"Your password must include a special character.\",\n",
    "    \"The digits in your password must add up to 25.\",\n",
    "    \"Your password must include a month of the year.\",\n",
    "    \"Your password must include a roman numeral.\",\n",
    "    \"Your password must include one of our sponsors: (Pepsi, Starbucks, Shell)\",\n",
    "    \"The roman numerals in your password should multiply to 35.\",\n",
    "]\n",
    "\n",
    "INSTRUCTIONS = \"\"\"You are playing a password game. Create a password that satisfies ALL the given rules.\n",
    "Return ONLY the password string, nothing else.\"\"\"\n",
    "\n",
    "def check_rule(password: str, rule_idx: int) -> bool:\n",
    "    pwd = password\n",
    "    \n",
    "    if rule_idx == 0:\n",
    "        return len(pwd) >= 5\n",
    "    elif rule_idx == 1:\n",
    "        return any(c.isdigit() for c in pwd)\n",
    "    elif rule_idx == 2:\n",
    "        return any(c.isupper() for c in pwd)\n",
    "    elif rule_idx == 3:\n",
    "        return any(not c.isalnum() for c in pwd)\n",
    "    elif rule_idx == 4:\n",
    "        return sum(int(c) for c in pwd if c.isdigit()) == 25\n",
    "    elif rule_idx == 5:\n",
    "        months = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december']\n",
    "        return any(m in pwd.lower() for m in months)\n",
    "    elif rule_idx == 6:\n",
    "        return bool(re.search(r'[IVXLCDM]+', pwd))\n",
    "    elif rule_idx == 7:\n",
    "        sponsors = ['pepsi', 'starbucks', 'shell']\n",
    "        return any(s in pwd.lower() for s in sponsors)\n",
    "    elif rule_idx == 8:\n",
    "        romans = re.findall(r'[IVXLCDM]+', pwd)\n",
    "        if not romans:\n",
    "            return False\n",
    "        roman_vals = {'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000}\n",
    "        product = 1\n",
    "        for r in romans:\n",
    "            val = 0\n",
    "            prev = 0\n",
    "            for c in reversed(r):\n",
    "                v = roman_vals.get(c, 0)\n",
    "                if v < prev:\n",
    "                    val -= v\n",
    "                else:\n",
    "                    val += v\n",
    "                prev = v\n",
    "            if val > 0:\n",
    "                product *= val\n",
    "        return product == 35\n",
    "    return False\n",
    "\n",
    "def compute_reward(password: str, num_active_rules: int) -> float:\n",
    "    passing = sum(check_rule(password, i) for i in range(num_active_rules))\n",
    "    rule_score = passing * config.reward_per_rule\n",
    "    length_penalty = len(password) * config.length_penalty\n",
    "    return rule_score - length_penalty\n",
    "\n",
    "def format_prompt(num_active_rules: int) -> str:\n",
    "    rules_text = \"\\n\".join([f\"{i+1}. {PASSWORD_RULES[i]}\" for i in range(num_active_rules)])\n",
    "    return f\"{INSTRUCTIONS}\\n\\nRules:\\n{rules_text}\\n\\nPassword:\"\n",
    "\n",
    "print(f\"Loaded {len(PASSWORD_RULES)} rules\")\n",
    "print(f\"Example prompt:\\n{format_prompt(3)[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PasswordDataset(Dataset):\n",
    "    def __init__(self, num_samples: int, max_rules: int):\n",
    "        self.prompts = []\n",
    "        self.num_rules = []\n",
    "        for _ in range(num_samples):\n",
    "            n = random.randint(3, min(max_rules, len(PASSWORD_RULES)))\n",
    "            self.prompts.append(format_prompt(n))\n",
    "            self.num_rules.append(n)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {'prompt': self.prompts[idx], 'num_rules': self.num_rules[idx]}\n",
    "\n",
    "train_dataset = PasswordDataset(config.num_train_samples, 9)\n",
    "val_dataset = PasswordDataset(config.num_val_samples, 9)\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"Your password must include one of our sponsors: (Pepsi, Starbucks, Shell) - case insensitive\",\n",
    "    \"The roman numerals in your password should multiply to 35.\",\n",
    "    \"Your password must include this CAPTCHA: {captcha}\",\n",
    "    \"Your password must include today's Wordle answer.\",\n",
    "    \"Your password must include a two letter symbol from the periodic table.\",\n",
    "    \"Your password must include the current phase of the moon as an emoji\",\n",
    "    \"Your password must include the name of this country: {country}\",\n",
    "    \"Your password must include a leap year.\",\n",
    "    \"\ud83e\udd5a \u2190 This is my chicken Paul. He hasn't hatched yet, please put him in your password and keep him safe.\",\n",
    "    \"The elements in your password must have atomic numbers that add up to 200.\",\n",
    "    \"Your password is not strong enough \ud83c\udfcb\ufe0f\u200d\u2642\ufe0f\",\n",
    "    \"Your password must contain one of the following affirmations: (I am loved, I am worthy, I am enough)\",\n",
    "    \"Paul has hatched! Please don't forget to feed him, he eats three \ud83d\udc1b.\",\n",
    "    \"A sacrifice must be made. Pick two letters that you will no longer be able to use.\",\n",
    "    \"Your password must include this color in hex. green\",\n",
    "    \"Your password must include the length of your password.\",\n",
    "    \"The length of your password must be a prime number.\",\n",
    "    \"Uh lets skip this one\",\n",
    "    \"Your password must contain three consecutive letters that appear in alphabetical order (like 'abc', 'hij', 'xyz')\"\n",
    "]\n",
    "\n",
    "instructions = \"\"\"\n",
    "You are playing a password game. After each step, you will be given a rule and the password has to comply to that rule and all the previous rules.\n",
    "Keep thinking and only submit as final answer if you are confident it's correct.\n",
    "If you can't get it, ask for a hint.\n",
    "You are allowed to use search.\n",
    "Goal is minimum length of final password.\n",
    "\n",
    "GOAL: When given a rule, in your answer, only give me the password string you have that satisfies the rule.\n",
    "\n",
    "Some clarifications as you may need them:\n",
    "1. Only capital letters count towards Roman Numbers - if it's a valid roman number.\n",
    "2. For a substring to count as an element, first (or only) letter of the symbol has to be capitalized.\n",
    "3. For other answers, you can play around with capitalization or not to satisfy the given rules. Checks are case insensitive othersie - not for roman or elements.\n",
    "4. If you have a conflict, state that and say you want to end the game.\n",
    "\n",
    "RESPONSE GUIDELINES:\n",
    "respond in markdown in following format: \n",
    "### Password\n",
    "{password}\n",
    "### Giveup\n",
    "{True if you want to give up, false otherwise}\n",
    "\"\"\"\n",
    "\n",
    "class PasswordGame:\n",
    "    def __init__(self):\n",
    "        self.current_rule = 0\n",
    "        self.game_active = True\n",
    "        self.captcha = create_captcha()\n",
    "        self.country = random_country()\n",
    "        self.wordle_answer = get_wordle_answer()\n",
    "        self.moon_phase = get_current_moon_phase()\n",
    "        self.password_history = []\n",
    "\n",
    "\n",
    "    def get_current_rule(self) -> Optional[str]:\n",
    "        if self.current_rule >= len(rules) or not self.game_active:\n",
    "            return None\n",
    "        rule = rules[self.current_rule]\n",
    "        if \"{captcha}\" in rule:\n",
    "            return rule.format(captcha=self.captcha)\n",
    "        elif \"{country}\" in rule:\n",
    "            return rule.format(country=self.country)\n",
    "        return rule\n",
    "\n",
    "    def get_all_rules_up_to_current(self) -> List[str]:\n",
    "        formatted_rules = []\n",
    "        for i, rule in enumerate(rules[:self.current_rule + 1]):\n",
    "            if \"{captcha}\" in rule:\n",
    "                formatted_rules.append(rule.format(captcha=self.captcha))\n",
    "            elif \"{country}\" in rule:\n",
    "                formatted_rules.append(rule.format(country=self.country))\n",
    "            else:\n",
    "                formatted_rules.append(rule)\n",
    "        return formatted_rules\n",
    "\n",
    "    def advance_rule(self):\n",
    "        self.current_rule += 1\n",
    "        if self.current_rule >= len(rules):\n",
    "            self.game_active = False\n",
    "\n",
    "    def end_game(self):\n",
    "        self.game_active = False\n",
    "\n",
    "    def calculate_reward(self, password: str) -> float:\n",
    "        \"\"\"Calculate reward: +1 per passing rule, -0.1 per character.\"\"\"\n",
    "        satisfied_rules = 0\n",
    "\n",
    "        # Check all rules up to current rule (inclusive when game ends)\n",
    "        rule_count = self.current_rule if self.game_active else len(rules)\n",
    "\n",
    "        for i in range(rule_count):\n",
    "            if self._check_rule(password, i):\n",
    "                satisfied_rules += 1\n",
    "\n",
    "        # +1 per passing rule, -0.1 per character\n",
    "        rule_score = satisfied_rules\n",
    "        length_penalty = len(password) * 0.1\n",
    "        total_reward = rule_score - length_penalty\n",
    "\n",
    "        return round(total_reward, 1)\n",
    "\n",
    "    def get_rule_feedback(self, password: str) -> Dict:\n",
    "        \"\"\"Get detailed feedback on which rules pass/fail.\"\"\"\n",
    "        feedback = {\n",
    "            \"password\": password,\n",
    "            \"length\": len(password),\n",
    "            \"rules_checked\": [],\n",
    "            \"total_passing\": 0,\n",
    "            \"reward\": 0.0\n",
    "        }\n",
    "\n",
    "        # For feedback, include current rule if game is active\n",
    "        rule_count = (self.current_rule + 1) if self.game_active else len(rules)\n",
    "\n",
    "        for i in range(rule_count):\n",
    "            passes = self._check_rule(password, i)\n",
    "            rule_text = rules[i]\n",
    "            if \"{captcha}\" in rule_text:\n",
    "                rule_text = rule_text.format(captcha=self.captcha)\n",
    "            elif \"{country}\" in rule_text:\n",
    "                rule_text = rule_text.format(country=self.country)\n",
    "\n",
    "            feedback[\"rules_checked\"].append({\n",
    "                \"rule_index\": i,\n",
    "                \"rule_text\": rule_text,\n",
    "                \"passes\": passes\n",
    "            })\n",
    "            if passes:\n",
    "                feedback[\"total_passing\"] += 1\n",
    "\n",
    "        feedback[\"reward\"] = self.calculate_reward(password)\n",
    "        return feedback\n",
    "\n",
    "    def _check_rule(self, password: str, rule_index: int) -> bool:\n",
    "        \"\"\"Comprehensive rule checking for all password rules.\"\"\"\n",
    "        if rule_index == 0:  # At least 5 characters\n",
    "            return len(password) >= 5\n",
    "\n",
    "        elif rule_index == 1:  # Include a number\n",
    "            return any(c.isdigit() for c in password)\n",
    "\n",
    "        elif rule_index == 2:  # Include uppercase letter\n",
    "            return any(c.isupper() for c in password)\n",
    "\n",
    "        elif rule_index == 3:  # Include special character\n",
    "            return any(not c.isalnum() for c in password)\n",
    "\n",
    "        elif rule_index == 4:  # Digits sum to 25\n",
    "            digit_sum = sum(int(c) for c in password if c.isdigit())\n",
    "            return digit_sum == 25\n",
    "\n",
    "        elif rule_index == 5:  # Include month\n",
    "            months = ['january', 'february', 'march', 'april', 'may', 'june',\n",
    "                     'july', 'august', 'september', 'october', 'november', 'december']\n",
    "            return any(month in password.lower() for month in months)\n",
    "\n",
    "        elif rule_index == 6:  # Include roman numeral\n",
    "            roman_pattern = r'[IVXLCDM]+'\n",
    "            return bool(re.search(roman_pattern, password))\n",
    "\n",
    "        elif rule_index == 7:  # Include sponsor\n",
    "            sponsors = ['pepsi', 'starbucks', 'shell']\n",
    "            return any(sponsor in password.lower() for sponsor in sponsors)\n",
    "\n",
    "        elif rule_index == 8:  # Roman numerals multiply to 35\n",
    "            return self._check_roman_multiply(password, 35)\n",
    "\n",
    "        elif rule_index == 9:  # Include CAPTCHA\n",
    "            return self.captcha in password.lower()\n",
    "\n",
    "        elif rule_index == 10:  # Include Wordle answer\n",
    "            return self.wordle_answer.lower() in password.lower()\n",
    "\n",
    "        elif rule_index == 11:  # Include periodic element\n",
    "            return self._check_periodic_element(password)\n",
    "\n",
    "        elif rule_index == 12:  # Include moon phase emoji\n",
    "            return self.moon_phase in password\n",
    "\n",
    "        elif rule_index == 13:  # Include country (dynamic)\n",
    "            return self.country.lower() in password.lower()\n",
    "\n",
    "        elif rule_index == 14:  # Include leap year\n",
    "            return self._check_leap_year(password)\n",
    "\n",
    "        elif rule_index == 15:  # Include Paul (egg emoji)\n",
    "            return '\ud83e\udd5a' in password\n",
    "\n",
    "        elif rule_index == 16:  # Atomic numbers sum to 200\n",
    "            return self._check_atomic_sum(password, 200)\n",
    "\n",
    "        elif rule_index == 17:  # Not strong enough (always fails)\n",
    "            return False\n",
    "\n",
    "        elif rule_index == 18:  # Include affirmation\n",
    "            affirmations = ['i am loved', 'i am worthy', 'i am enough']\n",
    "            return any(affirmation in password.lower() for affirmation in affirmations)\n",
    "\n",
    "        elif rule_index == 19:  # Paul eats 3 bugs\n",
    "            return password.count('\ud83d\udc1b') == 3\n",
    "\n",
    "        elif rule_index == 20:  # Sacrifice two letters\n",
    "            # This would need state tracking - simplified for now\n",
    "            return True\n",
    "\n",
    "        elif rule_index == 21:  # Include green hex\n",
    "            green_hex_pattern = r'#00[89ab]000|#008000'\n",
    "            return bool(re.search(green_hex_pattern, password.lower()))\n",
    "\n",
    "        elif rule_index == 22:  # Include password length\n",
    "            return str(len(password)) in password\n",
    "\n",
    "        elif rule_index == 23:  # Length is prime\n",
    "            return self._is_prime(len(password))\n",
    "\n",
    "        elif rule_index == 24:  # Skip this one\n",
    "            return True\n",
    "\n",
    "        elif rule_index == 25:  # Include 3 consecutive chars    \n",
    "            # Normalize to lowercase for case-insensitive checking\n",
    "            pwd = password.lower()\n",
    "    \n",
    "            for i in range(len(pwd) - 2):\n",
    "                triplet = pwd[i:i+3]\n",
    "        \n",
    "                # Must be three letters\n",
    "                if triplet.isalpha():\n",
    "                    # Check if they're consecutive: a->b->c, etc.\n",
    "                    if ord(triplet[2]) - ord(triplet[0]) == 2:\n",
    "                        # Verify the middle character is exactly +1\n",
    "                        if ord(triplet[1]) - ord(triplet[0]) == 1:\n",
    "                            return True\n",
    "            \n",
    "            return False\n",
    "\n",
    "    def get_game_state(self) -> Dict:\n",
    "        return {\n",
    "            \"current_rule_index\": self.current_rule,\n",
    "            \"current_rule\": self.get_current_rule(),\n",
    "            \"all_rules\": self.get_all_rules_up_to_current(),\n",
    "            \"game_active\": self.game_active,\n",
    "            \"instructions\": instructions,\n",
    "            \"captcha\": self.captcha,\n",
    "            \"country\": self.country,\n",
    "            \"wordle_answer\": self.wordle_answer,\n",
    "            \"moon_phase\": self.moon_phase\n",
    "        }\n",
    "\n",
    "    def get_instructions(self) -> str:\n",
    "        return instructions\n",
    "\n",
    "    def get_minimal_game_state(self) -> Dict:\n",
    "        \"\"\"Return minimal game state, only exposing non-searchable values when needed.\"\"\"\n",
    "        state = {\n",
    "            \"current_rule_index\": self.current_rule,\n",
    "            \"current_rule\": self.get_current_rule(),\n",
    "            \"all_rules\": self.get_all_rules_up_to_current(),\n",
    "            \"game_active\": self.game_active\n",
    "        }\n",
    "\n",
    "        # Only expose captcha when rule 9 (index 9) is active or passed\n",
    "        if self.current_rule >= 9:\n",
    "            state[\"captcha\"] = self.captcha\n",
    "\n",
    "        # Only expose country when rule 13 (index 13) is active or passed\n",
    "        if self.current_rule >= 13:\n",
    "            state[\"country\"] = self.country\n",
    "\n",
    "        return state\n",
    "\n",
    "    def _check_roman_multiply(self, password: str, target: int) -> bool:\n",
    "        \"\"\"Check if roman numerals in password multiply to target.\"\"\"\n",
    "        roman_values = {'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000}\n",
    "        roman_pattern = r'[IVXLCDM]+'\n",
    "        romans = re.findall(roman_pattern, password)\n",
    "\n",
    "        if not romans:\n",
    "            return False\n",
    "\n",
    "        product = 1\n",
    "        for roman in romans:\n",
    "            value = self._roman_to_int(roman)\n",
    "            if value > 0:\n",
    "                product *= value\n",
    "\n",
    "        return product == target\n",
    "\n",
    "    def _roman_to_int(self, roman: str) -> int:\n",
    "        \"\"\"Convert roman numeral to integer.\"\"\"\n",
    "        roman_values = {'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000}\n",
    "        total = 0\n",
    "        prev_value = 0\n",
    "\n",
    "        for char in reversed(roman):\n",
    "            value = roman_values.get(char, 0)\n",
    "            if value < prev_value:\n",
    "                total -= value\n",
    "            else:\n",
    "                total += value\n",
    "            prev_value = value\n",
    "\n",
    "        return total\n",
    "\n",
    "    def _check_periodic_element(self, password: str) -> bool:\n",
    "        \"\"\"Check for periodic table elements (first letter capitalized).\"\"\"\n",
    "        elements = [\n",
    "            'He', 'Li', 'Be', 'Ne', 'Na', 'Mg', 'Al', 'Si', 'Cl', 'Ar', 'Ca', 'Sc', 'Ti', 'Cr', 'Mn', 'Fe', 'Co', 'Ni',\n",
    "            'Cu', 'Zn', 'Ga', 'Ge', 'As', 'Se', 'Br', 'Kr', 'Rb', 'Sr', 'Zr', 'Nb', 'Mo', 'Tc', 'Ru', 'Rh', 'Pd',\n",
    "            'Ag', 'Cd', 'In', 'Sn', 'Sb', 'Te', 'Xe', 'Cs', 'Ba', 'La', 'Ce', 'Pr', 'Nd', 'Pm', 'Sm', 'Eu', 'Gd',\n",
    "            'Tb', 'Dy', 'Ho', 'Er', 'Tm', 'Yb', 'Lu', 'Hf', 'Ta', 'Re', 'Os', 'Ir', 'Pt', 'Au', 'Hg', 'Tl', 'Pb',\n",
    "            'Bi', 'Po', 'At', 'Rn', 'Fr', 'Ra', 'Ac', 'Th', 'Pa', 'Np', 'Pu', 'Am', 'Cm', 'Bk', 'Cf', 'Es', 'Fm',\n",
    "            'Md', 'No', 'Lr', 'Rf', 'Db', 'Sg', 'Bh', 'Hs', 'Mt', 'Ds', 'Rg', 'Cn', 'Nh', 'Fl', 'Mc', 'Lv', 'Ts', 'Og'\n",
    "        ]\n",
    "        return any(element in password for element in elements)\n",
    "\n",
    "    def _check_leap_year(self, password: str) -> bool:\n",
    "        \"\"\"Check for leap years in password.\"\"\"\n",
    "        numbers = re.findall(r'\\d{4}', password)\n",
    "        for num_str in numbers:\n",
    "            year = int(num_str)\n",
    "            if self._is_leap_year(year):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _is_leap_year(self, year: int) -> bool:\n",
    "        \"\"\"Check if year is leap year.\"\"\"\n",
    "        return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
    "\n",
    "    def _check_atomic_sum(self, password: str, target: int) -> bool:\n",
    "        \"\"\"Check if atomic numbers of elements sum to target.\"\"\"\n",
    "        element_atomic = {\n",
    "            'H': 1, 'He': 2, 'Li': 3, 'Be': 4, 'B': 5, 'C': 6, 'N': 7, 'O': 8, 'F': 9, 'Ne': 10,\n",
    "            'Na': 11, 'Mg': 12, 'Al': 13, 'Si': 14, 'P': 15, 'S': 16, 'Cl': 17, 'Ar': 18, 'K': 19, 'Ca': 20,\n",
    "            'Sc': 21, 'Ti': 22, 'V': 23, 'Cr': 24, 'Mn': 25, 'Fe': 26, 'Co': 27, 'Ni': 28, 'Cu': 29, 'Zn': 30,\n",
    "            'Ga': 31, 'Ge': 32, 'As': 33, 'Se': 34, 'Br': 35, 'Kr': 36, 'Rb': 37, 'Sr': 38, 'Y': 39, 'Zr': 40,\n",
    "            'Nb': 41, 'Mo': 42, 'Tc': 43, 'Ru': 44, 'Rh': 45, 'Pd': 46, 'Ag': 47, 'Cd': 48, 'In': 49, 'Sn': 50,\n",
    "            'Sb': 51, 'Te': 52, 'I': 53, 'Xe': 54, 'Cs': 55, 'Ba': 56, 'La': 57, 'Ce': 58, 'Pr': 59, 'Nd': 60,\n",
    "            'Pm': 61, 'Sm': 62, 'Eu': 63, 'Gd': 64, 'Tb': 65, 'Dy': 66, 'Ho': 67, 'Er': 68, 'Tm': 69, 'Yb': 70,\n",
    "            'Lu': 71, 'Hf': 72, 'Ta': 73, 'W': 74, 'Re': 75, 'Os': 76, 'Ir': 77, 'Pt': 78, 'Au': 79, 'Hg': 80,\n",
    "            'Tl': 81, 'Pb': 82, 'Bi': 83, 'Po': 84, 'At': 85, 'Rn': 86, 'Fr': 87, 'Ra': 88, 'Ac': 89, 'Th': 90,\n",
    "            'Pa': 91, 'U': 92, 'Np': 93, 'Pu': 94, 'Am': 95, 'Cm': 96, 'Bk': 97, 'Cf': 98, 'Es': 99, 'Fm': 100,\n",
    "            'Md': 101, 'No': 102, 'Lr': 103, 'Rf': 104, 'Db': 105, 'Sg': 106, 'Bh': 107, 'Hs': 108, 'Mt': 109, 'Ds': 110,\n",
    "            'Rg': 111, 'Cn': 112, 'Nh': 113, 'Fl': 114, 'Mc': 115, 'Lv': 116, 'Ts': 117, 'Og': 118\n",
    "        }\n",
    "\n",
    "        total_atomic = 0\n",
    "        for element, atomic_num in element_atomic.items():\n",
    "            if element in password:\n",
    "                total_atomic += atomic_num\n",
    "\n",
    "        return total_atomic == target\n",
    "\n",
    "    def step(self, password: str=None, give_up:bool=False):\n",
    "        \"\"\"\n",
    "        The main interaction function for the RL environment.\n",
    "        You submit a password, and it returns the new state.\n",
    "        \"\"\"\n",
    "\n",
    "        if give_up:\n",
    "          self.end_game()\n",
    "          reward = self.calculate_reward(password)\n",
    "          feedback = self.get_rule_feedback(password)\n",
    "          return {\n",
    "            \"game_ended\": True,\n",
    "            \"gave_up\": True,\n",
    "            \"reward\": reward,\n",
    "            \"final_password\": password,\n",
    "            \"rule_feedback\": feedback\n",
    "          }\n",
    "        if password is not None:\n",
    "          self.password_history.append(password)\n",
    "          \n",
    "        if len(self.password_history) == 0:\n",
    "          return {\"current_rule_index\": self.current_rule,\n",
    "          \"current_rule\": self.get_current_rule(),\n",
    "          \"game_active\": self.game_active,\n",
    "          \"instructions\": self.get_instructions(),\n",
    "          }\n",
    "\n",
    "        # Advance to next rule\n",
    "        self.advance_rule()\n",
    "\n",
    "        # Check if game ended naturally\n",
    "        if not self.game_active:\n",
    "          reward = self.calculate_reward(password)\n",
    "          feedback = self.get_rule_feedback(password)\n",
    "          return {\n",
    "            \"game_ended\": True,\n",
    "            \"gave_up\": False,\n",
    "            \"reward\": reward,\n",
    "            \"final_password\": password,\n",
    "            \"rule_feedback\": feedback\n",
    "          }\n",
    "\n",
    "        return self.get_minimal_game_state()\n",
    "        \n",
    "\n",
    "    def _is_prime(self, n: int) -> bool:\n",
    "        \"\"\"Check if number is prime.\"\"\"\n",
    "        if n < 2:\n",
    "            return False\n",
    "        if n == 2:\n",
    "            return True\n",
    "        if n % 2 == 0:\n",
    "            return False\n",
    "\n",
    "        for i in range(3, int(n**0.5) + 1, 2):\n",
    "            if n % i == 0:\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Eval"
   ]
  },
  {
   "id": "ff1f6f50",
   "metadata": {},
   "source": [
    "# Eval Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_model(model, dataset, num_samples=100, desc=\"Eval\"):\n    model.eval()\n    model.config.use_cache = True\n    \n    total_reward = 0.0\n    \n    with torch.no_grad():\n        for i in tqdm(range(min(num_samples, len(dataset))), desc=desc):\n            item = dataset[i]\n            prompt = item['prompt']\n            num_rules = item['num_rules']\n            \n            inputs = tokenizer([prompt], return_tensors=\"pt\", padding=True, truncation=True, max_length=config.max_prompt_length).to(DEVICE)\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=config.max_new_tokens,\n                do_sample=True,\n                temperature=config.temperature,\n                top_p=config.top_p,\n                top_k=config.top_k,\n                pad_token_id=tokenizer.pad_token_id,\n                eos_token_id=tokenizer.eos_token_id,\n            )\n            \n            generated = tokenizer.decode(outputs[0, inputs.input_ids.size(1):], skip_special_tokens=True)\n            password = generated.strip().split()[0] if generated.strip() else \"\"\n            reward = compute_reward(password, num_rules)\n            total_reward += reward\n    \n    model.config.use_cache = False\n    model.train()\n    return total_reward / min(num_samples, len(dataset))\n\nbaseline_reward = evaluate_model(policy_model, val_dataset, num_samples=100, desc=\"Baseline\")\nprint(f\"\\nBaseline reward: {baseline_reward:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Utils"
   ]
  },
  {
   "id": "f730e200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_resp(resp):\n",
    "    pattern = r\"### Password\\s*\\n([\\s\\S]+?)\\s*\\n### Giveup\\s*\\n(true|false)\"\n",
    "\n",
    "    # Use re.IGNORECASE to match \"True\", \"False\", \"true\", or \"false\"\n",
    "    match = re.search(pattern, resp, re.IGNORECASE)\n",
    "    if match:\n",
    "        # Group 1 is the password\n",
    "        password = match.group(1).strip()\n",
    "        \n",
    "        # Group 2 is the giveup string (\"true\" or \"false\")\n",
    "        # We convert it to a Python boolean\n",
    "        giveup_str = match.group(2).lower()\n",
    "        giveup_boolean = giveup_str == 'true'\n",
    "        \n",
    "        return password, giveup_boolean\n",
    "    else:\n",
    "        # Return None if the pattern doesn't match the response\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_probs(model, input_ids, attention_mask, return_values=False):\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=return_values)\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits[:, :-1, :], dim=-1)\n",
    "    token_log_probs = torch.gather(log_probs, dim=2, index=input_ids[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "    mask = attention_mask[:, 1:].bool()\n",
    "    token_log_probs = token_log_probs * mask\n",
    "    if return_values:\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "        values = value_head(hidden_states).squeeze(-1)\n",
    "        return token_log_probs, values\n",
    "    return token_log_probs\n",
    "\n",
    "def compute_advantages(rewards, values, masks, gamma=0.99, gae_lambda=0.95):\n",
    "    batch_size, seq_len = rewards.shape\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "    gae = 0\n",
    "    for t in reversed(range(seq_len)):\n",
    "        next_value = 0 if t == seq_len - 1 else values[:, t + 1]\n",
    "        delta = rewards[:, t] + gamma * next_value * masks[:, t] - values[:, t]\n",
    "        gae = delta + gamma * gae_lambda * masks[:, t] * gae\n",
    "        advantages[:, t] = gae\n",
    "    returns = advantages + values\n",
    "    return advantages, returns\n",
    "\n",
    "def whiten(values, mask):\n",
    "    mean = (values * mask).sum() / mask.sum()\n",
    "    var = ((values - mean) ** 2 * mask).sum() / mask.sum()\n",
    "    std = torch.sqrt(var + 1e-8)\n",
    "    return (values - mean) / std\n",
    "\n",
    "print(\"PPO utils defined\")"
   ]
  },
  {
   "id": "d3dccd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def continue_chat(message):\n",
    "  retries = 3\n",
    "  for i in range(retries):\n",
    "    try:\n",
    "      response = chat.send_message(message)\n",
    "      return response.text\n",
    "    except Exception as e:\n",
    "      if e.code == 503 and i < retries - 1:\n",
    "        print(f\"Server overloaded (503). Retrying in 5 seconds... (Attempt {i + 1}/{retries})\")\n",
    "        time.sleep(5)\n",
    "      else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "id": "6003f9e6",
   "metadata": {},
   "source": [
    "# Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    list(policy_model.parameters()) + list(value_head.parameters()),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "total_steps = config.num_epochs * config.num_steps_per_epoch\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup_steps, num_training_steps=total_steps)\n",
    "print(f\"Optimizer ready: {total_steps} steps\")"
   ]
  },
  {
   "id": "b4a1ddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiation\n",
    "num_instances = 3\n",
    "game_instances = {f\"game_{i}\": PasswordGame() for i in range(num_instances)}\n",
    "game = game_instances[\"game_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_run = wandb.init(project=config.wandb_project, name=config.wandb_run_name, config=asdict(config))\n",
    "print(f\"WandB: {wandb_run.get_url()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model.train()\n",
    "value_head.train()\n",
    "global_step = 0\n",
    "best_val_reward = -float('inf')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{config.num_epochs}\")\n",
    "    epoch_iter = iter(train_dataloader)\n",
    "    \n",
    "    for step in tqdm(range(config.num_steps_per_epoch), desc=f\"Epoch {epoch+1}\"):\n",
    "        try:\n",
    "            batch = next(epoch_iter)\n",
    "        except StopIteration:\n",
    "            epoch_iter = iter(train_dataloader)\n",
    "            batch = next(epoch_iter)\n",
    "        \n",
    "        prompts = batch['prompt']\n",
    "        num_rules_list = batch['num_rules']\n",
    "        \n",
    "        # Rollout\n",
    "        policy_model.eval()\n",
    "        policy_model.config.use_cache = True\n",
    "        with torch.no_grad():\n",
    "            prompt_inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=config.max_prompt_length).to(DEVICE)\n",
    "            all_responses = []\n",
    "            all_full_ids = []\n",
    "            all_masks = []\n",
    "            for _ in range(config.samples_per_prompt):\n",
    "                outputs = policy_model.generate(\n",
    "                    **prompt_inputs,\n",
    "                    max_new_tokens=config.max_new_tokens,\n",
    "                    do_sample=True,\n",
    "                    temperature=config.temperature,\n",
    "                    top_p=config.top_p,\n",
    "                    top_k=config.top_k,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "                generated_ids = outputs[:, prompt_inputs.input_ids.size(1):]\n",
    "                responses = []\n",
    "                for gen_ids in generated_ids:\n",
    "                    resp = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "                    password = resp.strip().split()[0] if resp.strip() else \"\"\n",
    "                    responses.append(password)\n",
    "                all_responses.extend(responses)\n",
    "                all_full_ids.append(outputs)\n",
    "                mask = torch.ones_like(outputs)\n",
    "                mask[outputs == tokenizer.pad_token_id] = 0\n",
    "                all_masks.append(mask)\n",
    "            all_full_ids = torch.cat(all_full_ids, dim=0)\n",
    "            all_masks = torch.cat(all_masks, dim=0)\n",
    "            expanded_num_rules = num_rules_list * config.samples_per_prompt\n",
    "        \n",
    "        # Rewards\n",
    "        rewards = torch.tensor([compute_reward(pwd, nr) for pwd, nr in zip(all_responses, expanded_num_rules)], device=DEVICE, dtype=dtype)\n",
    "        mean_reward = rewards.mean().item()\n",
    "        \n",
    "        # Old probs & values\n",
    "        with torch.no_grad():\n",
    "            old_log_probs, old_values = compute_log_probs(policy_model, all_full_ids, all_masks, return_values=True)\n",
    "            ref_log_probs = compute_log_probs(reference_model, all_full_ids, all_masks)\n",
    "            prompt_len = prompt_inputs.input_ids.size(1)\n",
    "            old_values_gen = old_values[:, prompt_len:]\n",
    "            generated_ids_all = all_full_ids[:, prompt_len:]\n",
    "        \n",
    "        # Advantages\n",
    "        response_mask = (generated_ids_all != tokenizer.pad_token_id).float()\n",
    "        reward_per_token = torch.zeros_like(generated_ids_all, dtype=dtype)\n",
    "        for i, reward in enumerate(rewards):\n",
    "            valid = generated_ids_all[i] != tokenizer.pad_token_id\n",
    "            reward_per_token[i][valid] = reward / valid.sum().clamp(min=1)\n",
    "        advantages, returns = compute_advantages(reward_per_token, old_values_gen, response_mask, config.gamma, config.gae_lambda)\n",
    "        if config.normalize_advantages:\n",
    "            advantages = whiten(advantages, response_mask)\n",
    "        \n",
    "        # PPO updates\n",
    "        policy_model.train()\n",
    "        policy_model.config.use_cache = False\n",
    "        for ppo_epoch in range(config.ppo_epochs):\n",
    "            curr_log_probs, curr_values = compute_log_probs(policy_model, all_full_ids, all_masks, return_values=True)\n",
    "            curr_values_gen = curr_values[:, prompt_len:]\n",
    "            curr_lp_gen = curr_log_probs[:, prompt_len-1:]\n",
    "            old_lp_gen = old_log_probs[:, prompt_len-1:]\n",
    "            ref_lp_gen = ref_log_probs[:, prompt_len-1:]\n",
    "            \n",
    "            ratio = torch.exp(curr_lp_gen - old_lp_gen.detach())\n",
    "            policy_loss = torch.max(\n",
    "                -advantages.detach() * ratio,\n",
    "                -advantages.detach() * torch.clamp(ratio, 1-config.clip_range, 1+config.clip_range)\n",
    "            )\n",
    "            policy_loss = (policy_loss * response_mask).sum() / response_mask.sum()\n",
    "            value_loss = ((curr_values_gen - returns.detach())**2 * response_mask).sum() / response_mask.sum()\n",
    "            kl_penalty = ((curr_lp_gen - ref_lp_gen.detach()) * response_mask).sum() / response_mask.sum()\n",
    "            \n",
    "            loss = policy_loss + config.value_loss_coef * value_loss + config.kl_coef * kl_penalty\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(list(policy_model.parameters()) + list(value_head.parameters()), config.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Logging\n",
    "        if global_step % config.log_interval == 0:\n",
    "            wandb.log({\"step\": global_step, \"loss\": loss.item(), \"reward\": mean_reward, \"kl\": kl_penalty.item()}, step=global_step)\n",
    "        \n",
    "        # Eval\n",
    "        if global_step % config.eval_interval == 0 and global_step > 0:\n",
    "            val_reward = evaluate_model(policy_model, val_dataset, num_samples=50, desc=f\"Eval@{global_step}\")\n",
    "            wandb.log({\"val_reward\": val_reward}, step=global_step)\n",
    "            if val_reward > best_val_reward:\n",
    "                best_val_reward = val_reward\n",
    "                best_dir = os.path.join(config.output_dir, \"best_model\")\n",
    "                os.makedirs(best_dir, exist_ok=True)\n",
    "                policy_model.save_pretrained(best_dir)\n",
    "                tokenizer.save_pretrained(best_dir)\n",
    "                print(f\"\\nBest: {best_val_reward:.4f}\")\n",
    "        \n",
    "        # Checkpoint\n",
    "        if global_step % config.save_interval == 0 and global_step > 0:\n",
    "            ckpt_dir = os.path.join(config.output_dir, f\"checkpoint-{global_step}\")\n",
    "            os.makedirs(ckpt_dir, exist_ok=True)\n",
    "            policy_model.save_pretrained(ckpt_dir)\n",
    "            tokenizer.save_pretrained(ckpt_dir)\n",
    "        \n",
    "        global_step += 1\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nTraining complete! Best val: {best_val_reward:.4f}\")"
   ]
  },
  {
   "id": "d12dfc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "start = game.step(password = None, give_up=False)\n",
    "print(\"start:\", start)\n",
    "r = continue_chat(str(start))\n",
    "pw,giveup = None, None\n",
    "while(game.game_active):\n",
    "  pw,giveup = parse_resp(r)\n",
    "  next = game.step(pw, give_up=giveup)\n",
    "  print(\"---\")\n",
    "  print(\"password\",pw, \"next:\", next)\n",
    "  r = continue_chat(str(next))\n",
    "  print(\"---GEMINI RESPONSE---\")\n",
    "  print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reward = evaluate_model(policy_model, val_dataset, num_samples=len(val_dataset), desc=\"Final\")\n",
    "print(f\"Final reward: {final_reward:.4f}\")\n",
    "print(f\"Improvement: {final_reward - baseline_reward:.4f}\")\n",
    "\n",
    "final_dir = os.path.join(config.output_dir, \"final_model\")\n",
    "os.makedirs(final_dir, exist_ok=True)\n",
    "policy_model.save_pretrained(final_dir)\n",
    "tokenizer.save_pretrained(final_dir)\n",
    "print(f\"Saved to {final_dir}\")\n",
    "\n",
    "summary = {\n",
    "    \"baseline\": baseline_reward,\n",
    "    \"final\": final_reward,\n",
    "    \"best_val\": best_val_reward,\n",
    "    \"improvement\": final_reward - baseline_reward\n",
    "}\n",
    "with open(os.path.join(config.output_dir, \"summary.json\"), \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "wandb.finish()\n",
    "print(f\"\\nSummary: {summary}\")"
   ]
  },
  {
   "id": "cf2fbfb5",
   "metadata": {},
   "source": [
    "# RL "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}