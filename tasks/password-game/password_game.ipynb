{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Password Game RL Training with VERL\n\nTrain Qwen3-0.6B to solve the Password Game using PPO.\n\n**Rules**: 9 progressive password rules  \n**Reward**: +1 per rule passed, -0.1 per character"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q flash-attn --no-build-isolation\n",
    "!pip install -q transformers accelerate datasets tokenizers wandb tqdm numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random, time, re\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "assert DEVICE == \"cuda\", \"GPU required\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass Config:\n    # Model\n    model_name: str = \"Qwen/Qwen3-0.6B\"\n    precision: str = \"bfloat16\"\n    use_flash_attn: bool = True\n    \n    # Training\n    num_epochs: int = 3\n    num_steps_per_epoch: int = 100\n    batch_size: int = 4\n    samples_per_prompt: int = 4\n    learning_rate: float = 1e-6\n    weight_decay: float = 0.01\n    max_grad_norm: float = 1.0\n    warmup_steps: int = 50\n    \n    # PPO\n    ppo_epochs: int = 4\n    clip_range: float = 0.2\n    value_loss_coef: float = 0.1\n    kl_coef: float = 0.05\n    gamma: float = 0.99\n    gae_lambda: float = 0.95\n    normalize_advantages: bool = True\n    \n    # Generation\n    max_prompt_length: int = 1024\n    max_new_tokens: int = 256\n    temperature: float = 0.8\n    top_p: float = 0.9\n    top_k: int = 50\n    \n    # Password Game\n    num_rules: int = 9\n    reward_per_rule: float = 1.0\n    length_penalty: float = 0.1\n    \n    # Data\n    num_train_samples: int = 1000\n    num_val_samples: int = 200\n    \n    # Logging\n    wandb_project: str = \"password-game-rl\"\n    wandb_run_name: Optional[str] = None\n    log_interval: int = 10\n    eval_interval: int = 50\n    save_interval: int = 100\n    output_dir: str = f\"./password_game_{int(time.time())}\"\n    seed: int = 42\n    \n    def __post_init__(self):\n        if self.wandb_run_name is None:\n            self.wandb_run_name = f\"password_ppo_{int(time.time())}\"\n        os.makedirs(self.output_dir, exist_ok=True)\n\nconfig = Config()\nprint(f\"Model: {config.model_name}\")\nprint(f\"Batch: {config.batch_size} x {config.samples_per_prompt} = {config.batch_size * config.samples_per_prompt}\")\nprint(f\"Output: {config.output_dir}\")\n\nwith open(os.path.join(config.output_dir, \"config.json\"), \"w\") as f:\n    json.dump(asdict(config), f, indent=2)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.bfloat16 if config.precision == \"bfloat16\" else torch.float32\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "print(f\"Tokenizer: {len(tokenizer)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\" if config.use_flash_attn else \"eager\"\n",
    ")\n",
    "policy_model.config.use_cache = False\n",
    "print(f\"Policy: {sum(p.numel() for p in policy_model.parameters())/1e9:.2f}B params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\" if config.use_flash_attn else \"eager\"\n",
    ")\n",
    "reference_model.eval()\n",
    "for param in reference_model.parameters():\n",
    "    param.requires_grad = False\n",
    "print(\"Reference: frozen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueHead(nn.Module):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "        nn.init.orthogonal_(self.linear.weight, gain=0.01)\n",
    "        nn.init.constant_(self.linear.bias, 0.0)\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        return self.linear(hidden_states)\n",
    "\n",
    "value_head = ValueHead(policy_model.config.hidden_size).to(DEVICE).to(dtype)\n",
    "print(f\"Value head: {policy_model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Password Game Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PASSWORD_RULES = [\n",
    "    \"Your password must be at least 5 characters.\",\n",
    "    \"Your password must include a number.\",\n",
    "    \"Your password must include an uppercase letter.\",\n",
    "    \"Your password must include a special character.\",\n",
    "    \"The digits in your password must add up to 25.\",\n",
    "    \"Your password must include a month of the year.\",\n",
    "    \"Your password must include a roman numeral.\",\n",
    "    \"Your password must include one of our sponsors: (Pepsi, Starbucks, Shell)\",\n",
    "    \"The roman numerals in your password should multiply to 35.\",\n",
    "]\n",
    "\n",
    "INSTRUCTIONS = \"\"\"You are playing a password game. Create a password that satisfies ALL the given rules.\n",
    "Return ONLY the password string, nothing else.\"\"\"\n",
    "\n",
    "def check_rule(password: str, rule_idx: int) -> bool:\n",
    "    pwd = password\n",
    "    \n",
    "    if rule_idx == 0:\n",
    "        return len(pwd) >= 5\n",
    "    elif rule_idx == 1:\n",
    "        return any(c.isdigit() for c in pwd)\n",
    "    elif rule_idx == 2:\n",
    "        return any(c.isupper() for c in pwd)\n",
    "    elif rule_idx == 3:\n",
    "        return any(not c.isalnum() for c in pwd)\n",
    "    elif rule_idx == 4:\n",
    "        return sum(int(c) for c in pwd if c.isdigit()) == 25\n",
    "    elif rule_idx == 5:\n",
    "        months = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december']\n",
    "        return any(m in pwd.lower() for m in months)\n",
    "    elif rule_idx == 6:\n",
    "        return bool(re.search(r'[IVXLCDM]+', pwd))\n",
    "    elif rule_idx == 7:\n",
    "        sponsors = ['pepsi', 'starbucks', 'shell']\n",
    "        return any(s in pwd.lower() for s in sponsors)\n",
    "    elif rule_idx == 8:\n",
    "        romans = re.findall(r'[IVXLCDM]+', pwd)\n",
    "        if not romans:\n",
    "            return False\n",
    "        roman_vals = {'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000}\n",
    "        product = 1\n",
    "        for r in romans:\n",
    "            val = 0\n",
    "            prev = 0\n",
    "            for c in reversed(r):\n",
    "                v = roman_vals.get(c, 0)\n",
    "                if v < prev:\n",
    "                    val -= v\n",
    "                else:\n",
    "                    val += v\n",
    "                prev = v\n",
    "            if val > 0:\n",
    "                product *= val\n",
    "        return product == 35\n",
    "    return False\n",
    "\n",
    "def compute_reward(password: str, num_active_rules: int) -> float:\n",
    "    passing = sum(check_rule(password, i) for i in range(num_active_rules))\n",
    "    rule_score = passing * config.reward_per_rule\n",
    "    length_penalty = len(password) * config.length_penalty\n",
    "    return rule_score - length_penalty\n",
    "\n",
    "def format_prompt(num_active_rules: int) -> str:\n",
    "    rules_text = \"\\n\".join([f\"{i+1}. {PASSWORD_RULES[i]}\" for i in range(num_active_rules)])\n",
    "    return f\"{INSTRUCTIONS}\\n\\nRules:\\n{rules_text}\\n\\nPassword:\"\n",
    "\n",
    "print(f\"Loaded {len(PASSWORD_RULES)} rules\")\n",
    "print(f\"Example prompt:\\n{format_prompt(3)[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PasswordDataset(Dataset):\n",
    "    def __init__(self, num_samples: int, max_rules: int):\n",
    "        self.prompts = []\n",
    "        self.num_rules = []\n",
    "        for _ in range(num_samples):\n",
    "            n = random.randint(3, min(max_rules, len(PASSWORD_RULES)))\n",
    "            self.prompts.append(format_prompt(n))\n",
    "            self.num_rules.append(n)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {'prompt': self.prompts[idx], 'num_rules': self.num_rules[idx]}\n",
    "\n",
    "train_dataset = PasswordDataset(config.num_train_samples, 9)\n",
    "val_dataset = PasswordDataset(config.num_val_samples, 9)\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_model(model, dataset, num_samples=100, desc=\"Eval\"):\n    model.eval()\n    model.config.use_cache = True\n    \n    total_reward = 0.0\n    \n    with torch.no_grad():\n        for i in tqdm(range(min(num_samples, len(dataset))), desc=desc):\n            item = dataset[i]\n            prompt = item['prompt']\n            num_rules = item['num_rules']\n            \n            inputs = tokenizer([prompt], return_tensors=\"pt\", padding=True, truncation=True, max_length=config.max_prompt_length).to(DEVICE)\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=config.max_new_tokens,\n                do_sample=True,\n                temperature=config.temperature,\n                top_p=config.top_p,\n                top_k=config.top_k,\n                pad_token_id=tokenizer.pad_token_id,\n                eos_token_id=tokenizer.eos_token_id,\n            )\n            \n            generated = tokenizer.decode(outputs[0, inputs.input_ids.size(1):], skip_special_tokens=True)\n            password = generated.strip().split()[0] if generated.strip() else \"\"\n            reward = compute_reward(password, num_rules)\n            total_reward += reward\n    \n    model.config.use_cache = False\n    model.train()\n    return total_reward / min(num_samples, len(dataset))\n\nbaseline_reward = evaluate_model(policy_model, val_dataset, num_samples=100, desc=\"Baseline\")\nprint(f\"\\nBaseline reward: {baseline_reward:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_probs(model, input_ids, attention_mask, return_values=False):\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=return_values)\n",
    "    logits = outputs.logits\n",
    "    log_probs = F.log_softmax(logits[:, :-1, :], dim=-1)\n",
    "    token_log_probs = torch.gather(log_probs, dim=2, index=input_ids[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "    mask = attention_mask[:, 1:].bool()\n",
    "    token_log_probs = token_log_probs * mask\n",
    "    if return_values:\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "        values = value_head(hidden_states).squeeze(-1)\n",
    "        return token_log_probs, values\n",
    "    return token_log_probs\n",
    "\n",
    "def compute_advantages(rewards, values, masks, gamma=0.99, gae_lambda=0.95):\n",
    "    batch_size, seq_len = rewards.shape\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "    gae = 0\n",
    "    for t in reversed(range(seq_len)):\n",
    "        next_value = 0 if t == seq_len - 1 else values[:, t + 1]\n",
    "        delta = rewards[:, t] + gamma * next_value * masks[:, t] - values[:, t]\n",
    "        gae = delta + gamma * gae_lambda * masks[:, t] * gae\n",
    "        advantages[:, t] = gae\n",
    "    returns = advantages + values\n",
    "    return advantages, returns\n",
    "\n",
    "def whiten(values, mask):\n",
    "    mean = (values * mask).sum() / mask.sum()\n",
    "    var = ((values - mean) ** 2 * mask).sum() / mask.sum()\n",
    "    std = torch.sqrt(var + 1e-8)\n",
    "    return (values - mean) / std\n",
    "\n",
    "print(\"PPO utils defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    list(policy_model.parameters()) + list(value_head.parameters()),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "total_steps = config.num_epochs * config.num_steps_per_epoch\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup_steps, num_training_steps=total_steps)\n",
    "print(f\"Optimizer ready: {total_steps} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_run = wandb.init(project=config.wandb_project, name=config.wandb_run_name, config=asdict(config))\n",
    "print(f\"WandB: {wandb_run.get_url()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model.train()\n",
    "value_head.train()\n",
    "global_step = 0\n",
    "best_val_reward = -float('inf')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{config.num_epochs}\")\n",
    "    epoch_iter = iter(train_dataloader)\n",
    "    \n",
    "    for step in tqdm(range(config.num_steps_per_epoch), desc=f\"Epoch {epoch+1}\"):\n",
    "        try:\n",
    "            batch = next(epoch_iter)\n",
    "        except StopIteration:\n",
    "            epoch_iter = iter(train_dataloader)\n",
    "            batch = next(epoch_iter)\n",
    "        \n",
    "        prompts = batch['prompt']\n",
    "        num_rules_list = batch['num_rules']\n",
    "        \n",
    "        # Rollout\n",
    "        policy_model.eval()\n",
    "        policy_model.config.use_cache = True\n",
    "        with torch.no_grad():\n",
    "            prompt_inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=config.max_prompt_length).to(DEVICE)\n",
    "            all_responses = []\n",
    "            all_full_ids = []\n",
    "            all_masks = []\n",
    "            for _ in range(config.samples_per_prompt):\n",
    "                outputs = policy_model.generate(\n",
    "                    **prompt_inputs,\n",
    "                    max_new_tokens=config.max_new_tokens,\n",
    "                    do_sample=True,\n",
    "                    temperature=config.temperature,\n",
    "                    top_p=config.top_p,\n",
    "                    top_k=config.top_k,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "                generated_ids = outputs[:, prompt_inputs.input_ids.size(1):]\n",
    "                responses = []\n",
    "                for gen_ids in generated_ids:\n",
    "                    resp = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "                    password = resp.strip().split()[0] if resp.strip() else \"\"\n",
    "                    responses.append(password)\n",
    "                all_responses.extend(responses)\n",
    "                all_full_ids.append(outputs)\n",
    "                mask = torch.ones_like(outputs)\n",
    "                mask[outputs == tokenizer.pad_token_id] = 0\n",
    "                all_masks.append(mask)\n",
    "            all_full_ids = torch.cat(all_full_ids, dim=0)\n",
    "            all_masks = torch.cat(all_masks, dim=0)\n",
    "            expanded_num_rules = num_rules_list * config.samples_per_prompt\n",
    "        \n",
    "        # Rewards\n",
    "        rewards = torch.tensor([compute_reward(pwd, nr) for pwd, nr in zip(all_responses, expanded_num_rules)], device=DEVICE, dtype=dtype)\n",
    "        mean_reward = rewards.mean().item()\n",
    "        \n",
    "        # Old probs & values\n",
    "        with torch.no_grad():\n",
    "            old_log_probs, old_values = compute_log_probs(policy_model, all_full_ids, all_masks, return_values=True)\n",
    "            ref_log_probs = compute_log_probs(reference_model, all_full_ids, all_masks)\n",
    "            prompt_len = prompt_inputs.input_ids.size(1)\n",
    "            old_values_gen = old_values[:, prompt_len:]\n",
    "            generated_ids_all = all_full_ids[:, prompt_len:]\n",
    "        \n",
    "        # Advantages\n",
    "        response_mask = (generated_ids_all != tokenizer.pad_token_id).float()\n",
    "        reward_per_token = torch.zeros_like(generated_ids_all, dtype=dtype)\n",
    "        for i, reward in enumerate(rewards):\n",
    "            valid = generated_ids_all[i] != tokenizer.pad_token_id\n",
    "            reward_per_token[i][valid] = reward / valid.sum().clamp(min=1)\n",
    "        advantages, returns = compute_advantages(reward_per_token, old_values_gen, response_mask, config.gamma, config.gae_lambda)\n",
    "        if config.normalize_advantages:\n",
    "            advantages = whiten(advantages, response_mask)\n",
    "        \n",
    "        # PPO updates\n",
    "        policy_model.train()\n",
    "        policy_model.config.use_cache = False\n",
    "        for ppo_epoch in range(config.ppo_epochs):\n",
    "            curr_log_probs, curr_values = compute_log_probs(policy_model, all_full_ids, all_masks, return_values=True)\n",
    "            curr_values_gen = curr_values[:, prompt_len:]\n",
    "            curr_lp_gen = curr_log_probs[:, prompt_len-1:]\n",
    "            old_lp_gen = old_log_probs[:, prompt_len-1:]\n",
    "            ref_lp_gen = ref_log_probs[:, prompt_len-1:]\n",
    "            \n",
    "            ratio = torch.exp(curr_lp_gen - old_lp_gen.detach())\n",
    "            policy_loss = torch.max(\n",
    "                -advantages.detach() * ratio,\n",
    "                -advantages.detach() * torch.clamp(ratio, 1-config.clip_range, 1+config.clip_range)\n",
    "            )\n",
    "            policy_loss = (policy_loss * response_mask).sum() / response_mask.sum()\n",
    "            value_loss = ((curr_values_gen - returns.detach())**2 * response_mask).sum() / response_mask.sum()\n",
    "            kl_penalty = ((curr_lp_gen - ref_lp_gen.detach()) * response_mask).sum() / response_mask.sum()\n",
    "            \n",
    "            loss = policy_loss + config.value_loss_coef * value_loss + config.kl_coef * kl_penalty\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(list(policy_model.parameters()) + list(value_head.parameters()), config.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Logging\n",
    "        if global_step % config.log_interval == 0:\n",
    "            wandb.log({\"step\": global_step, \"loss\": loss.item(), \"reward\": mean_reward, \"kl\": kl_penalty.item()}, step=global_step)\n",
    "        \n",
    "        # Eval\n",
    "        if global_step % config.eval_interval == 0 and global_step > 0:\n",
    "            val_reward = evaluate_model(policy_model, val_dataset, num_samples=50, desc=f\"Eval@{global_step}\")\n",
    "            wandb.log({\"val_reward\": val_reward}, step=global_step)\n",
    "            if val_reward > best_val_reward:\n",
    "                best_val_reward = val_reward\n",
    "                best_dir = os.path.join(config.output_dir, \"best_model\")\n",
    "                os.makedirs(best_dir, exist_ok=True)\n",
    "                policy_model.save_pretrained(best_dir)\n",
    "                tokenizer.save_pretrained(best_dir)\n",
    "                print(f\"\\nBest: {best_val_reward:.4f}\")\n",
    "        \n",
    "        # Checkpoint\n",
    "        if global_step % config.save_interval == 0 and global_step > 0:\n",
    "            ckpt_dir = os.path.join(config.output_dir, f\"checkpoint-{global_step}\")\n",
    "            os.makedirs(ckpt_dir, exist_ok=True)\n",
    "            policy_model.save_pretrained(ckpt_dir)\n",
    "            tokenizer.save_pretrained(ckpt_dir)\n",
    "        \n",
    "        global_step += 1\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nTraining complete! Best val: {best_val_reward:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reward = evaluate_model(policy_model, val_dataset, num_samples=len(val_dataset), desc=\"Final\")\n",
    "print(f\"Final reward: {final_reward:.4f}\")\n",
    "print(f\"Improvement: {final_reward - baseline_reward:.4f}\")\n",
    "\n",
    "final_dir = os.path.join(config.output_dir, \"final_model\")\n",
    "os.makedirs(final_dir, exist_ok=True)\n",
    "policy_model.save_pretrained(final_dir)\n",
    "tokenizer.save_pretrained(final_dir)\n",
    "print(f\"Saved to {final_dir}\")\n",
    "\n",
    "summary = {\n",
    "    \"baseline\": baseline_reward,\n",
    "    \"final\": final_reward,\n",
    "    \"best_val\": best_val_reward,\n",
    "    \"improvement\": final_reward - baseline_reward\n",
    "}\n",
    "with open(os.path.join(config.output_dir, \"summary.json\"), \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "wandb.finish()\n",
    "print(f\"\\nSummary: {summary}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}